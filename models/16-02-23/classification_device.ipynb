{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmIXglOU_ub6"
   },
   "source": [
    "# Setup Python Environment\n",
    "The next cell sets up the dependencies in required for the notebook, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz8AJT14z0Tk",
    "outputId": "e4989b99-b845-4e34-828d-d0347d6ae6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.24.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.29.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow==2.10.0-rc1 in /opt/conda/lib/python3.9/site-packages (2.10.0rc1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.10.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.43.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (15.0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (4.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.30.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.12.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (21.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (59.8.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.24.2)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (23.1.21)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.19.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.4.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.10.0-rc1) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow==2.10.0-rc1) (3.0.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.2.0)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "!apt-get -qq install xxd\n",
    "!pip install pandas numpy matplotlib\n",
    "!pip install tensorflow==2.10.0-rc1\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lTI_Sx0_90e"
   },
   "source": [
    "# Train Neural Network\n",
    "## Parse and prepare the data\n",
    "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVOiR1uQKzpy",
    "outputId": "e2797aae-fc03-4def-a699-308a495dce0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 12:30:39.686817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 12:30:39.752218: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-16 12:30:39.771264: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-16 12:30:40.144915: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-16 12:30:40.144950: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-16 12:30:40.144956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version = 2.10.0-rc1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Ftrl\n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
    "\n",
    "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
    "# the same random numbers each time the notebook is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laptop', 'phone_charging', 'school_box', 'school_pc', 'school_printer']\n"
     ]
    }
   ],
   "source": [
    "# SEED = 1337\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "# SEED=int(time.time())\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "CLASSES = [];\n",
    "\n",
    "for file in os.listdir(\"../data/own-csv\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        CLASSES.append(os.path.splitext(file)[0])\n",
    "\n",
    "CLASSES.sort()\n",
    "\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "YxYCUqzTAJeX",
    "outputId": "f5942731-9f3c-4ebf-ae67-d3c44d958173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;4mlaptop\u001b[0m class will be output \u001b[32m0\u001b[0m of the classifier\n",
      "119 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mphone_charging\u001b[0m class will be output \u001b[32m1\u001b[0m of the classifier\n",
      "209 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mschool_box\u001b[0m class will be output \u001b[32m2\u001b[0m of the classifier\n",
      "124 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mschool_pc\u001b[0m class will be output \u001b[32m3\u001b[0m of the classifier\n",
      "123 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mschool_printer\u001b[0m class will be output \u001b[32m4\u001b[0m of the classifier\n",
      "119 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "['laptop', 'phone_charging', 'school_box', 'school_pc', 'school_printer']\n",
      "[[55.     0.25   0.93  51.    20.   ]\n",
      " [52.     0.239  1.    52.     0.   ]\n",
      " [54.     0.247  0.96  52.    15.   ]\n",
      " ...\n",
      " [43.     0.199  0.31  14.    41.   ]\n",
      " [15.     0.069  0.99  15.     3.   ]\n",
      " [25.     0.113  0.55  14.    21.   ]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "694\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_WINDOW_LEN = 1\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# create a one-hot encoded matrix that is used in the output\n",
    "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "# read each csv file and push an input and output\n",
    "for class_index in range(NUM_CLASSES):\n",
    "  objectClass = CLASSES[class_index]\n",
    "  df = pd.read_csv(\"../data/own-csv/\" + objectClass + \".csv\") #, parse_dates=['timestamp']\n",
    "  df = df.drop(columns=['time'])\n",
    "  df = df.drop(columns=['Voltage'])\n",
    "  columns = list(df)\n",
    "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
    "  df = df.dropna()\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  # calculate the number of objectClass recordings in the file\n",
    "  num_recordings = int(df.shape[0] / SAMPLES_WINDOW_LEN)\n",
    "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
    "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
    "  \n",
    "  #tensors\n",
    "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
    "  for i in range(num_recordings):\n",
    "    tensor = []\n",
    "    row = []\n",
    "    for c in columns:\n",
    "      row.append(df[c][i])\n",
    "    tensor += row\n",
    "    inputs.append(tensor)\n",
    "    outputs.append(output)\n",
    "\n",
    "# convert the list to numpy array\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array(outputs)\n",
    "num_inputs_2 = len(inputs)\n",
    "print(CLASSES)\n",
    "print(inputs)\n",
    "print(outputs)\n",
    "print(num_inputs_2)\n",
    "\n",
    "# print(\"Data set parsing and preparation complete.\")\n",
    "# startTimestamp = df['timestamp'].iloc[0]\n",
    "# endTimestamp = df['timestamp'].iloc[-1]\n",
    "# print(endTimestamp - startTimestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OGt81LcfVetr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set randomization and splitting complete.\n",
      "[[1.80e+01 8.30e-02 4.00e-02 1.00e+00 1.80e+01]\n",
      " [2.70e+01 1.22e-01 8.20e-01 2.20e+01 1.50e+01]\n",
      " [7.10e+01 3.31e-01 1.00e+00 7.10e+01 0.00e+00]\n",
      " [1.70e+01 8.00e-02 5.00e-02 1.00e+00 1.70e+01]\n",
      " [2.40e+01 1.11e-01 6.00e-01 1.50e+01 1.90e+01]\n",
      " [2.50e+01 1.13e-01 5.70e-01 1.40e+01 2.00e+01]\n",
      " [1.20e+01 5.50e-02 3.50e-01 4.00e+00 1.10e+01]\n",
      " [2.40e+01 1.11e-01 5.70e-01 1.40e+01 2.00e+01]\n",
      " [3.00e+00 1.60e-02 4.60e-01 2.00e+00 3.00e+00]\n",
      " [3.10e+01 1.41e-01 8.80e-01 2.70e+01 1.50e+01]\n",
      " [1.20e+01 5.60e-02 3.30e-01 4.00e+00 1.20e+01]\n",
      " [1.20e+01 5.60e-02 3.40e-01 4.00e+00 1.20e+01]\n",
      " [1.80e+01 8.20e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [1.20e+01 5.30e-02 2.20e-01 3.00e+00 1.10e+01]\n",
      " [4.20e+01 1.93e-01 7.30e-01 3.10e+01 2.80e+01]\n",
      " [1.10e+01 5.20e-02 4.30e-01 5.00e+00 1.00e+01]\n",
      " [1.02e+02 4.64e-01 9.70e-01 9.90e+01 2.60e+01]\n",
      " [1.80e+01 8.20e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [2.50e+01 1.17e-01 5.60e-01 1.40e+01 2.10e+01]\n",
      " [1.20e+01 5.60e-02 2.00e-01 3.00e+00 1.20e+01]\n",
      " [2.60e+01 1.17e-01 5.50e-01 1.40e+01 2.10e+01]\n",
      " [1.20e+01 5.50e-02 3.50e-01 4.00e+00 1.10e+01]\n",
      " [2.40e+01 1.12e-01 6.00e-01 1.50e+01 2.00e+01]\n",
      " [2.00e+01 9.00e-02 5.50e-01 1.10e+01 1.70e+01]\n",
      " [2.60e+01 1.17e-01 5.80e-01 1.50e+01 2.10e+01]\n",
      " [1.80e+01 8.20e-02 4.00e-02 1.00e+00 1.80e+01]\n",
      " [1.00e+02 4.56e-01 9.90e-01 9.90e+01 1.70e+01]\n",
      " [3.20e+01 1.46e-01 9.30e-01 3.00e+01 1.20e+01]\n",
      " [3.00e+01 1.35e-01 8.00e-01 2.40e+01 1.80e+01]\n",
      " [4.20e+01 1.93e-01 7.40e-01 3.10e+01 2.80e+01]\n",
      " [3.00e+01 1.34e-01 7.40e-01 2.20e+01 2.00e+01]\n",
      " [1.20e+01 5.50e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [4.20e+01 1.94e-01 7.40e-01 3.10e+01 2.80e+01]\n",
      " [1.80e+01 8.20e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [5.00e+00 2.30e-02 1.40e-01 1.00e+00 5.00e+00]\n",
      " [1.30e+01 5.80e-02 4.70e-01 6.00e+00 1.10e+01]\n",
      " [2.40e+01 1.11e-01 5.90e-01 1.40e+01 2.00e+01]\n",
      " [1.80e+01 8.40e-02 5.60e-01 1.00e+01 1.50e+01]\n",
      " [1.80e+01 8.20e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [1.30e+01 5.80e-02 4.80e-01 6.00e+00 1.10e+01]\n",
      " [2.60e+01 1.17e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [1.30e+01 5.80e-02 4.80e-01 6.00e+00 1.10e+01]\n",
      " [8.40e+01 3.84e-01 1.00e+00 8.40e+01 0.00e+00]\n",
      " [2.60e+01 1.20e-01 5.70e-01 1.50e+01 2.20e+01]\n",
      " [1.20e+01 5.40e-02 2.40e-01 3.00e+00 1.20e+01]\n",
      " [5.00e+00 2.20e-02 1.60e-01 1.00e+00 5.00e+00]\n",
      " [4.60e+01 2.09e-01 1.00e+00 4.60e+01 0.00e+00]\n",
      " [3.20e+01 1.46e-01 9.00e-01 2.90e+01 1.40e+01]\n",
      " [2.60e+01 1.17e-01 5.60e-01 1.40e+01 2.10e+01]\n",
      " [1.10e+01 5.20e-02 4.40e-01 5.00e+00 1.00e+01]\n",
      " [4.20e+01 1.93e-01 7.40e-01 3.10e+01 2.80e+01]\n",
      " [2.60e+01 1.19e-01 5.50e-01 1.40e+01 2.20e+01]\n",
      " [5.60e+01 2.62e-01 8.80e-01 5.00e+01 2.70e+01]\n",
      " [1.90e+01 8.70e-02 5.20e-01 1.00e+01 1.60e+01]\n",
      " [2.60e+01 1.20e-01 5.60e-01 1.50e+01 2.20e+01]\n",
      " [1.10e+01 5.20e-02 4.20e-01 5.00e+00 1.00e+01]\n",
      " [2.70e+01 1.22e-01 5.50e-01 1.50e+01 2.20e+01]\n",
      " [4.00e+00 2.00e-02 1.40e-01 1.00e+00 4.00e+00]\n",
      " [2.50e+01 1.15e-01 5.80e-01 1.50e+01 2.10e+01]\n",
      " [2.40e+01 1.10e-01 6.00e-01 1.40e+01 1.90e+01]\n",
      " [1.50e+01 6.90e-02 9.90e-01 1.50e+01 3.00e+00]\n",
      " [1.20e+01 5.60e-02 3.30e-01 4.00e+00 1.20e+01]\n",
      " [2.60e+01 1.17e-01 5.60e-01 1.40e+01 2.10e+01]\n",
      " [1.20e+01 5.40e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [2.50e+01 1.16e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [1.20e+01 5.40e-02 2.40e-01 3.00e+00 1.20e+01]\n",
      " [4.20e+01 1.95e-01 7.50e-01 3.10e+01 2.80e+01]\n",
      " [2.50e+01 1.15e-01 5.80e-01 1.50e+01 2.10e+01]\n",
      " [1.80e+01 8.10e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [1.20e+01 5.30e-02 2.30e-01 3.00e+00 1.10e+01]\n",
      " [2.40e+01 1.10e-01 5.80e-01 1.40e+01 2.00e+01]\n",
      " [1.70e+01 7.70e-02 4.00e-02 1.00e+00 1.70e+01]\n",
      " [2.60e+01 1.19e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [2.70e+01 1.22e-01 5.60e-01 1.50e+01 2.20e+01]\n",
      " [1.20e+01 5.70e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [1.40e+01 6.40e-02 3.90e-01 5.00e+00 1.30e+01]\n",
      " [2.60e+01 1.18e-01 5.00e-01 1.30e+01 2.20e+01]\n",
      " [1.20e+01 5.70e-02 3.40e-01 4.00e+00 1.20e+01]\n",
      " [2.50e+01 1.16e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [1.20e+01 5.70e-02 4.90e-01 6.00e+00 1.10e+01]\n",
      " [4.20e+01 1.94e-01 7.50e-01 3.10e+01 2.80e+01]\n",
      " [1.20e+01 5.50e-02 3.80e-01 5.00e+00 1.10e+01]\n",
      " [5.50e+01 2.50e-01 9.30e-01 5.10e+01 2.00e+01]\n",
      " [7.60e+01 3.55e-01 9.60e-01 7.40e+01 2.10e+01]\n",
      " [2.50e+01 1.15e-01 5.70e-01 1.40e+01 2.10e+01]\n",
      " [2.50e+01 1.14e-01 5.70e-01 1.40e+01 2.00e+01]\n",
      " [3.00e+01 1.36e-01 7.70e-01 2.30e+01 1.90e+01]\n",
      " [2.50e+01 1.14e-01 5.80e-01 1.50e+01 2.00e+01]\n",
      " [1.10e+01 5.20e-02 4.20e-01 5.00e+00 1.00e+01]\n",
      " [2.60e+01 1.19e-01 5.60e-01 1.50e+01 2.20e+01]\n",
      " [2.50e+01 1.16e-01 5.60e-01 1.40e+01 2.10e+01]\n",
      " [1.80e+01 8.30e-02 4.00e-02 1.00e+00 1.80e+01]\n",
      " [2.50e+01 1.13e-01 5.70e-01 1.40e+01 2.00e+01]\n",
      " [3.10e+01 1.42e-01 9.10e-01 2.90e+01 1.30e+01]\n",
      " [1.50e+01 6.70e-02 2.10e-01 3.00e+00 1.40e+01]\n",
      " [5.00e+00 2.30e-02 1.60e-01 1.00e+00 5.00e+00]\n",
      " [1.20e+01 5.40e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [2.50e+01 1.16e-01 5.70e-01 1.40e+01 2.10e+01]\n",
      " [1.80e+01 8.10e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [2.10e+01 9.70e-02 5.60e-01 1.20e+01 1.80e+01]\n",
      " [2.40e+01 1.11e-01 6.00e-01 1.50e+01 2.00e+01]\n",
      " [2.60e+01 1.20e-01 5.80e-01 1.50e+01 2.10e+01]\n",
      " [1.20e+01 5.60e-02 3.30e-01 4.00e+00 1.20e+01]\n",
      " [5.00e+00 2.30e-02 1.60e-01 1.00e+00 5.00e+00]\n",
      " [2.50e+01 1.16e-01 5.70e-01 1.40e+01 2.10e+01]\n",
      " [1.20e+01 5.40e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [7.70e+01 3.56e-01 9.50e-01 7.30e+01 2.30e+01]\n",
      " [1.20e+01 5.40e-02 2.30e-01 3.00e+00 1.20e+01]\n",
      " [1.01e+02 4.59e-01 9.70e-01 9.80e+01 2.50e+01]\n",
      " [2.50e+01 1.14e-01 5.70e-01 1.40e+01 2.10e+01]\n",
      " [1.50e+01 7.00e-02 5.70e-01 9.00e+00 1.30e+01]\n",
      " [2.60e+01 1.17e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [1.20e+01 5.50e-02 2.20e-01 3.00e+00 1.20e+01]\n",
      " [2.60e+01 1.21e-01 5.60e-01 1.50e+01 2.20e+01]\n",
      " [1.01e+02 4.63e-01 9.80e-01 1.00e+02 2.00e+01]\n",
      " [1.30e+01 5.80e-02 4.40e-01 6.00e+00 1.10e+01]\n",
      " [1.80e+01 8.20e-02 5.00e-02 1.00e+00 1.80e+01]\n",
      " [1.20e+01 5.40e-02 2.50e-01 3.00e+00 1.20e+01]\n",
      " [4.20e+01 1.94e-01 7.30e-01 3.10e+01 2.90e+01]\n",
      " [2.50e+01 1.16e-01 5.70e-01 1.40e+01 2.10e+01]\n",
      " [2.60e+01 1.17e-01 5.70e-01 1.50e+01 2.10e+01]\n",
      " [1.01e+02 4.61e-01 9.80e-01 9.90e+01 2.20e+01]\n",
      " [1.00e+02 4.54e-01 9.90e-01 9.90e+01 1.40e+01]\n",
      " [0.00e+00 0.00e+00 0.00e+00 0.00e+00 0.00e+00]\n",
      " [1.20e+01 5.70e-02 5.00e-01 6.00e+00 1.10e+01]\n",
      " [4.20e+01 1.93e-01 7.20e-01 3.00e+01 2.90e+01]\n",
      " [2.20e+01 1.02e-01 6.10e-01 1.40e+01 1.80e+01]\n",
      " [1.20e+01 5.50e-02 2.20e-01 3.00e+00 1.20e+01]\n",
      " [1.30e+01 5.80e-02 4.60e-01 6.00e+00 1.10e+01]\n",
      " [1.30e+01 5.80e-02 3.20e-01 4.00e+00 1.20e+01]\n",
      " [1.20e+01 5.40e-02 2.40e-01 3.00e+00 1.20e+01]\n",
      " [1.20e+01 5.30e-02 2.50e-01 3.00e+00 1.10e+01]\n",
      " [1.10e+01 5.20e-02 4.50e-01 5.00e+00 1.00e+01]\n",
      " [2.50e+01 1.15e-01 5.60e-01 1.40e+01 2.10e+01]\n",
      " [1.30e+01 5.80e-02 4.70e-01 6.00e+00 1.10e+01]\n",
      " [2.40e+01 1.12e-01 5.70e-01 1.40e+01 2.00e+01]\n",
      " [2.50e+01 1.15e-01 5.80e-01 1.50e+01 2.10e+01]\n",
      " [3.00e+01 1.36e-01 8.00e-01 2.40e+01 1.80e+01]]\n",
      "694\n"
     ]
    }
   ],
   "source": [
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "# https://stackoverflow.com/a/37710486/2020087\n",
    "num_inputs = len(inputs)\n",
    "randomize = np.arange(num_inputs)\n",
    "np.random.shuffle(randomize)\n",
    "\n",
    "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
    "inputs = inputs[randomize]\n",
    "outputs = outputs[randomize]\n",
    "\n",
    "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
    "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
    "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
    "\n",
    "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "print(\"Data set randomization and splitting complete.\")\n",
    "print(inputs_test)\n",
    "print(num_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Enr4twhJJgex"
   },
   "source": [
    "# Test code voor de tijd in de csv om te zetten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_snKH6OAPx4"
   },
   "source": [
    "#Build & Train the Model\n",
    "Build and train a TensorFlow model using the high-level Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "Ic2Z4XtgAaNh",
    "outputId": "ed119d8f-6237-4813-9575-9ae579eea919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 12:30:40.771493: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-16 12:30:40.771510: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 376fa9da9c60\n",
      "2023-02-16 12:30:40.771514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 376fa9da9c60\n",
      "2023-02-16 12:30:40.771571: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.78.1\n",
      "2023-02-16 12:30:40.771582: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.78.1\n",
      "2023-02-16 12:30:40.771586: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.78.1\n",
      "2023-02-16 12:30:40.771822: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3130 - mae: 0.3414 - val_loss: 0.3004 - val_mae: 0.3282\n",
      "Epoch 2/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.3072 - mae: 0.3414 - val_loss: 0.2931 - val_mae: 0.3286\n",
      "Epoch 3/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.2991 - mae: 0.3414 - val_loss: 0.2827 - val_mae: 0.3284\n",
      "Epoch 4/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.2864 - mae: 0.3401 - val_loss: 0.2624 - val_mae: 0.3249\n",
      "Epoch 5/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.2629 - mae: 0.3346 - val_loss: 0.2357 - val_mae: 0.3168\n",
      "Epoch 6/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.2366 - mae: 0.3262 - val_loss: 0.2120 - val_mae: 0.3103\n",
      "Epoch 7/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.2097 - mae: 0.3172 - val_loss: 0.1957 - val_mae: 0.3060\n",
      "Epoch 8/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.1989 - mae: 0.3124 - val_loss: 0.1922 - val_mae: 0.3046\n",
      "Epoch 9/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.1960 - mae: 0.3110 - val_loss: 0.1906 - val_mae: 0.3047\n",
      "Epoch 10/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.1939 - mae: 0.3110 - val_loss: 0.1889 - val_mae: 0.3049\n",
      "Epoch 11/1500\n",
      "52/52 [==============================] - 0s 759us/step - loss: 0.1914 - mae: 0.3109 - val_loss: 0.1866 - val_mae: 0.3049\n",
      "Epoch 12/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.1880 - mae: 0.3102 - val_loss: 0.1818 - val_mae: 0.3037\n",
      "Epoch 13/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.1823 - mae: 0.3087 - val_loss: 0.1742 - val_mae: 0.3014\n",
      "Epoch 14/1500\n",
      "52/52 [==============================] - 0s 763us/step - loss: 0.1736 - mae: 0.3055 - val_loss: 0.1644 - val_mae: 0.2977\n",
      "Epoch 15/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.1615 - mae: 0.3002 - val_loss: 0.1514 - val_mae: 0.2916\n",
      "Epoch 16/1500\n",
      "52/52 [==============================] - 0s 765us/step - loss: 0.1462 - mae: 0.2910 - val_loss: 0.1403 - val_mae: 0.2850\n",
      "Epoch 17/1500\n",
      "52/52 [==============================] - 0s 769us/step - loss: 0.1394 - mae: 0.2870 - val_loss: 0.1389 - val_mae: 0.2848\n",
      "Epoch 18/1500\n",
      "52/52 [==============================] - 0s 762us/step - loss: 0.1386 - mae: 0.2865 - val_loss: 0.1382 - val_mae: 0.2840\n",
      "Epoch 19/1500\n",
      "52/52 [==============================] - 0s 752us/step - loss: 0.1380 - mae: 0.2857 - val_loss: 0.1377 - val_mae: 0.2833\n",
      "Epoch 20/1500\n",
      "52/52 [==============================] - 0s 776us/step - loss: 0.1373 - mae: 0.2848 - val_loss: 0.1370 - val_mae: 0.2824\n",
      "Epoch 21/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.1367 - mae: 0.2840 - val_loss: 0.1365 - val_mae: 0.2819\n",
      "Epoch 22/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1361 - mae: 0.2836 - val_loss: 0.1359 - val_mae: 0.2811\n",
      "Epoch 23/1500\n",
      "52/52 [==============================] - 0s 772us/step - loss: 0.1354 - mae: 0.2823 - val_loss: 0.1353 - val_mae: 0.2803\n",
      "Epoch 24/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.1347 - mae: 0.2814 - val_loss: 0.1345 - val_mae: 0.2793\n",
      "Epoch 25/1500\n",
      "52/52 [==============================] - 0s 762us/step - loss: 0.1339 - mae: 0.2805 - val_loss: 0.1338 - val_mae: 0.2784\n",
      "Epoch 26/1500\n",
      "52/52 [==============================] - 0s 772us/step - loss: 0.1330 - mae: 0.2797 - val_loss: 0.1330 - val_mae: 0.2776\n",
      "Epoch 27/1500\n",
      "52/52 [==============================] - 0s 755us/step - loss: 0.1322 - mae: 0.2785 - val_loss: 0.1322 - val_mae: 0.2765\n",
      "Epoch 28/1500\n",
      "52/52 [==============================] - 0s 759us/step - loss: 0.1312 - mae: 0.2775 - val_loss: 0.1314 - val_mae: 0.2756\n",
      "Epoch 29/1500\n",
      "52/52 [==============================] - 0s 760us/step - loss: 0.1302 - mae: 0.2763 - val_loss: 0.1305 - val_mae: 0.2744\n",
      "Epoch 30/1500\n",
      "52/52 [==============================] - 0s 791us/step - loss: 0.1293 - mae: 0.2751 - val_loss: 0.1296 - val_mae: 0.2731\n",
      "Epoch 31/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.1282 - mae: 0.2737 - val_loss: 0.1286 - val_mae: 0.2719\n",
      "Epoch 32/1500\n",
      "52/52 [==============================] - 0s 779us/step - loss: 0.1272 - mae: 0.2722 - val_loss: 0.1277 - val_mae: 0.2704\n",
      "Epoch 33/1500\n",
      "52/52 [==============================] - 0s 772us/step - loss: 0.1261 - mae: 0.2709 - val_loss: 0.1267 - val_mae: 0.2693\n",
      "Epoch 34/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1250 - mae: 0.2693 - val_loss: 0.1258 - val_mae: 0.2678\n",
      "Epoch 35/1500\n",
      "52/52 [==============================] - 0s 756us/step - loss: 0.1241 - mae: 0.2679 - val_loss: 0.1248 - val_mae: 0.2663\n",
      "Epoch 36/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.1230 - mae: 0.2666 - val_loss: 0.1239 - val_mae: 0.2652\n",
      "Epoch 37/1500\n",
      "52/52 [==============================] - 0s 789us/step - loss: 0.1220 - mae: 0.2650 - val_loss: 0.1231 - val_mae: 0.2635\n",
      "Epoch 38/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.1210 - mae: 0.2636 - val_loss: 0.1222 - val_mae: 0.2622\n",
      "Epoch 39/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.1202 - mae: 0.2622 - val_loss: 0.1215 - val_mae: 0.2610\n",
      "Epoch 40/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.1192 - mae: 0.2604 - val_loss: 0.1208 - val_mae: 0.2597\n",
      "Epoch 41/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.1183 - mae: 0.2592 - val_loss: 0.1200 - val_mae: 0.2583\n",
      "Epoch 42/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.1175 - mae: 0.2580 - val_loss: 0.1194 - val_mae: 0.2571\n",
      "Epoch 43/1500\n",
      "52/52 [==============================] - 0s 767us/step - loss: 0.1165 - mae: 0.2564 - val_loss: 0.1186 - val_mae: 0.2558\n",
      "Epoch 44/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.1157 - mae: 0.2549 - val_loss: 0.1181 - val_mae: 0.2546\n",
      "Epoch 45/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.1148 - mae: 0.2536 - val_loss: 0.1175 - val_mae: 0.2534\n",
      "Epoch 46/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.1140 - mae: 0.2522 - val_loss: 0.1169 - val_mae: 0.2521\n",
      "Epoch 47/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.1132 - mae: 0.2510 - val_loss: 0.1163 - val_mae: 0.2511\n",
      "Epoch 48/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.1124 - mae: 0.2497 - val_loss: 0.1158 - val_mae: 0.2502\n",
      "Epoch 49/1500\n",
      "52/52 [==============================] - 0s 783us/step - loss: 0.1117 - mae: 0.2481 - val_loss: 0.1153 - val_mae: 0.2486\n",
      "Epoch 50/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.1108 - mae: 0.2470 - val_loss: 0.1147 - val_mae: 0.2477\n",
      "Epoch 51/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.1100 - mae: 0.2456 - val_loss: 0.1142 - val_mae: 0.2464\n",
      "Epoch 52/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.1092 - mae: 0.2437 - val_loss: 0.1138 - val_mae: 0.2453\n",
      "Epoch 53/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.1084 - mae: 0.2427 - val_loss: 0.1131 - val_mae: 0.2442\n",
      "Epoch 54/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.1076 - mae: 0.2411 - val_loss: 0.1125 - val_mae: 0.2430\n",
      "Epoch 55/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.1068 - mae: 0.2400 - val_loss: 0.1120 - val_mae: 0.2420\n",
      "Epoch 56/1500\n",
      "52/52 [==============================] - 0s 786us/step - loss: 0.1061 - mae: 0.2384 - val_loss: 0.1115 - val_mae: 0.2406\n",
      "Epoch 57/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.1053 - mae: 0.2370 - val_loss: 0.1110 - val_mae: 0.2396\n",
      "Epoch 58/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.1045 - mae: 0.2356 - val_loss: 0.1106 - val_mae: 0.2386\n",
      "Epoch 59/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.1038 - mae: 0.2343 - val_loss: 0.1103 - val_mae: 0.2373\n",
      "Epoch 60/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.1032 - mae: 0.2331 - val_loss: 0.1097 - val_mae: 0.2365\n",
      "Epoch 61/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.1025 - mae: 0.2321 - val_loss: 0.1093 - val_mae: 0.2355\n",
      "Epoch 62/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.1018 - mae: 0.2307 - val_loss: 0.1090 - val_mae: 0.2345\n",
      "Epoch 63/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.1011 - mae: 0.2295 - val_loss: 0.1085 - val_mae: 0.2336\n",
      "Epoch 64/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.1006 - mae: 0.2279 - val_loss: 0.1083 - val_mae: 0.2324\n",
      "Epoch 65/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0999 - mae: 0.2269 - val_loss: 0.1080 - val_mae: 0.2315\n",
      "Epoch 66/1500\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.0993 - mae: 0.2253 - val_loss: 0.1076 - val_mae: 0.2301\n",
      "Epoch 67/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0987 - mae: 0.2244 - val_loss: 0.1072 - val_mae: 0.2298\n",
      "Epoch 68/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0981 - mae: 0.2233 - val_loss: 0.1070 - val_mae: 0.2286\n",
      "Epoch 69/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0976 - mae: 0.2222 - val_loss: 0.1067 - val_mae: 0.2277\n",
      "Epoch 70/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.0971 - mae: 0.2210 - val_loss: 0.1063 - val_mae: 0.2268\n",
      "Epoch 71/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0965 - mae: 0.2200 - val_loss: 0.1061 - val_mae: 0.2258\n",
      "Epoch 72/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0960 - mae: 0.2188 - val_loss: 0.1057 - val_mae: 0.2251\n",
      "Epoch 73/1500\n",
      "52/52 [==============================] - 0s 757us/step - loss: 0.0955 - mae: 0.2176 - val_loss: 0.1053 - val_mae: 0.2243\n",
      "Epoch 74/1500\n",
      "52/52 [==============================] - 0s 752us/step - loss: 0.0951 - mae: 0.2170 - val_loss: 0.1052 - val_mae: 0.2234\n",
      "Epoch 75/1500\n",
      "52/52 [==============================] - 0s 757us/step - loss: 0.0946 - mae: 0.2156 - val_loss: 0.1048 - val_mae: 0.2227\n",
      "Epoch 76/1500\n",
      "52/52 [==============================] - 0s 761us/step - loss: 0.0941 - mae: 0.2150 - val_loss: 0.1046 - val_mae: 0.2217\n",
      "Epoch 77/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0936 - mae: 0.2135 - val_loss: 0.1042 - val_mae: 0.2211\n",
      "Epoch 78/1500\n",
      "52/52 [==============================] - 0s 749us/step - loss: 0.0932 - mae: 0.2126 - val_loss: 0.1040 - val_mae: 0.2202\n",
      "Epoch 79/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0928 - mae: 0.2120 - val_loss: 0.1038 - val_mae: 0.2195\n",
      "Epoch 80/1500\n",
      "52/52 [==============================] - 0s 770us/step - loss: 0.0923 - mae: 0.2110 - val_loss: 0.1035 - val_mae: 0.2189\n",
      "Epoch 81/1500\n",
      "52/52 [==============================] - 0s 755us/step - loss: 0.0919 - mae: 0.2099 - val_loss: 0.1032 - val_mae: 0.2182\n",
      "Epoch 82/1500\n",
      "52/52 [==============================] - 0s 768us/step - loss: 0.0916 - mae: 0.2100 - val_loss: 0.1028 - val_mae: 0.2177\n",
      "Epoch 83/1500\n",
      "52/52 [==============================] - 0s 758us/step - loss: 0.0911 - mae: 0.2081 - val_loss: 0.1030 - val_mae: 0.2166\n",
      "Epoch 84/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0907 - mae: 0.2074 - val_loss: 0.1024 - val_mae: 0.2162\n",
      "Epoch 85/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0903 - mae: 0.2063 - val_loss: 0.1023 - val_mae: 0.2153\n",
      "Epoch 86/1500\n",
      "52/52 [==============================] - 0s 765us/step - loss: 0.0900 - mae: 0.2062 - val_loss: 0.1020 - val_mae: 0.2147\n",
      "Epoch 87/1500\n",
      "52/52 [==============================] - 0s 776us/step - loss: 0.0895 - mae: 0.2049 - val_loss: 0.1016 - val_mae: 0.2144\n",
      "Epoch 88/1500\n",
      "52/52 [==============================] - 0s 760us/step - loss: 0.0891 - mae: 0.2040 - val_loss: 0.1015 - val_mae: 0.2137\n",
      "Epoch 89/1500\n",
      "52/52 [==============================] - 0s 779us/step - loss: 0.0888 - mae: 0.2034 - val_loss: 0.1013 - val_mae: 0.2126\n",
      "Epoch 90/1500\n",
      "52/52 [==============================] - 0s 786us/step - loss: 0.0886 - mae: 0.2033 - val_loss: 0.1010 - val_mae: 0.2125\n",
      "Epoch 91/1500\n",
      "52/52 [==============================] - 0s 771us/step - loss: 0.0881 - mae: 0.2016 - val_loss: 0.1009 - val_mae: 0.2116\n",
      "Epoch 92/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0878 - mae: 0.2011 - val_loss: 0.1005 - val_mae: 0.2112\n",
      "Epoch 93/1500\n",
      "52/52 [==============================] - 0s 772us/step - loss: 0.0874 - mae: 0.2004 - val_loss: 0.1007 - val_mae: 0.2103\n",
      "Epoch 94/1500\n",
      "52/52 [==============================] - 0s 776us/step - loss: 0.0872 - mae: 0.1996 - val_loss: 0.1002 - val_mae: 0.2100\n",
      "Epoch 95/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0868 - mae: 0.1992 - val_loss: 0.0999 - val_mae: 0.2096\n",
      "Epoch 96/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0865 - mae: 0.1983 - val_loss: 0.0997 - val_mae: 0.2091\n",
      "Epoch 97/1500\n",
      "52/52 [==============================] - 0s 789us/step - loss: 0.0862 - mae: 0.1976 - val_loss: 0.0995 - val_mae: 0.2084\n",
      "Epoch 98/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.0858 - mae: 0.1969 - val_loss: 0.0992 - val_mae: 0.2080\n",
      "Epoch 99/1500\n",
      "52/52 [==============================] - 0s 760us/step - loss: 0.0856 - mae: 0.1962 - val_loss: 0.0991 - val_mae: 0.2074\n",
      "Epoch 100/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.0853 - mae: 0.1957 - val_loss: 0.0988 - val_mae: 0.2072\n",
      "Epoch 101/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0849 - mae: 0.1951 - val_loss: 0.0987 - val_mae: 0.2064\n",
      "Epoch 102/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0846 - mae: 0.1942 - val_loss: 0.0985 - val_mae: 0.2059\n",
      "Epoch 103/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0844 - mae: 0.1936 - val_loss: 0.0984 - val_mae: 0.2052\n",
      "Epoch 104/1500\n",
      "52/52 [==============================] - 0s 773us/step - loss: 0.0841 - mae: 0.1930 - val_loss: 0.0981 - val_mae: 0.2048\n",
      "Epoch 105/1500\n",
      "52/52 [==============================] - 0s 770us/step - loss: 0.0838 - mae: 0.1926 - val_loss: 0.0977 - val_mae: 0.2047\n",
      "Epoch 106/1500\n",
      "52/52 [==============================] - 0s 786us/step - loss: 0.0835 - mae: 0.1918 - val_loss: 0.0976 - val_mae: 0.2040\n",
      "Epoch 107/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0833 - mae: 0.1915 - val_loss: 0.0976 - val_mae: 0.2032\n",
      "Epoch 108/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0830 - mae: 0.1906 - val_loss: 0.0971 - val_mae: 0.2035\n",
      "Epoch 109/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0827 - mae: 0.1903 - val_loss: 0.0969 - val_mae: 0.2029\n",
      "Epoch 110/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.0825 - mae: 0.1897 - val_loss: 0.0967 - val_mae: 0.2023\n",
      "Epoch 111/1500\n",
      "52/52 [==============================] - 0s 786us/step - loss: 0.0822 - mae: 0.1891 - val_loss: 0.0966 - val_mae: 0.2020\n",
      "Epoch 112/1500\n",
      "52/52 [==============================] - 0s 769us/step - loss: 0.0820 - mae: 0.1886 - val_loss: 0.0964 - val_mae: 0.2012\n",
      "Epoch 113/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0817 - mae: 0.1879 - val_loss: 0.0962 - val_mae: 0.2008\n",
      "Epoch 114/1500\n",
      "52/52 [==============================] - 0s 765us/step - loss: 0.0815 - mae: 0.1872 - val_loss: 0.0958 - val_mae: 0.2008\n",
      "Epoch 115/1500\n",
      "52/52 [==============================] - 0s 765us/step - loss: 0.0812 - mae: 0.1878 - val_loss: 0.0958 - val_mae: 0.2000\n",
      "Epoch 116/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.0810 - mae: 0.1865 - val_loss: 0.0957 - val_mae: 0.1995\n",
      "Epoch 117/1500\n",
      "52/52 [==============================] - 0s 759us/step - loss: 0.0807 - mae: 0.1861 - val_loss: 0.0952 - val_mae: 0.1995\n",
      "Epoch 118/1500\n",
      "52/52 [==============================] - 0s 758us/step - loss: 0.0804 - mae: 0.1858 - val_loss: 0.0952 - val_mae: 0.1989\n",
      "Epoch 119/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0802 - mae: 0.1851 - val_loss: 0.0949 - val_mae: 0.1985\n",
      "Epoch 120/1500\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.0799 - mae: 0.1846 - val_loss: 0.0949 - val_mae: 0.1980\n",
      "Epoch 121/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0797 - mae: 0.1839 - val_loss: 0.0944 - val_mae: 0.1977\n",
      "Epoch 122/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0794 - mae: 0.1833 - val_loss: 0.0943 - val_mae: 0.1974\n",
      "Epoch 123/1500\n",
      "52/52 [==============================] - 0s 780us/step - loss: 0.0792 - mae: 0.1830 - val_loss: 0.0941 - val_mae: 0.1970\n",
      "Epoch 124/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0790 - mae: 0.1827 - val_loss: 0.0938 - val_mae: 0.1967\n",
      "Epoch 125/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0787 - mae: 0.1824 - val_loss: 0.0937 - val_mae: 0.1962\n",
      "Epoch 126/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0785 - mae: 0.1815 - val_loss: 0.0935 - val_mae: 0.1958\n",
      "Epoch 127/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0783 - mae: 0.1817 - val_loss: 0.0933 - val_mae: 0.1953\n",
      "Epoch 128/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0780 - mae: 0.1806 - val_loss: 0.0933 - val_mae: 0.1949\n",
      "Epoch 129/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0778 - mae: 0.1804 - val_loss: 0.0930 - val_mae: 0.1943\n",
      "Epoch 130/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0775 - mae: 0.1797 - val_loss: 0.0927 - val_mae: 0.1942\n",
      "Epoch 131/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0773 - mae: 0.1793 - val_loss: 0.0925 - val_mae: 0.1941\n",
      "Epoch 132/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0771 - mae: 0.1793 - val_loss: 0.0922 - val_mae: 0.1938\n",
      "Epoch 133/1500\n",
      "52/52 [==============================] - 0s 779us/step - loss: 0.0768 - mae: 0.1787 - val_loss: 0.0921 - val_mae: 0.1932\n",
      "Epoch 134/1500\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.0766 - mae: 0.1781 - val_loss: 0.0920 - val_mae: 0.1926\n",
      "Epoch 135/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.0764 - mae: 0.1777 - val_loss: 0.0917 - val_mae: 0.1923\n",
      "Epoch 136/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0762 - mae: 0.1773 - val_loss: 0.0914 - val_mae: 0.1923\n",
      "Epoch 137/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0760 - mae: 0.1765 - val_loss: 0.0911 - val_mae: 0.1923\n",
      "Epoch 138/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0757 - mae: 0.1771 - val_loss: 0.0911 - val_mae: 0.1913\n",
      "Epoch 139/1500\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.0754 - mae: 0.1758 - val_loss: 0.0911 - val_mae: 0.1908\n",
      "Epoch 140/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0752 - mae: 0.1754 - val_loss: 0.0907 - val_mae: 0.1908\n",
      "Epoch 141/1500\n",
      "52/52 [==============================] - 0s 787us/step - loss: 0.0750 - mae: 0.1751 - val_loss: 0.0904 - val_mae: 0.1905\n",
      "Epoch 142/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0747 - mae: 0.1748 - val_loss: 0.0902 - val_mae: 0.1900\n",
      "Epoch 143/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0745 - mae: 0.1745 - val_loss: 0.0900 - val_mae: 0.1897\n",
      "Epoch 144/1500\n",
      "52/52 [==============================] - 0s 789us/step - loss: 0.0743 - mae: 0.1738 - val_loss: 0.0898 - val_mae: 0.1893\n",
      "Epoch 145/1500\n",
      "52/52 [==============================] - 0s 772us/step - loss: 0.0741 - mae: 0.1737 - val_loss: 0.0896 - val_mae: 0.1889\n",
      "Epoch 146/1500\n",
      "52/52 [==============================] - 0s 768us/step - loss: 0.0738 - mae: 0.1729 - val_loss: 0.0895 - val_mae: 0.1884\n",
      "Epoch 147/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0736 - mae: 0.1726 - val_loss: 0.0892 - val_mae: 0.1885\n",
      "Epoch 148/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0734 - mae: 0.1727 - val_loss: 0.0891 - val_mae: 0.1877\n",
      "Epoch 149/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0731 - mae: 0.1718 - val_loss: 0.0888 - val_mae: 0.1876\n",
      "Epoch 150/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0730 - mae: 0.1712 - val_loss: 0.0888 - val_mae: 0.1867\n",
      "Epoch 151/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0727 - mae: 0.1709 - val_loss: 0.0884 - val_mae: 0.1869\n",
      "Epoch 152/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0725 - mae: 0.1706 - val_loss: 0.0881 - val_mae: 0.1865\n",
      "Epoch 153/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0722 - mae: 0.1705 - val_loss: 0.0879 - val_mae: 0.1864\n",
      "Epoch 154/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0720 - mae: 0.1701 - val_loss: 0.0878 - val_mae: 0.1860\n",
      "Epoch 155/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0718 - mae: 0.1698 - val_loss: 0.0876 - val_mae: 0.1854\n",
      "Epoch 156/1500\n",
      "52/52 [==============================] - 0s 787us/step - loss: 0.0716 - mae: 0.1690 - val_loss: 0.0874 - val_mae: 0.1853\n",
      "Epoch 157/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0714 - mae: 0.1688 - val_loss: 0.0872 - val_mae: 0.1848\n",
      "Epoch 158/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0711 - mae: 0.1677 - val_loss: 0.0869 - val_mae: 0.1847\n",
      "Epoch 159/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0709 - mae: 0.1685 - val_loss: 0.0865 - val_mae: 0.1842\n",
      "Epoch 160/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0707 - mae: 0.1673 - val_loss: 0.0864 - val_mae: 0.1841\n",
      "Epoch 161/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0704 - mae: 0.1672 - val_loss: 0.0862 - val_mae: 0.1834\n",
      "Epoch 162/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0703 - mae: 0.1668 - val_loss: 0.0860 - val_mae: 0.1830\n",
      "Epoch 163/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0700 - mae: 0.1658 - val_loss: 0.0858 - val_mae: 0.1828\n",
      "Epoch 164/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0698 - mae: 0.1662 - val_loss: 0.0856 - val_mae: 0.1824\n",
      "Epoch 165/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0696 - mae: 0.1656 - val_loss: 0.0854 - val_mae: 0.1821\n",
      "Epoch 166/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0693 - mae: 0.1652 - val_loss: 0.0852 - val_mae: 0.1817\n",
      "Epoch 167/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0692 - mae: 0.1642 - val_loss: 0.0850 - val_mae: 0.1810\n",
      "Epoch 168/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0689 - mae: 0.1646 - val_loss: 0.0846 - val_mae: 0.1812\n",
      "Epoch 169/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0687 - mae: 0.1641 - val_loss: 0.0844 - val_mae: 0.1808\n",
      "Epoch 170/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0685 - mae: 0.1640 - val_loss: 0.0843 - val_mae: 0.1802\n",
      "Epoch 171/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.0682 - mae: 0.1627 - val_loss: 0.0841 - val_mae: 0.1798\n",
      "Epoch 172/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0680 - mae: 0.1628 - val_loss: 0.0838 - val_mae: 0.1794\n",
      "Epoch 173/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0678 - mae: 0.1624 - val_loss: 0.0835 - val_mae: 0.1790\n",
      "Epoch 174/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0676 - mae: 0.1623 - val_loss: 0.0834 - val_mae: 0.1788\n",
      "Epoch 175/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0674 - mae: 0.1614 - val_loss: 0.0831 - val_mae: 0.1785\n",
      "Epoch 176/1500\n",
      "52/52 [==============================] - 0s 792us/step - loss: 0.0672 - mae: 0.1616 - val_loss: 0.0830 - val_mae: 0.1779\n",
      "Epoch 177/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0669 - mae: 0.1605 - val_loss: 0.0827 - val_mae: 0.1778\n",
      "Epoch 178/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0667 - mae: 0.1607 - val_loss: 0.0824 - val_mae: 0.1774\n",
      "Epoch 179/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0665 - mae: 0.1604 - val_loss: 0.0823 - val_mae: 0.1771\n",
      "Epoch 180/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0663 - mae: 0.1596 - val_loss: 0.0819 - val_mae: 0.1772\n",
      "Epoch 181/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0661 - mae: 0.1592 - val_loss: 0.0818 - val_mae: 0.1763\n",
      "Epoch 182/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0658 - mae: 0.1595 - val_loss: 0.0816 - val_mae: 0.1761\n",
      "Epoch 183/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0656 - mae: 0.1584 - val_loss: 0.0814 - val_mae: 0.1757\n",
      "Epoch 184/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0654 - mae: 0.1580 - val_loss: 0.0812 - val_mae: 0.1752\n",
      "Epoch 185/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0652 - mae: 0.1580 - val_loss: 0.0809 - val_mae: 0.1747\n",
      "Epoch 186/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0650 - mae: 0.1572 - val_loss: 0.0807 - val_mae: 0.1746\n",
      "Epoch 187/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0648 - mae: 0.1571 - val_loss: 0.0805 - val_mae: 0.1740\n",
      "Epoch 188/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0646 - mae: 0.1567 - val_loss: 0.0802 - val_mae: 0.1741\n",
      "Epoch 189/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0644 - mae: 0.1562 - val_loss: 0.0800 - val_mae: 0.1736\n",
      "Epoch 190/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0641 - mae: 0.1558 - val_loss: 0.0799 - val_mae: 0.1730\n",
      "Epoch 191/1500\n",
      "52/52 [==============================] - 0s 769us/step - loss: 0.0640 - mae: 0.1557 - val_loss: 0.0797 - val_mae: 0.1729\n",
      "Epoch 192/1500\n",
      "52/52 [==============================] - 0s 757us/step - loss: 0.0637 - mae: 0.1554 - val_loss: 0.0794 - val_mae: 0.1728\n",
      "Epoch 193/1500\n",
      "52/52 [==============================] - 0s 769us/step - loss: 0.0636 - mae: 0.1546 - val_loss: 0.0793 - val_mae: 0.1721\n",
      "Epoch 194/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0633 - mae: 0.1542 - val_loss: 0.0788 - val_mae: 0.1720\n",
      "Epoch 195/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0631 - mae: 0.1542 - val_loss: 0.0786 - val_mae: 0.1714\n",
      "Epoch 196/1500\n",
      "52/52 [==============================] - 0s 762us/step - loss: 0.0629 - mae: 0.1536 - val_loss: 0.0784 - val_mae: 0.1709\n",
      "Epoch 197/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.0627 - mae: 0.1530 - val_loss: 0.0783 - val_mae: 0.1704\n",
      "Epoch 198/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0625 - mae: 0.1529 - val_loss: 0.0781 - val_mae: 0.1699\n",
      "Epoch 199/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0623 - mae: 0.1523 - val_loss: 0.0778 - val_mae: 0.1698\n",
      "Epoch 200/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0621 - mae: 0.1521 - val_loss: 0.0776 - val_mae: 0.1695\n",
      "Epoch 201/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0619 - mae: 0.1513 - val_loss: 0.0773 - val_mae: 0.1687\n",
      "Epoch 202/1500\n",
      "52/52 [==============================] - 0s 789us/step - loss: 0.0617 - mae: 0.1511 - val_loss: 0.0772 - val_mae: 0.1684\n",
      "Epoch 203/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0615 - mae: 0.1504 - val_loss: 0.0768 - val_mae: 0.1679\n",
      "Epoch 204/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0612 - mae: 0.1498 - val_loss: 0.0765 - val_mae: 0.1675\n",
      "Epoch 205/1500\n",
      "52/52 [==============================] - 0s 783us/step - loss: 0.0611 - mae: 0.1498 - val_loss: 0.0762 - val_mae: 0.1674\n",
      "Epoch 206/1500\n",
      "52/52 [==============================] - 0s 774us/step - loss: 0.0608 - mae: 0.1489 - val_loss: 0.0761 - val_mae: 0.1665\n",
      "Epoch 207/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.0606 - mae: 0.1489 - val_loss: 0.0760 - val_mae: 0.1663\n",
      "Epoch 208/1500\n",
      "52/52 [==============================] - 0s 778us/step - loss: 0.0605 - mae: 0.1485 - val_loss: 0.0757 - val_mae: 0.1660\n",
      "Epoch 209/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0602 - mae: 0.1476 - val_loss: 0.0755 - val_mae: 0.1654\n",
      "Epoch 210/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0600 - mae: 0.1476 - val_loss: 0.0753 - val_mae: 0.1653\n",
      "Epoch 211/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0598 - mae: 0.1470 - val_loss: 0.0751 - val_mae: 0.1651\n",
      "Epoch 212/1500\n",
      "52/52 [==============================] - 0s 776us/step - loss: 0.0597 - mae: 0.1470 - val_loss: 0.0749 - val_mae: 0.1646\n",
      "Epoch 213/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0595 - mae: 0.1464 - val_loss: 0.0747 - val_mae: 0.1643\n",
      "Epoch 214/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.0593 - mae: 0.1461 - val_loss: 0.0746 - val_mae: 0.1638\n",
      "Epoch 215/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0591 - mae: 0.1460 - val_loss: 0.0744 - val_mae: 0.1636\n",
      "Epoch 216/1500\n",
      "52/52 [==============================] - 0s 792us/step - loss: 0.0589 - mae: 0.1448 - val_loss: 0.0741 - val_mae: 0.1632\n",
      "Epoch 217/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0587 - mae: 0.1449 - val_loss: 0.0738 - val_mae: 0.1631\n",
      "Epoch 218/1500\n",
      "52/52 [==============================] - 0s 787us/step - loss: 0.0586 - mae: 0.1445 - val_loss: 0.0737 - val_mae: 0.1626\n",
      "Epoch 219/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.0583 - mae: 0.1442 - val_loss: 0.0735 - val_mae: 0.1625\n",
      "Epoch 220/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0582 - mae: 0.1443 - val_loss: 0.0733 - val_mae: 0.1620\n",
      "Epoch 221/1500\n",
      "52/52 [==============================] - 0s 773us/step - loss: 0.0580 - mae: 0.1434 - val_loss: 0.0731 - val_mae: 0.1616\n",
      "Epoch 222/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0579 - mae: 0.1433 - val_loss: 0.0726 - val_mae: 0.1616\n",
      "Epoch 223/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.0576 - mae: 0.1429 - val_loss: 0.0727 - val_mae: 0.1612\n",
      "Epoch 224/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0574 - mae: 0.1426 - val_loss: 0.0725 - val_mae: 0.1607\n",
      "Epoch 225/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0573 - mae: 0.1421 - val_loss: 0.0724 - val_mae: 0.1604\n",
      "Epoch 226/1500\n",
      "52/52 [==============================] - 0s 774us/step - loss: 0.0571 - mae: 0.1417 - val_loss: 0.0721 - val_mae: 0.1601\n",
      "Epoch 227/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0570 - mae: 0.1416 - val_loss: 0.0720 - val_mae: 0.1598\n",
      "Epoch 228/1500\n",
      "52/52 [==============================] - 0s 776us/step - loss: 0.0567 - mae: 0.1411 - val_loss: 0.0716 - val_mae: 0.1595\n",
      "Epoch 229/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0566 - mae: 0.1410 - val_loss: 0.0714 - val_mae: 0.1593\n",
      "Epoch 230/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0564 - mae: 0.1408 - val_loss: 0.0716 - val_mae: 0.1590\n",
      "Epoch 231/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0563 - mae: 0.1403 - val_loss: 0.0711 - val_mae: 0.1587\n",
      "Epoch 232/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0561 - mae: 0.1397 - val_loss: 0.0710 - val_mae: 0.1584\n",
      "Epoch 233/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0559 - mae: 0.1396 - val_loss: 0.0705 - val_mae: 0.1580\n",
      "Epoch 234/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0557 - mae: 0.1392 - val_loss: 0.0706 - val_mae: 0.1578\n",
      "Epoch 235/1500\n",
      "52/52 [==============================] - 0s 950us/step - loss: 0.0556 - mae: 0.1386 - val_loss: 0.0704 - val_mae: 0.1574\n",
      "Epoch 236/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0555 - mae: 0.1390 - val_loss: 0.0702 - val_mae: 0.1572\n",
      "Epoch 237/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0553 - mae: 0.1384 - val_loss: 0.0700 - val_mae: 0.1569\n",
      "Epoch 238/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0551 - mae: 0.1379 - val_loss: 0.0696 - val_mae: 0.1567\n",
      "Epoch 239/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0549 - mae: 0.1377 - val_loss: 0.0697 - val_mae: 0.1565\n",
      "Epoch 240/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0548 - mae: 0.1374 - val_loss: 0.0693 - val_mae: 0.1562\n",
      "Epoch 241/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0546 - mae: 0.1372 - val_loss: 0.0691 - val_mae: 0.1559\n",
      "Epoch 242/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0544 - mae: 0.1368 - val_loss: 0.0689 - val_mae: 0.1556\n",
      "Epoch 243/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0543 - mae: 0.1369 - val_loss: 0.0690 - val_mae: 0.1554\n",
      "Epoch 244/1500\n",
      "52/52 [==============================] - 0s 783us/step - loss: 0.0542 - mae: 0.1362 - val_loss: 0.0685 - val_mae: 0.1550\n",
      "Epoch 245/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0539 - mae: 0.1359 - val_loss: 0.0685 - val_mae: 0.1547\n",
      "Epoch 246/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0538 - mae: 0.1354 - val_loss: 0.0682 - val_mae: 0.1546\n",
      "Epoch 247/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0537 - mae: 0.1352 - val_loss: 0.0679 - val_mae: 0.1543\n",
      "Epoch 248/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0535 - mae: 0.1354 - val_loss: 0.0681 - val_mae: 0.1540\n",
      "Epoch 249/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0533 - mae: 0.1347 - val_loss: 0.0677 - val_mae: 0.1537\n",
      "Epoch 250/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0531 - mae: 0.1343 - val_loss: 0.0675 - val_mae: 0.1536\n",
      "Epoch 251/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0530 - mae: 0.1344 - val_loss: 0.0676 - val_mae: 0.1532\n",
      "Epoch 252/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0529 - mae: 0.1340 - val_loss: 0.0670 - val_mae: 0.1531\n",
      "Epoch 253/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0527 - mae: 0.1337 - val_loss: 0.0672 - val_mae: 0.1526\n",
      "Epoch 254/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0526 - mae: 0.1336 - val_loss: 0.0668 - val_mae: 0.1524\n",
      "Epoch 255/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0524 - mae: 0.1331 - val_loss: 0.0667 - val_mae: 0.1521\n",
      "Epoch 256/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0522 - mae: 0.1328 - val_loss: 0.0665 - val_mae: 0.1518\n",
      "Epoch 257/1500\n",
      "52/52 [==============================] - 0s 790us/step - loss: 0.0521 - mae: 0.1325 - val_loss: 0.0664 - val_mae: 0.1515\n",
      "Epoch 258/1500\n",
      "52/52 [==============================] - 0s 768us/step - loss: 0.0519 - mae: 0.1323 - val_loss: 0.0662 - val_mae: 0.1513\n",
      "Epoch 259/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0518 - mae: 0.1320 - val_loss: 0.0659 - val_mae: 0.1510\n",
      "Epoch 260/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0516 - mae: 0.1318 - val_loss: 0.0660 - val_mae: 0.1508\n",
      "Epoch 261/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0515 - mae: 0.1315 - val_loss: 0.0656 - val_mae: 0.1506\n",
      "Epoch 262/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0513 - mae: 0.1312 - val_loss: 0.0655 - val_mae: 0.1503\n",
      "Epoch 263/1500\n",
      "52/52 [==============================] - 0s 791us/step - loss: 0.0512 - mae: 0.1310 - val_loss: 0.0654 - val_mae: 0.1500\n",
      "Epoch 264/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0510 - mae: 0.1307 - val_loss: 0.0651 - val_mae: 0.1498\n",
      "Epoch 265/1500\n",
      "52/52 [==============================] - 0s 792us/step - loss: 0.0509 - mae: 0.1306 - val_loss: 0.0650 - val_mae: 0.1494\n",
      "Epoch 266/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0507 - mae: 0.1302 - val_loss: 0.0650 - val_mae: 0.1492\n",
      "Epoch 267/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0506 - mae: 0.1300 - val_loss: 0.0647 - val_mae: 0.1490\n",
      "Epoch 268/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0504 - mae: 0.1294 - val_loss: 0.0646 - val_mae: 0.1486\n",
      "Epoch 269/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0503 - mae: 0.1293 - val_loss: 0.0644 - val_mae: 0.1485\n",
      "Epoch 270/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0502 - mae: 0.1293 - val_loss: 0.0642 - val_mae: 0.1482\n",
      "Epoch 271/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0500 - mae: 0.1287 - val_loss: 0.0642 - val_mae: 0.1479\n",
      "Epoch 272/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0498 - mae: 0.1283 - val_loss: 0.0639 - val_mae: 0.1477\n",
      "Epoch 273/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0497 - mae: 0.1283 - val_loss: 0.0638 - val_mae: 0.1476\n",
      "Epoch 274/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0497 - mae: 0.1280 - val_loss: 0.0636 - val_mae: 0.1472\n",
      "Epoch 275/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0494 - mae: 0.1279 - val_loss: 0.0635 - val_mae: 0.1472\n",
      "Epoch 276/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0493 - mae: 0.1277 - val_loss: 0.0634 - val_mae: 0.1468\n",
      "Epoch 277/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0491 - mae: 0.1272 - val_loss: 0.0632 - val_mae: 0.1465\n",
      "Epoch 278/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0490 - mae: 0.1269 - val_loss: 0.0631 - val_mae: 0.1463\n",
      "Epoch 279/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0489 - mae: 0.1269 - val_loss: 0.0629 - val_mae: 0.1461\n",
      "Epoch 280/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0487 - mae: 0.1267 - val_loss: 0.0629 - val_mae: 0.1458\n",
      "Epoch 281/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0486 - mae: 0.1263 - val_loss: 0.0628 - val_mae: 0.1455\n",
      "Epoch 282/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0485 - mae: 0.1260 - val_loss: 0.0625 - val_mae: 0.1455\n",
      "Epoch 283/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0483 - mae: 0.1260 - val_loss: 0.0625 - val_mae: 0.1451\n",
      "Epoch 284/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0481 - mae: 0.1255 - val_loss: 0.0623 - val_mae: 0.1447\n",
      "Epoch 285/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0480 - mae: 0.1252 - val_loss: 0.0621 - val_mae: 0.1446\n",
      "Epoch 286/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0480 - mae: 0.1250 - val_loss: 0.0620 - val_mae: 0.1445\n",
      "Epoch 287/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0477 - mae: 0.1251 - val_loss: 0.0620 - val_mae: 0.1442\n",
      "Epoch 288/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0476 - mae: 0.1244 - val_loss: 0.0618 - val_mae: 0.1439\n",
      "Epoch 289/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0475 - mae: 0.1245 - val_loss: 0.0616 - val_mae: 0.1437\n",
      "Epoch 290/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0473 - mae: 0.1241 - val_loss: 0.0615 - val_mae: 0.1434\n",
      "Epoch 291/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0472 - mae: 0.1236 - val_loss: 0.0613 - val_mae: 0.1431\n",
      "Epoch 292/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0471 - mae: 0.1239 - val_loss: 0.0612 - val_mae: 0.1429\n",
      "Epoch 293/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0469 - mae: 0.1233 - val_loss: 0.0611 - val_mae: 0.1426\n",
      "Epoch 294/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0468 - mae: 0.1230 - val_loss: 0.0610 - val_mae: 0.1424\n",
      "Epoch 295/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0466 - mae: 0.1228 - val_loss: 0.0608 - val_mae: 0.1423\n",
      "Epoch 296/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0465 - mae: 0.1226 - val_loss: 0.0607 - val_mae: 0.1420\n",
      "Epoch 297/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0464 - mae: 0.1225 - val_loss: 0.0606 - val_mae: 0.1418\n",
      "Epoch 298/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0463 - mae: 0.1221 - val_loss: 0.0604 - val_mae: 0.1416\n",
      "Epoch 299/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0462 - mae: 0.1218 - val_loss: 0.0603 - val_mae: 0.1414\n",
      "Epoch 300/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0460 - mae: 0.1217 - val_loss: 0.0601 - val_mae: 0.1411\n",
      "Epoch 301/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0458 - mae: 0.1213 - val_loss: 0.0600 - val_mae: 0.1408\n",
      "Epoch 302/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0457 - mae: 0.1213 - val_loss: 0.0598 - val_mae: 0.1406\n",
      "Epoch 303/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0456 - mae: 0.1209 - val_loss: 0.0598 - val_mae: 0.1402\n",
      "Epoch 304/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0455 - mae: 0.1208 - val_loss: 0.0596 - val_mae: 0.1401\n",
      "Epoch 305/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0453 - mae: 0.1205 - val_loss: 0.0596 - val_mae: 0.1399\n",
      "Epoch 306/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0452 - mae: 0.1201 - val_loss: 0.0594 - val_mae: 0.1396\n",
      "Epoch 307/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0451 - mae: 0.1200 - val_loss: 0.0594 - val_mae: 0.1393\n",
      "Epoch 308/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0450 - mae: 0.1198 - val_loss: 0.0592 - val_mae: 0.1391\n",
      "Epoch 309/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0448 - mae: 0.1194 - val_loss: 0.0590 - val_mae: 0.1390\n",
      "Epoch 310/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0448 - mae: 0.1194 - val_loss: 0.0588 - val_mae: 0.1389\n",
      "Epoch 311/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0446 - mae: 0.1192 - val_loss: 0.0587 - val_mae: 0.1385\n",
      "Epoch 312/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0445 - mae: 0.1188 - val_loss: 0.0586 - val_mae: 0.1383\n",
      "Epoch 313/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0444 - mae: 0.1186 - val_loss: 0.0587 - val_mae: 0.1380\n",
      "Epoch 314/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0442 - mae: 0.1184 - val_loss: 0.0583 - val_mae: 0.1380\n",
      "Epoch 315/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0441 - mae: 0.1182 - val_loss: 0.0584 - val_mae: 0.1375\n",
      "Epoch 316/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0439 - mae: 0.1179 - val_loss: 0.0583 - val_mae: 0.1374\n",
      "Epoch 317/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0438 - mae: 0.1177 - val_loss: 0.0581 - val_mae: 0.1371\n",
      "Epoch 318/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0437 - mae: 0.1176 - val_loss: 0.0578 - val_mae: 0.1369\n",
      "Epoch 319/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0435 - mae: 0.1172 - val_loss: 0.0577 - val_mae: 0.1366\n",
      "Epoch 320/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.0434 - mae: 0.1170 - val_loss: 0.0577 - val_mae: 0.1364\n",
      "Epoch 321/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0433 - mae: 0.1166 - val_loss: 0.0574 - val_mae: 0.1364\n",
      "Epoch 322/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0432 - mae: 0.1168 - val_loss: 0.0574 - val_mae: 0.1359\n",
      "Epoch 323/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0431 - mae: 0.1164 - val_loss: 0.0573 - val_mae: 0.1357\n",
      "Epoch 324/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0430 - mae: 0.1160 - val_loss: 0.0572 - val_mae: 0.1354\n",
      "Epoch 325/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0428 - mae: 0.1156 - val_loss: 0.0570 - val_mae: 0.1352\n",
      "Epoch 326/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0427 - mae: 0.1155 - val_loss: 0.0569 - val_mae: 0.1350\n",
      "Epoch 327/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0426 - mae: 0.1155 - val_loss: 0.0567 - val_mae: 0.1349\n",
      "Epoch 328/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0424 - mae: 0.1154 - val_loss: 0.0568 - val_mae: 0.1346\n",
      "Epoch 329/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0423 - mae: 0.1150 - val_loss: 0.0566 - val_mae: 0.1344\n",
      "Epoch 330/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0422 - mae: 0.1146 - val_loss: 0.0564 - val_mae: 0.1342\n",
      "Epoch 331/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0421 - mae: 0.1144 - val_loss: 0.0562 - val_mae: 0.1340\n",
      "Epoch 332/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0419 - mae: 0.1144 - val_loss: 0.0562 - val_mae: 0.1337\n",
      "Epoch 333/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0418 - mae: 0.1142 - val_loss: 0.0561 - val_mae: 0.1335\n",
      "Epoch 334/1500\n",
      "52/52 [==============================] - 0s 775us/step - loss: 0.0417 - mae: 0.1138 - val_loss: 0.0559 - val_mae: 0.1333\n",
      "Epoch 335/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0417 - mae: 0.1139 - val_loss: 0.0559 - val_mae: 0.1330\n",
      "Epoch 336/1500\n",
      "52/52 [==============================] - 0s 918us/step - loss: 0.0415 - mae: 0.1134 - val_loss: 0.0557 - val_mae: 0.1328\n",
      "Epoch 337/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0413 - mae: 0.1130 - val_loss: 0.0555 - val_mae: 0.1325\n",
      "Epoch 338/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0412 - mae: 0.1131 - val_loss: 0.0555 - val_mae: 0.1323\n",
      "Epoch 339/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0411 - mae: 0.1127 - val_loss: 0.0555 - val_mae: 0.1320\n",
      "Epoch 340/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0410 - mae: 0.1123 - val_loss: 0.0552 - val_mae: 0.1319\n",
      "Epoch 341/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0409 - mae: 0.1120 - val_loss: 0.0550 - val_mae: 0.1317\n",
      "Epoch 342/1500\n",
      "52/52 [==============================] - 0s 784us/step - loss: 0.0408 - mae: 0.1121 - val_loss: 0.0549 - val_mae: 0.1315\n",
      "Epoch 343/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0406 - mae: 0.1118 - val_loss: 0.0548 - val_mae: 0.1312\n",
      "Epoch 344/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0405 - mae: 0.1115 - val_loss: 0.0547 - val_mae: 0.1310\n",
      "Epoch 345/1500\n",
      "52/52 [==============================] - 0s 782us/step - loss: 0.0404 - mae: 0.1112 - val_loss: 0.0546 - val_mae: 0.1307\n",
      "Epoch 346/1500\n",
      "52/52 [==============================] - 0s 792us/step - loss: 0.0403 - mae: 0.1110 - val_loss: 0.0544 - val_mae: 0.1306\n",
      "Epoch 347/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0401 - mae: 0.1108 - val_loss: 0.0544 - val_mae: 0.1304\n",
      "Epoch 348/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0400 - mae: 0.1109 - val_loss: 0.0542 - val_mae: 0.1301\n",
      "Epoch 349/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0400 - mae: 0.1104 - val_loss: 0.0541 - val_mae: 0.1299\n",
      "Epoch 350/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0398 - mae: 0.1102 - val_loss: 0.0540 - val_mae: 0.1298\n",
      "Epoch 351/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0397 - mae: 0.1101 - val_loss: 0.0539 - val_mae: 0.1294\n",
      "Epoch 352/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0396 - mae: 0.1099 - val_loss: 0.0539 - val_mae: 0.1293\n",
      "Epoch 353/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0395 - mae: 0.1096 - val_loss: 0.0536 - val_mae: 0.1291\n",
      "Epoch 354/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0394 - mae: 0.1094 - val_loss: 0.0536 - val_mae: 0.1288\n",
      "Epoch 355/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0392 - mae: 0.1092 - val_loss: 0.0533 - val_mae: 0.1286\n",
      "Epoch 356/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0391 - mae: 0.1087 - val_loss: 0.0531 - val_mae: 0.1284\n",
      "Epoch 357/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0390 - mae: 0.1088 - val_loss: 0.0530 - val_mae: 0.1282\n",
      "Epoch 358/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0389 - mae: 0.1088 - val_loss: 0.0529 - val_mae: 0.1279\n",
      "Epoch 359/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0387 - mae: 0.1083 - val_loss: 0.0529 - val_mae: 0.1276\n",
      "Epoch 360/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0387 - mae: 0.1082 - val_loss: 0.0528 - val_mae: 0.1274\n",
      "Epoch 361/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0385 - mae: 0.1077 - val_loss: 0.0527 - val_mae: 0.1272\n",
      "Epoch 362/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0384 - mae: 0.1076 - val_loss: 0.0525 - val_mae: 0.1270\n",
      "Epoch 363/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0383 - mae: 0.1074 - val_loss: 0.0524 - val_mae: 0.1268\n",
      "Epoch 364/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0382 - mae: 0.1074 - val_loss: 0.0523 - val_mae: 0.1265\n",
      "Epoch 365/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0382 - mae: 0.1068 - val_loss: 0.0522 - val_mae: 0.1263\n",
      "Epoch 366/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0380 - mae: 0.1068 - val_loss: 0.0520 - val_mae: 0.1261\n",
      "Epoch 367/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0379 - mae: 0.1065 - val_loss: 0.0519 - val_mae: 0.1258\n",
      "Epoch 368/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0378 - mae: 0.1063 - val_loss: 0.0518 - val_mae: 0.1257\n",
      "Epoch 369/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0376 - mae: 0.1061 - val_loss: 0.0516 - val_mae: 0.1256\n",
      "Epoch 370/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0375 - mae: 0.1061 - val_loss: 0.0515 - val_mae: 0.1253\n",
      "Epoch 371/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0374 - mae: 0.1057 - val_loss: 0.0514 - val_mae: 0.1251\n",
      "Epoch 372/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0373 - mae: 0.1055 - val_loss: 0.0514 - val_mae: 0.1248\n",
      "Epoch 373/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0372 - mae: 0.1053 - val_loss: 0.0512 - val_mae: 0.1246\n",
      "Epoch 374/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0371 - mae: 0.1051 - val_loss: 0.0510 - val_mae: 0.1244\n",
      "Epoch 375/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0370 - mae: 0.1047 - val_loss: 0.0509 - val_mae: 0.1241\n",
      "Epoch 376/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0369 - mae: 0.1046 - val_loss: 0.0508 - val_mae: 0.1240\n",
      "Epoch 377/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0368 - mae: 0.1050 - val_loss: 0.0508 - val_mae: 0.1238\n",
      "Epoch 378/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0367 - mae: 0.1041 - val_loss: 0.0508 - val_mae: 0.1234\n",
      "Epoch 379/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0365 - mae: 0.1038 - val_loss: 0.0505 - val_mae: 0.1233\n",
      "Epoch 380/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0364 - mae: 0.1039 - val_loss: 0.0505 - val_mae: 0.1231\n",
      "Epoch 381/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0363 - mae: 0.1035 - val_loss: 0.0502 - val_mae: 0.1228\n",
      "Epoch 382/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0362 - mae: 0.1034 - val_loss: 0.0502 - val_mae: 0.1227\n",
      "Epoch 383/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0361 - mae: 0.1035 - val_loss: 0.0499 - val_mae: 0.1225\n",
      "Epoch 384/1500\n",
      "52/52 [==============================] - 0s 781us/step - loss: 0.0359 - mae: 0.1030 - val_loss: 0.0499 - val_mae: 0.1222\n",
      "Epoch 385/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0359 - mae: 0.1027 - val_loss: 0.0498 - val_mae: 0.1219\n",
      "Epoch 386/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0358 - mae: 0.1026 - val_loss: 0.0498 - val_mae: 0.1217\n",
      "Epoch 387/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0356 - mae: 0.1022 - val_loss: 0.0496 - val_mae: 0.1214\n",
      "Epoch 388/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0355 - mae: 0.1021 - val_loss: 0.0495 - val_mae: 0.1213\n",
      "Epoch 389/1500\n",
      "52/52 [==============================] - 0s 786us/step - loss: 0.0354 - mae: 0.1020 - val_loss: 0.0493 - val_mae: 0.1212\n",
      "Epoch 390/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0353 - mae: 0.1016 - val_loss: 0.0493 - val_mae: 0.1208\n",
      "Epoch 391/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0353 - mae: 0.1018 - val_loss: 0.0492 - val_mae: 0.1206\n",
      "Epoch 392/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0351 - mae: 0.1012 - val_loss: 0.0490 - val_mae: 0.1204\n",
      "Epoch 393/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0350 - mae: 0.1008 - val_loss: 0.0489 - val_mae: 0.1202\n",
      "Epoch 394/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0349 - mae: 0.1009 - val_loss: 0.0487 - val_mae: 0.1200\n",
      "Epoch 395/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0348 - mae: 0.1005 - val_loss: 0.0487 - val_mae: 0.1198\n",
      "Epoch 396/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0347 - mae: 0.1006 - val_loss: 0.0484 - val_mae: 0.1196\n",
      "Epoch 397/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0345 - mae: 0.1002 - val_loss: 0.0485 - val_mae: 0.1193\n",
      "Epoch 398/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0344 - mae: 0.1000 - val_loss: 0.0483 - val_mae: 0.1191\n",
      "Epoch 399/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0343 - mae: 0.0998 - val_loss: 0.0483 - val_mae: 0.1189\n",
      "Epoch 400/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0342 - mae: 0.0995 - val_loss: 0.0483 - val_mae: 0.1188\n",
      "Epoch 401/1500\n",
      "52/52 [==============================] - 0s 788us/step - loss: 0.0342 - mae: 0.0996 - val_loss: 0.0481 - val_mae: 0.1186\n",
      "Epoch 402/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0340 - mae: 0.0989 - val_loss: 0.0478 - val_mae: 0.1183\n",
      "Epoch 403/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0339 - mae: 0.0992 - val_loss: 0.0477 - val_mae: 0.1181\n",
      "Epoch 404/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0338 - mae: 0.0988 - val_loss: 0.0476 - val_mae: 0.1179\n",
      "Epoch 405/1500\n",
      "52/52 [==============================] - 0s 777us/step - loss: 0.0337 - mae: 0.0985 - val_loss: 0.0474 - val_mae: 0.1177\n",
      "Epoch 406/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0336 - mae: 0.0983 - val_loss: 0.0472 - val_mae: 0.1176\n",
      "Epoch 407/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0335 - mae: 0.0984 - val_loss: 0.0473 - val_mae: 0.1172\n",
      "Epoch 408/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0333 - mae: 0.0981 - val_loss: 0.0470 - val_mae: 0.1171\n",
      "Epoch 409/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0333 - mae: 0.0978 - val_loss: 0.0471 - val_mae: 0.1167\n",
      "Epoch 410/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0331 - mae: 0.0976 - val_loss: 0.0470 - val_mae: 0.1166\n",
      "Epoch 411/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0330 - mae: 0.0972 - val_loss: 0.0467 - val_mae: 0.1163\n",
      "Epoch 412/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0329 - mae: 0.0970 - val_loss: 0.0466 - val_mae: 0.1160\n",
      "Epoch 413/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0327 - mae: 0.0969 - val_loss: 0.0465 - val_mae: 0.1159\n",
      "Epoch 414/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0326 - mae: 0.0965 - val_loss: 0.0465 - val_mae: 0.1155\n",
      "Epoch 415/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0325 - mae: 0.0965 - val_loss: 0.0463 - val_mae: 0.1154\n",
      "Epoch 416/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0324 - mae: 0.0959 - val_loss: 0.0462 - val_mae: 0.1151\n",
      "Epoch 417/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0323 - mae: 0.0958 - val_loss: 0.0460 - val_mae: 0.1149\n",
      "Epoch 418/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0322 - mae: 0.0960 - val_loss: 0.0460 - val_mae: 0.1147\n",
      "Epoch 419/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0321 - mae: 0.0953 - val_loss: 0.0459 - val_mae: 0.1144\n",
      "Epoch 420/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0320 - mae: 0.0951 - val_loss: 0.0456 - val_mae: 0.1143\n",
      "Epoch 421/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0319 - mae: 0.0950 - val_loss: 0.0456 - val_mae: 0.1140\n",
      "Epoch 422/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0318 - mae: 0.0948 - val_loss: 0.0454 - val_mae: 0.1138\n",
      "Epoch 423/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0317 - mae: 0.0948 - val_loss: 0.0454 - val_mae: 0.1135\n",
      "Epoch 424/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0315 - mae: 0.0944 - val_loss: 0.0454 - val_mae: 0.1133\n",
      "Epoch 425/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0314 - mae: 0.0940 - val_loss: 0.0452 - val_mae: 0.1131\n",
      "Epoch 426/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0314 - mae: 0.0939 - val_loss: 0.0450 - val_mae: 0.1129\n",
      "Epoch 427/1500\n",
      "52/52 [==============================] - 0s 896us/step - loss: 0.0313 - mae: 0.0939 - val_loss: 0.0449 - val_mae: 0.1127\n",
      "Epoch 428/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0311 - mae: 0.0938 - val_loss: 0.0449 - val_mae: 0.1124\n",
      "Epoch 429/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0310 - mae: 0.0933 - val_loss: 0.0448 - val_mae: 0.1122\n",
      "Epoch 430/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0309 - mae: 0.0930 - val_loss: 0.0446 - val_mae: 0.1119\n",
      "Epoch 431/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0308 - mae: 0.0927 - val_loss: 0.0445 - val_mae: 0.1117\n",
      "Epoch 432/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0307 - mae: 0.0928 - val_loss: 0.0443 - val_mae: 0.1115\n",
      "Epoch 433/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0306 - mae: 0.0923 - val_loss: 0.0443 - val_mae: 0.1114\n",
      "Epoch 434/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0306 - mae: 0.0926 - val_loss: 0.0442 - val_mae: 0.1111\n",
      "Epoch 435/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0304 - mae: 0.0918 - val_loss: 0.0441 - val_mae: 0.1108\n",
      "Epoch 436/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0303 - mae: 0.0917 - val_loss: 0.0439 - val_mae: 0.1107\n",
      "Epoch 437/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0303 - mae: 0.0914 - val_loss: 0.0438 - val_mae: 0.1104\n",
      "Epoch 438/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0301 - mae: 0.0916 - val_loss: 0.0436 - val_mae: 0.1101\n",
      "Epoch 439/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0300 - mae: 0.0913 - val_loss: 0.0436 - val_mae: 0.1100\n",
      "Epoch 440/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0299 - mae: 0.0910 - val_loss: 0.0435 - val_mae: 0.1096\n",
      "Epoch 441/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0298 - mae: 0.0907 - val_loss: 0.0434 - val_mae: 0.1095\n",
      "Epoch 442/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0298 - mae: 0.0904 - val_loss: 0.0431 - val_mae: 0.1093\n",
      "Epoch 443/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0296 - mae: 0.0904 - val_loss: 0.0431 - val_mae: 0.1091\n",
      "Epoch 444/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0295 - mae: 0.0900 - val_loss: 0.0430 - val_mae: 0.1088\n",
      "Epoch 445/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0294 - mae: 0.0900 - val_loss: 0.0429 - val_mae: 0.1085\n",
      "Epoch 446/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0293 - mae: 0.0896 - val_loss: 0.0427 - val_mae: 0.1083\n",
      "Epoch 447/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0292 - mae: 0.0894 - val_loss: 0.0426 - val_mae: 0.1081\n",
      "Epoch 448/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0291 - mae: 0.0895 - val_loss: 0.0426 - val_mae: 0.1079\n",
      "Epoch 449/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0290 - mae: 0.0890 - val_loss: 0.0425 - val_mae: 0.1077\n",
      "Epoch 450/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0289 - mae: 0.0886 - val_loss: 0.0423 - val_mae: 0.1075\n",
      "Epoch 451/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0288 - mae: 0.0887 - val_loss: 0.0421 - val_mae: 0.1072\n",
      "Epoch 452/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0287 - mae: 0.0885 - val_loss: 0.0421 - val_mae: 0.1069\n",
      "Epoch 453/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0286 - mae: 0.0883 - val_loss: 0.0420 - val_mae: 0.1067\n",
      "Epoch 454/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0285 - mae: 0.0878 - val_loss: 0.0419 - val_mae: 0.1065\n",
      "Epoch 455/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0284 - mae: 0.0877 - val_loss: 0.0418 - val_mae: 0.1062\n",
      "Epoch 456/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0283 - mae: 0.0879 - val_loss: 0.0417 - val_mae: 0.1061\n",
      "Epoch 457/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0283 - mae: 0.0872 - val_loss: 0.0415 - val_mae: 0.1058\n",
      "Epoch 458/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0282 - mae: 0.0872 - val_loss: 0.0415 - val_mae: 0.1055\n",
      "Epoch 459/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0281 - mae: 0.0873 - val_loss: 0.0414 - val_mae: 0.1054\n",
      "Epoch 460/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0280 - mae: 0.0870 - val_loss: 0.0412 - val_mae: 0.1051\n",
      "Epoch 461/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0278 - mae: 0.0866 - val_loss: 0.0413 - val_mae: 0.1049\n",
      "Epoch 462/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.0277 - mae: 0.0862 - val_loss: 0.0411 - val_mae: 0.1047\n",
      "Epoch 463/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0277 - mae: 0.0859 - val_loss: 0.0409 - val_mae: 0.1044\n",
      "Epoch 464/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0275 - mae: 0.0859 - val_loss: 0.0409 - val_mae: 0.1043\n",
      "Epoch 465/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0275 - mae: 0.0857 - val_loss: 0.0407 - val_mae: 0.1040\n",
      "Epoch 466/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0275 - mae: 0.0854 - val_loss: 0.0406 - val_mae: 0.1038\n",
      "Epoch 467/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0274 - mae: 0.0859 - val_loss: 0.0407 - val_mae: 0.1038\n",
      "Epoch 468/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0272 - mae: 0.0850 - val_loss: 0.0404 - val_mae: 0.1034\n",
      "Epoch 469/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0271 - mae: 0.0849 - val_loss: 0.0403 - val_mae: 0.1031\n",
      "Epoch 470/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0271 - mae: 0.0849 - val_loss: 0.0403 - val_mae: 0.1030\n",
      "Epoch 471/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0269 - mae: 0.0845 - val_loss: 0.0401 - val_mae: 0.1027\n",
      "Epoch 472/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0268 - mae: 0.0843 - val_loss: 0.0400 - val_mae: 0.1025\n",
      "Epoch 473/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0267 - mae: 0.0841 - val_loss: 0.0400 - val_mae: 0.1024\n",
      "Epoch 474/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.0266 - mae: 0.0840 - val_loss: 0.0398 - val_mae: 0.1021\n",
      "Epoch 475/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0266 - mae: 0.0836 - val_loss: 0.0399 - val_mae: 0.1018\n",
      "Epoch 476/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0265 - mae: 0.0835 - val_loss: 0.0397 - val_mae: 0.1017\n",
      "Epoch 477/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0264 - mae: 0.0836 - val_loss: 0.0395 - val_mae: 0.1016\n",
      "Epoch 478/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0263 - mae: 0.0833 - val_loss: 0.0395 - val_mae: 0.1011\n",
      "Epoch 479/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0262 - mae: 0.0827 - val_loss: 0.0394 - val_mae: 0.1008\n",
      "Epoch 480/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0261 - mae: 0.0826 - val_loss: 0.0393 - val_mae: 0.1007\n",
      "Epoch 481/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0261 - mae: 0.0827 - val_loss: 0.0392 - val_mae: 0.1006\n",
      "Epoch 482/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0260 - mae: 0.0824 - val_loss: 0.0390 - val_mae: 0.1003\n",
      "Epoch 483/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0259 - mae: 0.0823 - val_loss: 0.0388 - val_mae: 0.1000\n",
      "Epoch 484/1500\n",
      "52/52 [==============================] - 0s 881us/step - loss: 0.0258 - mae: 0.0820 - val_loss: 0.0388 - val_mae: 0.0997\n",
      "Epoch 485/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0257 - mae: 0.0818 - val_loss: 0.0388 - val_mae: 0.0996\n",
      "Epoch 486/1500\n",
      "52/52 [==============================] - 0s 792us/step - loss: 0.0256 - mae: 0.0814 - val_loss: 0.0386 - val_mae: 0.0994\n",
      "Epoch 487/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0255 - mae: 0.0813 - val_loss: 0.0384 - val_mae: 0.0991\n",
      "Epoch 488/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0254 - mae: 0.0813 - val_loss: 0.0385 - val_mae: 0.0991\n",
      "Epoch 489/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0254 - mae: 0.0809 - val_loss: 0.0383 - val_mae: 0.0987\n",
      "Epoch 490/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0253 - mae: 0.0808 - val_loss: 0.0382 - val_mae: 0.0985\n",
      "Epoch 491/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0252 - mae: 0.0807 - val_loss: 0.0382 - val_mae: 0.0983\n",
      "Epoch 492/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0251 - mae: 0.0804 - val_loss: 0.0380 - val_mae: 0.0980\n",
      "Epoch 493/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0250 - mae: 0.0802 - val_loss: 0.0379 - val_mae: 0.0978\n",
      "Epoch 494/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0250 - mae: 0.0800 - val_loss: 0.0378 - val_mae: 0.0975\n",
      "Epoch 495/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0248 - mae: 0.0801 - val_loss: 0.0377 - val_mae: 0.0975\n",
      "Epoch 496/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0247 - mae: 0.0797 - val_loss: 0.0376 - val_mae: 0.0972\n",
      "Epoch 497/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0246 - mae: 0.0794 - val_loss: 0.0376 - val_mae: 0.0971\n",
      "Epoch 498/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0246 - mae: 0.0791 - val_loss: 0.0375 - val_mae: 0.0969\n",
      "Epoch 499/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0245 - mae: 0.0793 - val_loss: 0.0375 - val_mae: 0.0968\n",
      "Epoch 500/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0245 - mae: 0.0789 - val_loss: 0.0373 - val_mae: 0.0965\n",
      "Epoch 501/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0243 - mae: 0.0789 - val_loss: 0.0372 - val_mae: 0.0964\n",
      "Epoch 502/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0243 - mae: 0.0786 - val_loss: 0.0371 - val_mae: 0.0960\n",
      "Epoch 503/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0242 - mae: 0.0783 - val_loss: 0.0370 - val_mae: 0.0958\n",
      "Epoch 504/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0242 - mae: 0.0786 - val_loss: 0.0368 - val_mae: 0.0957\n",
      "Epoch 505/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0240 - mae: 0.0782 - val_loss: 0.0368 - val_mae: 0.0955\n",
      "Epoch 506/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0239 - mae: 0.0778 - val_loss: 0.0367 - val_mae: 0.0952\n",
      "Epoch 507/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0239 - mae: 0.0780 - val_loss: 0.0368 - val_mae: 0.0953\n",
      "Epoch 508/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0238 - mae: 0.0774 - val_loss: 0.0365 - val_mae: 0.0948\n",
      "Epoch 509/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0237 - mae: 0.0774 - val_loss: 0.0365 - val_mae: 0.0947\n",
      "Epoch 510/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0237 - mae: 0.0772 - val_loss: 0.0363 - val_mae: 0.0945\n",
      "Epoch 511/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0236 - mae: 0.0771 - val_loss: 0.0363 - val_mae: 0.0943\n",
      "Epoch 512/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0235 - mae: 0.0770 - val_loss: 0.0363 - val_mae: 0.0941\n",
      "Epoch 513/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0234 - mae: 0.0768 - val_loss: 0.0361 - val_mae: 0.0940\n",
      "Epoch 514/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0233 - mae: 0.0765 - val_loss: 0.0361 - val_mae: 0.0938\n",
      "Epoch 515/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0233 - mae: 0.0764 - val_loss: 0.0358 - val_mae: 0.0934\n",
      "Epoch 516/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0232 - mae: 0.0762 - val_loss: 0.0359 - val_mae: 0.0934\n",
      "Epoch 517/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0232 - mae: 0.0762 - val_loss: 0.0357 - val_mae: 0.0931\n",
      "Epoch 518/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0231 - mae: 0.0759 - val_loss: 0.0357 - val_mae: 0.0928\n",
      "Epoch 519/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0230 - mae: 0.0757 - val_loss: 0.0357 - val_mae: 0.0929\n",
      "Epoch 520/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0229 - mae: 0.0757 - val_loss: 0.0356 - val_mae: 0.0927\n",
      "Epoch 521/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0228 - mae: 0.0753 - val_loss: 0.0355 - val_mae: 0.0924\n",
      "Epoch 522/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0228 - mae: 0.0751 - val_loss: 0.0353 - val_mae: 0.0921\n",
      "Epoch 523/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0227 - mae: 0.0750 - val_loss: 0.0354 - val_mae: 0.0921\n",
      "Epoch 524/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0226 - mae: 0.0749 - val_loss: 0.0354 - val_mae: 0.0919\n",
      "Epoch 525/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0225 - mae: 0.0746 - val_loss: 0.0351 - val_mae: 0.0916\n",
      "Epoch 526/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0224 - mae: 0.0746 - val_loss: 0.0351 - val_mae: 0.0914\n",
      "Epoch 527/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0224 - mae: 0.0745 - val_loss: 0.0349 - val_mae: 0.0912\n",
      "Epoch 528/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0224 - mae: 0.0743 - val_loss: 0.0349 - val_mae: 0.0911\n",
      "Epoch 529/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0222 - mae: 0.0738 - val_loss: 0.0348 - val_mae: 0.0908\n",
      "Epoch 530/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0222 - mae: 0.0737 - val_loss: 0.0348 - val_mae: 0.0906\n",
      "Epoch 531/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0221 - mae: 0.0737 - val_loss: 0.0347 - val_mae: 0.0906\n",
      "Epoch 532/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0220 - mae: 0.0734 - val_loss: 0.0346 - val_mae: 0.0903\n",
      "Epoch 533/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0220 - mae: 0.0732 - val_loss: 0.0344 - val_mae: 0.0899\n",
      "Epoch 534/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0219 - mae: 0.0733 - val_loss: 0.0344 - val_mae: 0.0900\n",
      "Epoch 535/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0218 - mae: 0.0729 - val_loss: 0.0344 - val_mae: 0.0896\n",
      "Epoch 536/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0217 - mae: 0.0726 - val_loss: 0.0342 - val_mae: 0.0895\n",
      "Epoch 537/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0217 - mae: 0.0726 - val_loss: 0.0342 - val_mae: 0.0894\n",
      "Epoch 538/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0216 - mae: 0.0725 - val_loss: 0.0342 - val_mae: 0.0893\n",
      "Epoch 539/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0216 - mae: 0.0721 - val_loss: 0.0340 - val_mae: 0.0888\n",
      "Epoch 540/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0215 - mae: 0.0724 - val_loss: 0.0339 - val_mae: 0.0888\n",
      "Epoch 541/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0215 - mae: 0.0719 - val_loss: 0.0339 - val_mae: 0.0885\n",
      "Epoch 542/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0213 - mae: 0.0716 - val_loss: 0.0338 - val_mae: 0.0884\n",
      "Epoch 543/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0213 - mae: 0.0717 - val_loss: 0.0337 - val_mae: 0.0881\n",
      "Epoch 544/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0212 - mae: 0.0712 - val_loss: 0.0338 - val_mae: 0.0881\n",
      "Epoch 545/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0211 - mae: 0.0712 - val_loss: 0.0336 - val_mae: 0.0879\n",
      "Epoch 546/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0211 - mae: 0.0712 - val_loss: 0.0335 - val_mae: 0.0876\n",
      "Epoch 547/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0210 - mae: 0.0709 - val_loss: 0.0335 - val_mae: 0.0875\n",
      "Epoch 548/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0210 - mae: 0.0707 - val_loss: 0.0335 - val_mae: 0.0873\n",
      "Epoch 549/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0208 - mae: 0.0705 - val_loss: 0.0332 - val_mae: 0.0869\n",
      "Epoch 550/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0208 - mae: 0.0705 - val_loss: 0.0332 - val_mae: 0.0868\n",
      "Epoch 551/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0207 - mae: 0.0701 - val_loss: 0.0332 - val_mae: 0.0867\n",
      "Epoch 552/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0207 - mae: 0.0700 - val_loss: 0.0331 - val_mae: 0.0864\n",
      "Epoch 553/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0206 - mae: 0.0699 - val_loss: 0.0331 - val_mae: 0.0864\n",
      "Epoch 554/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0205 - mae: 0.0697 - val_loss: 0.0330 - val_mae: 0.0861\n",
      "Epoch 555/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0205 - mae: 0.0693 - val_loss: 0.0331 - val_mae: 0.0860\n",
      "Epoch 556/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0204 - mae: 0.0693 - val_loss: 0.0329 - val_mae: 0.0857\n",
      "Epoch 557/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0203 - mae: 0.0691 - val_loss: 0.0327 - val_mae: 0.0856\n",
      "Epoch 558/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0202 - mae: 0.0689 - val_loss: 0.0327 - val_mae: 0.0853\n",
      "Epoch 559/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0201 - mae: 0.0687 - val_loss: 0.0325 - val_mae: 0.0851\n",
      "Epoch 560/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0201 - mae: 0.0686 - val_loss: 0.0326 - val_mae: 0.0850\n",
      "Epoch 561/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0200 - mae: 0.0682 - val_loss: 0.0324 - val_mae: 0.0847\n",
      "Epoch 562/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0200 - mae: 0.0681 - val_loss: 0.0325 - val_mae: 0.0846\n",
      "Epoch 563/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0199 - mae: 0.0681 - val_loss: 0.0323 - val_mae: 0.0843\n",
      "Epoch 564/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0198 - mae: 0.0679 - val_loss: 0.0322 - val_mae: 0.0840\n",
      "Epoch 565/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0197 - mae: 0.0675 - val_loss: 0.0322 - val_mae: 0.0840\n",
      "Epoch 566/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0197 - mae: 0.0673 - val_loss: 0.0321 - val_mae: 0.0837\n",
      "Epoch 567/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0196 - mae: 0.0673 - val_loss: 0.0319 - val_mae: 0.0834\n",
      "Epoch 568/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0195 - mae: 0.0672 - val_loss: 0.0319 - val_mae: 0.0832\n",
      "Epoch 569/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0195 - mae: 0.0669 - val_loss: 0.0319 - val_mae: 0.0830\n",
      "Epoch 570/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0194 - mae: 0.0666 - val_loss: 0.0318 - val_mae: 0.0828\n",
      "Epoch 571/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0194 - mae: 0.0668 - val_loss: 0.0317 - val_mae: 0.0826\n",
      "Epoch 572/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0193 - mae: 0.0664 - val_loss: 0.0317 - val_mae: 0.0826\n",
      "Epoch 573/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0193 - mae: 0.0661 - val_loss: 0.0317 - val_mae: 0.0825\n",
      "Epoch 574/1500\n",
      "52/52 [==============================] - 0s 791us/step - loss: 0.0191 - mae: 0.0661 - val_loss: 0.0315 - val_mae: 0.0822\n",
      "Epoch 575/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0191 - mae: 0.0659 - val_loss: 0.0315 - val_mae: 0.0820\n",
      "Epoch 576/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0191 - mae: 0.0658 - val_loss: 0.0314 - val_mae: 0.0817\n",
      "Epoch 577/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0190 - mae: 0.0655 - val_loss: 0.0314 - val_mae: 0.0816\n",
      "Epoch 578/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0189 - mae: 0.0653 - val_loss: 0.0313 - val_mae: 0.0814\n",
      "Epoch 579/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0189 - mae: 0.0653 - val_loss: 0.0312 - val_mae: 0.0812\n",
      "Epoch 580/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0188 - mae: 0.0649 - val_loss: 0.0312 - val_mae: 0.0810\n",
      "Epoch 581/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0187 - mae: 0.0650 - val_loss: 0.0311 - val_mae: 0.0809\n",
      "Epoch 582/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0187 - mae: 0.0650 - val_loss: 0.0310 - val_mae: 0.0807\n",
      "Epoch 583/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0186 - mae: 0.0645 - val_loss: 0.0309 - val_mae: 0.0804\n",
      "Epoch 584/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0185 - mae: 0.0643 - val_loss: 0.0309 - val_mae: 0.0802\n",
      "Epoch 585/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0185 - mae: 0.0642 - val_loss: 0.0308 - val_mae: 0.0799\n",
      "Epoch 586/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0184 - mae: 0.0640 - val_loss: 0.0307 - val_mae: 0.0798\n",
      "Epoch 587/1500\n",
      "52/52 [==============================] - 0s 785us/step - loss: 0.0184 - mae: 0.0638 - val_loss: 0.0307 - val_mae: 0.0796\n",
      "Epoch 588/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0184 - mae: 0.0639 - val_loss: 0.0307 - val_mae: 0.0796\n",
      "Epoch 589/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0183 - mae: 0.0634 - val_loss: 0.0307 - val_mae: 0.0793\n",
      "Epoch 590/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0182 - mae: 0.0633 - val_loss: 0.0306 - val_mae: 0.0792\n",
      "Epoch 591/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0181 - mae: 0.0631 - val_loss: 0.0304 - val_mae: 0.0788\n",
      "Epoch 592/1500\n",
      "52/52 [==============================] - 0s 897us/step - loss: 0.0181 - mae: 0.0633 - val_loss: 0.0304 - val_mae: 0.0787\n",
      "Epoch 593/1500\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.0180 - mae: 0.0628 - val_loss: 0.0304 - val_mae: 0.0786\n",
      "Epoch 594/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0180 - mae: 0.0625 - val_loss: 0.0303 - val_mae: 0.0783\n",
      "Epoch 595/1500\n",
      "52/52 [==============================] - 0s 880us/step - loss: 0.0179 - mae: 0.0625 - val_loss: 0.0301 - val_mae: 0.0779\n",
      "Epoch 596/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0179 - mae: 0.0624 - val_loss: 0.0301 - val_mae: 0.0779\n",
      "Epoch 597/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0178 - mae: 0.0619 - val_loss: 0.0300 - val_mae: 0.0775\n",
      "Epoch 598/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.0177 - mae: 0.0620 - val_loss: 0.0300 - val_mae: 0.0775\n",
      "Epoch 599/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0177 - mae: 0.0620 - val_loss: 0.0300 - val_mae: 0.0774\n",
      "Epoch 600/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0177 - mae: 0.0618 - val_loss: 0.0298 - val_mae: 0.0771\n",
      "Epoch 601/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0176 - mae: 0.0615 - val_loss: 0.0298 - val_mae: 0.0770\n",
      "Epoch 602/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0176 - mae: 0.0614 - val_loss: 0.0297 - val_mae: 0.0768\n",
      "Epoch 603/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0175 - mae: 0.0611 - val_loss: 0.0298 - val_mae: 0.0768\n",
      "Epoch 604/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0174 - mae: 0.0609 - val_loss: 0.0298 - val_mae: 0.0766\n",
      "Epoch 605/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0174 - mae: 0.0612 - val_loss: 0.0297 - val_mae: 0.0765\n",
      "Epoch 606/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0173 - mae: 0.0608 - val_loss: 0.0295 - val_mae: 0.0760\n",
      "Epoch 607/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0173 - mae: 0.0607 - val_loss: 0.0295 - val_mae: 0.0759\n",
      "Epoch 608/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0172 - mae: 0.0604 - val_loss: 0.0294 - val_mae: 0.0756\n",
      "Epoch 609/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0171 - mae: 0.0601 - val_loss: 0.0293 - val_mae: 0.0754\n",
      "Epoch 610/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0171 - mae: 0.0603 - val_loss: 0.0294 - val_mae: 0.0756\n",
      "Epoch 611/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0171 - mae: 0.0602 - val_loss: 0.0293 - val_mae: 0.0751\n",
      "Epoch 612/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0171 - mae: 0.0598 - val_loss: 0.0291 - val_mae: 0.0749\n",
      "Epoch 613/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0170 - mae: 0.0596 - val_loss: 0.0292 - val_mae: 0.0750\n",
      "Epoch 614/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0169 - mae: 0.0595 - val_loss: 0.0292 - val_mae: 0.0750\n",
      "Epoch 615/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0169 - mae: 0.0593 - val_loss: 0.0290 - val_mae: 0.0744\n",
      "Epoch 616/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0168 - mae: 0.0590 - val_loss: 0.0290 - val_mae: 0.0742\n",
      "Epoch 617/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0168 - mae: 0.0593 - val_loss: 0.0290 - val_mae: 0.0746\n",
      "Epoch 618/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0167 - mae: 0.0591 - val_loss: 0.0289 - val_mae: 0.0742\n",
      "Epoch 619/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0166 - mae: 0.0586 - val_loss: 0.0288 - val_mae: 0.0739\n",
      "Epoch 620/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0166 - mae: 0.0588 - val_loss: 0.0288 - val_mae: 0.0738\n",
      "Epoch 621/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0166 - mae: 0.0587 - val_loss: 0.0289 - val_mae: 0.0737\n",
      "Epoch 622/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0165 - mae: 0.0584 - val_loss: 0.0288 - val_mae: 0.0737\n",
      "Epoch 623/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0165 - mae: 0.0580 - val_loss: 0.0285 - val_mae: 0.0729\n",
      "Epoch 624/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0164 - mae: 0.0581 - val_loss: 0.0286 - val_mae: 0.0729\n",
      "Epoch 625/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0164 - mae: 0.0581 - val_loss: 0.0285 - val_mae: 0.0729\n",
      "Epoch 626/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0163 - mae: 0.0577 - val_loss: 0.0285 - val_mae: 0.0728\n",
      "Epoch 627/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0163 - mae: 0.0576 - val_loss: 0.0285 - val_mae: 0.0727\n",
      "Epoch 628/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0162 - mae: 0.0575 - val_loss: 0.0283 - val_mae: 0.0723\n",
      "Epoch 629/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0162 - mae: 0.0573 - val_loss: 0.0283 - val_mae: 0.0722\n",
      "Epoch 630/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0161 - mae: 0.0570 - val_loss: 0.0282 - val_mae: 0.0719\n",
      "Epoch 631/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0161 - mae: 0.0572 - val_loss: 0.0281 - val_mae: 0.0719\n",
      "Epoch 632/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0160 - mae: 0.0569 - val_loss: 0.0281 - val_mae: 0.0717\n",
      "Epoch 633/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0160 - mae: 0.0567 - val_loss: 0.0281 - val_mae: 0.0715\n",
      "Epoch 634/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0160 - mae: 0.0568 - val_loss: 0.0280 - val_mae: 0.0713\n",
      "Epoch 635/1500\n",
      "52/52 [==============================] - 0s 921us/step - loss: 0.0159 - mae: 0.0563 - val_loss: 0.0280 - val_mae: 0.0713\n",
      "Epoch 636/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0159 - mae: 0.0565 - val_loss: 0.0279 - val_mae: 0.0712\n",
      "Epoch 637/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0158 - mae: 0.0562 - val_loss: 0.0279 - val_mae: 0.0710\n",
      "Epoch 638/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0157 - mae: 0.0560 - val_loss: 0.0278 - val_mae: 0.0707\n",
      "Epoch 639/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0157 - mae: 0.0560 - val_loss: 0.0277 - val_mae: 0.0704\n",
      "Epoch 640/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0156 - mae: 0.0556 - val_loss: 0.0278 - val_mae: 0.0705\n",
      "Epoch 641/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0156 - mae: 0.0558 - val_loss: 0.0277 - val_mae: 0.0705\n",
      "Epoch 642/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0156 - mae: 0.0558 - val_loss: 0.0276 - val_mae: 0.0701\n",
      "Epoch 643/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0155 - mae: 0.0554 - val_loss: 0.0276 - val_mae: 0.0700\n",
      "Epoch 644/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0155 - mae: 0.0553 - val_loss: 0.0276 - val_mae: 0.0699\n",
      "Epoch 645/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0154 - mae: 0.0550 - val_loss: 0.0276 - val_mae: 0.0698\n",
      "Epoch 646/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.0154 - mae: 0.0548 - val_loss: 0.0275 - val_mae: 0.0696\n",
      "Epoch 647/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0154 - mae: 0.0549 - val_loss: 0.0275 - val_mae: 0.0696\n",
      "Epoch 648/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0154 - mae: 0.0548 - val_loss: 0.0275 - val_mae: 0.0694\n",
      "Epoch 649/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0153 - mae: 0.0548 - val_loss: 0.0274 - val_mae: 0.0693\n",
      "Epoch 650/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0152 - mae: 0.0544 - val_loss: 0.0273 - val_mae: 0.0690\n",
      "Epoch 651/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0152 - mae: 0.0545 - val_loss: 0.0272 - val_mae: 0.0688\n",
      "Epoch 652/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0152 - mae: 0.0541 - val_loss: 0.0271 - val_mae: 0.0685\n",
      "Epoch 653/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0151 - mae: 0.0540 - val_loss: 0.0273 - val_mae: 0.0689\n",
      "Epoch 654/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0151 - mae: 0.0541 - val_loss: 0.0271 - val_mae: 0.0684\n",
      "Epoch 655/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0150 - mae: 0.0539 - val_loss: 0.0271 - val_mae: 0.0684\n",
      "Epoch 656/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0150 - mae: 0.0538 - val_loss: 0.0271 - val_mae: 0.0683\n",
      "Epoch 657/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0150 - mae: 0.0537 - val_loss: 0.0269 - val_mae: 0.0678\n",
      "Epoch 658/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0149 - mae: 0.0535 - val_loss: 0.0270 - val_mae: 0.0680\n",
      "Epoch 659/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0149 - mae: 0.0533 - val_loss: 0.0270 - val_mae: 0.0679\n",
      "Epoch 660/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0149 - mae: 0.0532 - val_loss: 0.0269 - val_mae: 0.0676\n",
      "Epoch 661/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0148 - mae: 0.0531 - val_loss: 0.0267 - val_mae: 0.0673\n",
      "Epoch 662/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0148 - mae: 0.0529 - val_loss: 0.0267 - val_mae: 0.0670\n",
      "Epoch 663/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0147 - mae: 0.0529 - val_loss: 0.0268 - val_mae: 0.0672\n",
      "Epoch 664/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0147 - mae: 0.0527 - val_loss: 0.0268 - val_mae: 0.0671\n",
      "Epoch 665/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0146 - mae: 0.0526 - val_loss: 0.0268 - val_mae: 0.0672\n",
      "Epoch 666/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0146 - mae: 0.0526 - val_loss: 0.0268 - val_mae: 0.0672\n",
      "Epoch 667/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0146 - mae: 0.0524 - val_loss: 0.0268 - val_mae: 0.0670\n",
      "Epoch 668/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0145 - mae: 0.0521 - val_loss: 0.0266 - val_mae: 0.0665\n",
      "Epoch 669/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0145 - mae: 0.0522 - val_loss: 0.0265 - val_mae: 0.0663\n",
      "Epoch 670/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0145 - mae: 0.0522 - val_loss: 0.0265 - val_mae: 0.0663\n",
      "Epoch 671/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0144 - mae: 0.0518 - val_loss: 0.0264 - val_mae: 0.0659\n",
      "Epoch 672/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0144 - mae: 0.0517 - val_loss: 0.0264 - val_mae: 0.0660\n",
      "Epoch 673/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0143 - mae: 0.0516 - val_loss: 0.0263 - val_mae: 0.0657\n",
      "Epoch 674/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0143 - mae: 0.0514 - val_loss: 0.0264 - val_mae: 0.0659\n",
      "Epoch 675/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0144 - mae: 0.0520 - val_loss: 0.0265 - val_mae: 0.0661\n",
      "Epoch 676/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0143 - mae: 0.0513 - val_loss: 0.0264 - val_mae: 0.0657\n",
      "Epoch 677/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0142 - mae: 0.0511 - val_loss: 0.0263 - val_mae: 0.0656\n",
      "Epoch 678/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0142 - mae: 0.0510 - val_loss: 0.0262 - val_mae: 0.0652\n",
      "Epoch 679/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0141 - mae: 0.0510 - val_loss: 0.0262 - val_mae: 0.0652\n",
      "Epoch 680/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0141 - mae: 0.0508 - val_loss: 0.0262 - val_mae: 0.0649\n",
      "Epoch 681/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0140 - mae: 0.0506 - val_loss: 0.0260 - val_mae: 0.0647\n",
      "Epoch 682/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0140 - mae: 0.0507 - val_loss: 0.0261 - val_mae: 0.0649\n",
      "Epoch 683/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0140 - mae: 0.0505 - val_loss: 0.0260 - val_mae: 0.0648\n",
      "Epoch 684/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0140 - mae: 0.0504 - val_loss: 0.0259 - val_mae: 0.0642\n",
      "Epoch 685/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0139 - mae: 0.0503 - val_loss: 0.0259 - val_mae: 0.0641\n",
      "Epoch 686/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0139 - mae: 0.0502 - val_loss: 0.0259 - val_mae: 0.0645\n",
      "Epoch 687/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0138 - mae: 0.0502 - val_loss: 0.0259 - val_mae: 0.0643\n",
      "Epoch 688/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0138 - mae: 0.0499 - val_loss: 0.0258 - val_mae: 0.0638\n",
      "Epoch 689/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0138 - mae: 0.0499 - val_loss: 0.0258 - val_mae: 0.0639\n",
      "Epoch 690/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0137 - mae: 0.0496 - val_loss: 0.0258 - val_mae: 0.0639\n",
      "Epoch 691/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0137 - mae: 0.0497 - val_loss: 0.0257 - val_mae: 0.0636\n",
      "Epoch 692/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0137 - mae: 0.0494 - val_loss: 0.0259 - val_mae: 0.0639\n",
      "Epoch 693/1500\n",
      "52/52 [==============================] - 0s 964us/step - loss: 0.0136 - mae: 0.0496 - val_loss: 0.0256 - val_mae: 0.0634\n",
      "Epoch 694/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0136 - mae: 0.0493 - val_loss: 0.0257 - val_mae: 0.0633\n",
      "Epoch 695/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0137 - mae: 0.0491 - val_loss: 0.0258 - val_mae: 0.0635\n",
      "Epoch 696/1500\n",
      "52/52 [==============================] - 0s 931us/step - loss: 0.0136 - mae: 0.0495 - val_loss: 0.0256 - val_mae: 0.0631\n",
      "Epoch 697/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0135 - mae: 0.0489 - val_loss: 0.0256 - val_mae: 0.0632\n",
      "Epoch 698/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0135 - mae: 0.0488 - val_loss: 0.0255 - val_mae: 0.0628\n",
      "Epoch 699/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0135 - mae: 0.0488 - val_loss: 0.0254 - val_mae: 0.0625\n",
      "Epoch 700/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0135 - mae: 0.0489 - val_loss: 0.0254 - val_mae: 0.0625\n",
      "Epoch 701/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0134 - mae: 0.0483 - val_loss: 0.0255 - val_mae: 0.0625\n",
      "Epoch 702/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0134 - mae: 0.0486 - val_loss: 0.0253 - val_mae: 0.0622\n",
      "Epoch 703/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0133 - mae: 0.0482 - val_loss: 0.0253 - val_mae: 0.0622\n",
      "Epoch 704/1500\n",
      "52/52 [==============================] - 0s 802us/step - loss: 0.0133 - mae: 0.0483 - val_loss: 0.0254 - val_mae: 0.0624\n",
      "Epoch 705/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0133 - mae: 0.0482 - val_loss: 0.0253 - val_mae: 0.0621\n",
      "Epoch 706/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0132 - mae: 0.0480 - val_loss: 0.0253 - val_mae: 0.0618\n",
      "Epoch 707/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0132 - mae: 0.0478 - val_loss: 0.0252 - val_mae: 0.0617\n",
      "Epoch 708/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0132 - mae: 0.0477 - val_loss: 0.0253 - val_mae: 0.0619\n",
      "Epoch 709/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0132 - mae: 0.0479 - val_loss: 0.0252 - val_mae: 0.0617\n",
      "Epoch 710/1500\n",
      "52/52 [==============================] - 0s 799us/step - loss: 0.0131 - mae: 0.0475 - val_loss: 0.0251 - val_mae: 0.0612\n",
      "Epoch 711/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0131 - mae: 0.0475 - val_loss: 0.0251 - val_mae: 0.0612\n",
      "Epoch 712/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0131 - mae: 0.0472 - val_loss: 0.0251 - val_mae: 0.0613\n",
      "Epoch 713/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0130 - mae: 0.0474 - val_loss: 0.0249 - val_mae: 0.0609\n",
      "Epoch 714/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0130 - mae: 0.0471 - val_loss: 0.0250 - val_mae: 0.0609\n",
      "Epoch 715/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0130 - mae: 0.0470 - val_loss: 0.0250 - val_mae: 0.0612\n",
      "Epoch 716/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0130 - mae: 0.0470 - val_loss: 0.0250 - val_mae: 0.0607\n",
      "Epoch 717/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0129 - mae: 0.0470 - val_loss: 0.0250 - val_mae: 0.0608\n",
      "Epoch 718/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0129 - mae: 0.0468 - val_loss: 0.0249 - val_mae: 0.0605\n",
      "Epoch 719/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.0128 - mae: 0.0466 - val_loss: 0.0249 - val_mae: 0.0605\n",
      "Epoch 720/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0128 - mae: 0.0465 - val_loss: 0.0248 - val_mae: 0.0604\n",
      "Epoch 721/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0128 - mae: 0.0465 - val_loss: 0.0248 - val_mae: 0.0602\n",
      "Epoch 722/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0128 - mae: 0.0464 - val_loss: 0.0247 - val_mae: 0.0599\n",
      "Epoch 723/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0128 - mae: 0.0465 - val_loss: 0.0248 - val_mae: 0.0601\n",
      "Epoch 724/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0127 - mae: 0.0462 - val_loss: 0.0248 - val_mae: 0.0600\n",
      "Epoch 725/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0127 - mae: 0.0460 - val_loss: 0.0246 - val_mae: 0.0595\n",
      "Epoch 726/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0127 - mae: 0.0458 - val_loss: 0.0246 - val_mae: 0.0596\n",
      "Epoch 727/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0126 - mae: 0.0460 - val_loss: 0.0246 - val_mae: 0.0595\n",
      "Epoch 728/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0126 - mae: 0.0458 - val_loss: 0.0247 - val_mae: 0.0596\n",
      "Epoch 729/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0126 - mae: 0.0456 - val_loss: 0.0246 - val_mae: 0.0592\n",
      "Epoch 730/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0126 - mae: 0.0455 - val_loss: 0.0244 - val_mae: 0.0589\n",
      "Epoch 731/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0125 - mae: 0.0457 - val_loss: 0.0245 - val_mae: 0.0592\n",
      "Epoch 732/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0125 - mae: 0.0452 - val_loss: 0.0245 - val_mae: 0.0588\n",
      "Epoch 733/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0125 - mae: 0.0452 - val_loss: 0.0244 - val_mae: 0.0587\n",
      "Epoch 734/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0125 - mae: 0.0452 - val_loss: 0.0244 - val_mae: 0.0587\n",
      "Epoch 735/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0124 - mae: 0.0450 - val_loss: 0.0244 - val_mae: 0.0586\n",
      "Epoch 736/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0124 - mae: 0.0450 - val_loss: 0.0244 - val_mae: 0.0586\n",
      "Epoch 737/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0124 - mae: 0.0448 - val_loss: 0.0244 - val_mae: 0.0584\n",
      "Epoch 738/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0124 - mae: 0.0451 - val_loss: 0.0244 - val_mae: 0.0585\n",
      "Epoch 739/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0123 - mae: 0.0446 - val_loss: 0.0243 - val_mae: 0.0581\n",
      "Epoch 740/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0123 - mae: 0.0443 - val_loss: 0.0243 - val_mae: 0.0580\n",
      "Epoch 741/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.0123 - mae: 0.0445 - val_loss: 0.0243 - val_mae: 0.0583\n",
      "Epoch 742/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0123 - mae: 0.0444 - val_loss: 0.0242 - val_mae: 0.0579\n",
      "Epoch 743/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0123 - mae: 0.0447 - val_loss: 0.0243 - val_mae: 0.0581\n",
      "Epoch 744/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0122 - mae: 0.0443 - val_loss: 0.0241 - val_mae: 0.0576\n",
      "Epoch 745/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0122 - mae: 0.0439 - val_loss: 0.0242 - val_mae: 0.0576\n",
      "Epoch 746/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0121 - mae: 0.0439 - val_loss: 0.0241 - val_mae: 0.0574\n",
      "Epoch 747/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0121 - mae: 0.0441 - val_loss: 0.0240 - val_mae: 0.0573\n",
      "Epoch 748/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0121 - mae: 0.0439 - val_loss: 0.0241 - val_mae: 0.0574\n",
      "Epoch 749/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0121 - mae: 0.0435 - val_loss: 0.0240 - val_mae: 0.0570\n",
      "Epoch 750/1500\n",
      "52/52 [==============================] - 0s 794us/step - loss: 0.0121 - mae: 0.0438 - val_loss: 0.0241 - val_mae: 0.0571\n",
      "Epoch 751/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0120 - mae: 0.0436 - val_loss: 0.0240 - val_mae: 0.0570\n",
      "Epoch 752/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0120 - mae: 0.0435 - val_loss: 0.0240 - val_mae: 0.0570\n",
      "Epoch 753/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0120 - mae: 0.0433 - val_loss: 0.0240 - val_mae: 0.0568\n",
      "Epoch 754/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0119 - mae: 0.0433 - val_loss: 0.0239 - val_mae: 0.0566\n",
      "Epoch 755/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0120 - mae: 0.0432 - val_loss: 0.0240 - val_mae: 0.0567\n",
      "Epoch 756/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0119 - mae: 0.0430 - val_loss: 0.0239 - val_mae: 0.0566\n",
      "Epoch 757/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0119 - mae: 0.0432 - val_loss: 0.0239 - val_mae: 0.0565\n",
      "Epoch 758/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0119 - mae: 0.0427 - val_loss: 0.0239 - val_mae: 0.0563\n",
      "Epoch 759/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0118 - mae: 0.0426 - val_loss: 0.0239 - val_mae: 0.0563\n",
      "Epoch 760/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0118 - mae: 0.0428 - val_loss: 0.0238 - val_mae: 0.0560\n",
      "Epoch 761/1500\n",
      "52/52 [==============================] - 0s 808us/step - loss: 0.0118 - mae: 0.0426 - val_loss: 0.0237 - val_mae: 0.0558\n",
      "Epoch 762/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0118 - mae: 0.0425 - val_loss: 0.0238 - val_mae: 0.0562\n",
      "Epoch 763/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0118 - mae: 0.0426 - val_loss: 0.0238 - val_mae: 0.0561\n",
      "Epoch 764/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0117 - mae: 0.0425 - val_loss: 0.0237 - val_mae: 0.0556\n",
      "Epoch 765/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0117 - mae: 0.0421 - val_loss: 0.0237 - val_mae: 0.0555\n",
      "Epoch 766/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0117 - mae: 0.0423 - val_loss: 0.0238 - val_mae: 0.0558\n",
      "Epoch 767/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0117 - mae: 0.0422 - val_loss: 0.0237 - val_mae: 0.0557\n",
      "Epoch 768/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0116 - mae: 0.0422 - val_loss: 0.0236 - val_mae: 0.0552\n",
      "Epoch 769/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0116 - mae: 0.0420 - val_loss: 0.0236 - val_mae: 0.0551\n",
      "Epoch 770/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0116 - mae: 0.0417 - val_loss: 0.0236 - val_mae: 0.0552\n",
      "Epoch 771/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0116 - mae: 0.0416 - val_loss: 0.0236 - val_mae: 0.0553\n",
      "Epoch 772/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0116 - mae: 0.0417 - val_loss: 0.0235 - val_mae: 0.0550\n",
      "Epoch 773/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0116 - mae: 0.0415 - val_loss: 0.0235 - val_mae: 0.0548\n",
      "Epoch 774/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0115 - mae: 0.0415 - val_loss: 0.0235 - val_mae: 0.0549\n",
      "Epoch 775/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0115 - mae: 0.0415 - val_loss: 0.0235 - val_mae: 0.0549\n",
      "Epoch 776/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0115 - mae: 0.0412 - val_loss: 0.0236 - val_mae: 0.0549\n",
      "Epoch 777/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0114 - mae: 0.0412 - val_loss: 0.0234 - val_mae: 0.0546\n",
      "Epoch 778/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0114 - mae: 0.0411 - val_loss: 0.0233 - val_mae: 0.0543\n",
      "Epoch 779/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0114 - mae: 0.0411 - val_loss: 0.0234 - val_mae: 0.0544\n",
      "Epoch 780/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0114 - mae: 0.0411 - val_loss: 0.0233 - val_mae: 0.0542\n",
      "Epoch 781/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0114 - mae: 0.0408 - val_loss: 0.0233 - val_mae: 0.0541\n",
      "Epoch 782/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0113 - mae: 0.0408 - val_loss: 0.0233 - val_mae: 0.0538\n",
      "Epoch 783/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0114 - mae: 0.0410 - val_loss: 0.0233 - val_mae: 0.0540\n",
      "Epoch 784/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0113 - mae: 0.0406 - val_loss: 0.0233 - val_mae: 0.0539\n",
      "Epoch 785/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0113 - mae: 0.0404 - val_loss: 0.0232 - val_mae: 0.0536\n",
      "Epoch 786/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0113 - mae: 0.0405 - val_loss: 0.0232 - val_mae: 0.0536\n",
      "Epoch 787/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.0112 - mae: 0.0404 - val_loss: 0.0232 - val_mae: 0.0539\n",
      "Epoch 788/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0112 - mae: 0.0404 - val_loss: 0.0233 - val_mae: 0.0538\n",
      "Epoch 789/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0112 - mae: 0.0403 - val_loss: 0.0231 - val_mae: 0.0532\n",
      "Epoch 790/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0112 - mae: 0.0403 - val_loss: 0.0232 - val_mae: 0.0533\n",
      "Epoch 791/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0112 - mae: 0.0398 - val_loss: 0.0232 - val_mae: 0.0532\n",
      "Epoch 792/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0112 - mae: 0.0399 - val_loss: 0.0231 - val_mae: 0.0530\n",
      "Epoch 793/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0111 - mae: 0.0401 - val_loss: 0.0231 - val_mae: 0.0530\n",
      "Epoch 794/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0111 - mae: 0.0397 - val_loss: 0.0231 - val_mae: 0.0530\n",
      "Epoch 795/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0111 - mae: 0.0397 - val_loss: 0.0231 - val_mae: 0.0531\n",
      "Epoch 796/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0111 - mae: 0.0397 - val_loss: 0.0231 - val_mae: 0.0530\n",
      "Epoch 797/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0110 - mae: 0.0396 - val_loss: 0.0230 - val_mae: 0.0529\n",
      "Epoch 798/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0110 - mae: 0.0398 - val_loss: 0.0230 - val_mae: 0.0527\n",
      "Epoch 799/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0110 - mae: 0.0394 - val_loss: 0.0230 - val_mae: 0.0524\n",
      "Epoch 800/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0110 - mae: 0.0396 - val_loss: 0.0229 - val_mae: 0.0524\n",
      "Epoch 801/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0110 - mae: 0.0394 - val_loss: 0.0230 - val_mae: 0.0526\n",
      "Epoch 802/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0110 - mae: 0.0391 - val_loss: 0.0230 - val_mae: 0.0525\n",
      "Epoch 803/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0110 - mae: 0.0395 - val_loss: 0.0230 - val_mae: 0.0527\n",
      "Epoch 804/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0109 - mae: 0.0390 - val_loss: 0.0230 - val_mae: 0.0523\n",
      "Epoch 805/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0109 - mae: 0.0389 - val_loss: 0.0229 - val_mae: 0.0523\n",
      "Epoch 806/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0109 - mae: 0.0392 - val_loss: 0.0228 - val_mae: 0.0520\n",
      "Epoch 807/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0109 - mae: 0.0387 - val_loss: 0.0228 - val_mae: 0.0518\n",
      "Epoch 808/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0108 - mae: 0.0389 - val_loss: 0.0228 - val_mae: 0.0519\n",
      "Epoch 809/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0108 - mae: 0.0389 - val_loss: 0.0228 - val_mae: 0.0520\n",
      "Epoch 810/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0108 - mae: 0.0387 - val_loss: 0.0229 - val_mae: 0.0520\n",
      "Epoch 811/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0108 - mae: 0.0386 - val_loss: 0.0228 - val_mae: 0.0516\n",
      "Epoch 812/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0108 - mae: 0.0385 - val_loss: 0.0228 - val_mae: 0.0515\n",
      "Epoch 813/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0108 - mae: 0.0385 - val_loss: 0.0227 - val_mae: 0.0517\n",
      "Epoch 814/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0107 - mae: 0.0385 - val_loss: 0.0227 - val_mae: 0.0516\n",
      "Epoch 815/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0107 - mae: 0.0385 - val_loss: 0.0228 - val_mae: 0.0516\n",
      "Epoch 816/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0107 - mae: 0.0383 - val_loss: 0.0227 - val_mae: 0.0512\n",
      "Epoch 817/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0107 - mae: 0.0383 - val_loss: 0.0227 - val_mae: 0.0515\n",
      "Epoch 818/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0107 - mae: 0.0382 - val_loss: 0.0227 - val_mae: 0.0516\n",
      "Epoch 819/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0106 - mae: 0.0380 - val_loss: 0.0226 - val_mae: 0.0511\n",
      "Epoch 820/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.0106 - mae: 0.0380 - val_loss: 0.0226 - val_mae: 0.0509\n",
      "Epoch 821/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0106 - mae: 0.0380 - val_loss: 0.0226 - val_mae: 0.0511\n",
      "Epoch 822/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0106 - mae: 0.0379 - val_loss: 0.0225 - val_mae: 0.0505\n",
      "Epoch 823/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0106 - mae: 0.0379 - val_loss: 0.0226 - val_mae: 0.0511\n",
      "Epoch 824/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0106 - mae: 0.0377 - val_loss: 0.0226 - val_mae: 0.0508\n",
      "Epoch 825/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0106 - mae: 0.0376 - val_loss: 0.0226 - val_mae: 0.0509\n",
      "Epoch 826/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0106 - mae: 0.0376 - val_loss: 0.0225 - val_mae: 0.0506\n",
      "Epoch 827/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0106 - mae: 0.0377 - val_loss: 0.0224 - val_mae: 0.0504\n",
      "Epoch 828/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0105 - mae: 0.0375 - val_loss: 0.0225 - val_mae: 0.0507\n",
      "Epoch 829/1500\n",
      "52/52 [==============================] - 0s 885us/step - loss: 0.0105 - mae: 0.0374 - val_loss: 0.0225 - val_mae: 0.0505\n",
      "Epoch 830/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0105 - mae: 0.0376 - val_loss: 0.0225 - val_mae: 0.0507\n",
      "Epoch 831/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0105 - mae: 0.0372 - val_loss: 0.0224 - val_mae: 0.0502\n",
      "Epoch 832/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0104 - mae: 0.0371 - val_loss: 0.0225 - val_mae: 0.0504\n",
      "Epoch 833/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0104 - mae: 0.0371 - val_loss: 0.0225 - val_mae: 0.0505\n",
      "Epoch 834/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0104 - mae: 0.0374 - val_loss: 0.0224 - val_mae: 0.0500\n",
      "Epoch 835/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0104 - mae: 0.0368 - val_loss: 0.0225 - val_mae: 0.0500\n",
      "Epoch 836/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0104 - mae: 0.0371 - val_loss: 0.0224 - val_mae: 0.0500\n",
      "Epoch 837/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0104 - mae: 0.0368 - val_loss: 0.0224 - val_mae: 0.0499\n",
      "Epoch 838/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0104 - mae: 0.0371 - val_loss: 0.0224 - val_mae: 0.0500\n",
      "Epoch 839/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0104 - mae: 0.0368 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 840/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0103 - mae: 0.0367 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 841/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0103 - mae: 0.0367 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 842/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0103 - mae: 0.0367 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 843/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0103 - mae: 0.0365 - val_loss: 0.0223 - val_mae: 0.0497\n",
      "Epoch 844/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0103 - mae: 0.0364 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 845/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0103 - mae: 0.0365 - val_loss: 0.0223 - val_mae: 0.0496\n",
      "Epoch 846/1500\n",
      "52/52 [==============================] - 0s 876us/step - loss: 0.0102 - mae: 0.0363 - val_loss: 0.0223 - val_mae: 0.0493\n",
      "Epoch 847/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.0102 - mae: 0.0362 - val_loss: 0.0223 - val_mae: 0.0495\n",
      "Epoch 848/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0102 - mae: 0.0362 - val_loss: 0.0222 - val_mae: 0.0490\n",
      "Epoch 849/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0102 - mae: 0.0364 - val_loss: 0.0223 - val_mae: 0.0494\n",
      "Epoch 850/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0102 - mae: 0.0360 - val_loss: 0.0222 - val_mae: 0.0490\n",
      "Epoch 851/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0102 - mae: 0.0360 - val_loss: 0.0221 - val_mae: 0.0488\n",
      "Epoch 852/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0102 - mae: 0.0360 - val_loss: 0.0222 - val_mae: 0.0489\n",
      "Epoch 853/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0101 - mae: 0.0357 - val_loss: 0.0221 - val_mae: 0.0486\n",
      "Epoch 854/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0101 - mae: 0.0359 - val_loss: 0.0221 - val_mae: 0.0487\n",
      "Epoch 855/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0101 - mae: 0.0360 - val_loss: 0.0221 - val_mae: 0.0488\n",
      "Epoch 856/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0101 - mae: 0.0357 - val_loss: 0.0221 - val_mae: 0.0487\n",
      "Epoch 857/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0101 - mae: 0.0357 - val_loss: 0.0221 - val_mae: 0.0487\n",
      "Epoch 858/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0101 - mae: 0.0358 - val_loss: 0.0221 - val_mae: 0.0487\n",
      "Epoch 859/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0101 - mae: 0.0355 - val_loss: 0.0220 - val_mae: 0.0484\n",
      "Epoch 860/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0100 - mae: 0.0354 - val_loss: 0.0220 - val_mae: 0.0481\n",
      "Epoch 861/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0101 - mae: 0.0353 - val_loss: 0.0221 - val_mae: 0.0482\n",
      "Epoch 862/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0100 - mae: 0.0355 - val_loss: 0.0220 - val_mae: 0.0484\n",
      "Epoch 863/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0100 - mae: 0.0352 - val_loss: 0.0220 - val_mae: 0.0483\n",
      "Epoch 864/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0100 - mae: 0.0352 - val_loss: 0.0220 - val_mae: 0.0481\n",
      "Epoch 865/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0101 - mae: 0.0354 - val_loss: 0.0220 - val_mae: 0.0484\n",
      "Epoch 866/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0100 - mae: 0.0351 - val_loss: 0.0220 - val_mae: 0.0482\n",
      "Epoch 867/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0100 - mae: 0.0349 - val_loss: 0.0220 - val_mae: 0.0481\n",
      "Epoch 868/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0099 - mae: 0.0350 - val_loss: 0.0219 - val_mae: 0.0480\n",
      "Epoch 869/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0099 - mae: 0.0350 - val_loss: 0.0219 - val_mae: 0.0480\n",
      "Epoch 870/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0099 - mae: 0.0350 - val_loss: 0.0220 - val_mae: 0.0478\n",
      "Epoch 871/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0099 - mae: 0.0347 - val_loss: 0.0220 - val_mae: 0.0478\n",
      "Epoch 872/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0099 - mae: 0.0350 - val_loss: 0.0219 - val_mae: 0.0480\n",
      "Epoch 873/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0099 - mae: 0.0347 - val_loss: 0.0219 - val_mae: 0.0478\n",
      "Epoch 874/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0099 - mae: 0.0347 - val_loss: 0.0219 - val_mae: 0.0478\n",
      "Epoch 875/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0098 - mae: 0.0347 - val_loss: 0.0218 - val_mae: 0.0474\n",
      "Epoch 876/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0099 - mae: 0.0345 - val_loss: 0.0219 - val_mae: 0.0475\n",
      "Epoch 877/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0098 - mae: 0.0347 - val_loss: 0.0219 - val_mae: 0.0477\n",
      "Epoch 878/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0099 - mae: 0.0343 - val_loss: 0.0219 - val_mae: 0.0472\n",
      "Epoch 879/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0098 - mae: 0.0342 - val_loss: 0.0218 - val_mae: 0.0471\n",
      "Epoch 880/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0098 - mae: 0.0345 - val_loss: 0.0218 - val_mae: 0.0472\n",
      "Epoch 881/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0098 - mae: 0.0344 - val_loss: 0.0218 - val_mae: 0.0474\n",
      "Epoch 882/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0098 - mae: 0.0342 - val_loss: 0.0218 - val_mae: 0.0469\n",
      "Epoch 883/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0097 - mae: 0.0343 - val_loss: 0.0218 - val_mae: 0.0474\n",
      "Epoch 884/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0097 - mae: 0.0341 - val_loss: 0.0218 - val_mae: 0.0471\n",
      "Epoch 885/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0097 - mae: 0.0340 - val_loss: 0.0218 - val_mae: 0.0470\n",
      "Epoch 886/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0098 - mae: 0.0339 - val_loss: 0.0218 - val_mae: 0.0469\n",
      "Epoch 887/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0097 - mae: 0.0339 - val_loss: 0.0218 - val_mae: 0.0471\n",
      "Epoch 888/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0097 - mae: 0.0341 - val_loss: 0.0217 - val_mae: 0.0464\n",
      "Epoch 889/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0097 - mae: 0.0341 - val_loss: 0.0217 - val_mae: 0.0470\n",
      "Epoch 890/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0097 - mae: 0.0338 - val_loss: 0.0217 - val_mae: 0.0466\n",
      "Epoch 891/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0097 - mae: 0.0339 - val_loss: 0.0217 - val_mae: 0.0468\n",
      "Epoch 892/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0096 - mae: 0.0336 - val_loss: 0.0217 - val_mae: 0.0465\n",
      "Epoch 893/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0096 - mae: 0.0336 - val_loss: 0.0217 - val_mae: 0.0467\n",
      "Epoch 894/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0097 - mae: 0.0338 - val_loss: 0.0217 - val_mae: 0.0468\n",
      "Epoch 895/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0096 - mae: 0.0334 - val_loss: 0.0216 - val_mae: 0.0464\n",
      "Epoch 896/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0096 - mae: 0.0336 - val_loss: 0.0216 - val_mae: 0.0460\n",
      "Epoch 897/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0096 - mae: 0.0334 - val_loss: 0.0216 - val_mae: 0.0464\n",
      "Epoch 898/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0096 - mae: 0.0335 - val_loss: 0.0216 - val_mae: 0.0462\n",
      "Epoch 899/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0096 - mae: 0.0332 - val_loss: 0.0216 - val_mae: 0.0461\n",
      "Epoch 900/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0096 - mae: 0.0334 - val_loss: 0.0216 - val_mae: 0.0461\n",
      "Epoch 901/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0096 - mae: 0.0334 - val_loss: 0.0216 - val_mae: 0.0461\n",
      "Epoch 902/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0095 - mae: 0.0331 - val_loss: 0.0216 - val_mae: 0.0462\n",
      "Epoch 903/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0096 - mae: 0.0331 - val_loss: 0.0216 - val_mae: 0.0463\n",
      "Epoch 904/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0095 - mae: 0.0330 - val_loss: 0.0216 - val_mae: 0.0459\n",
      "Epoch 905/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0095 - mae: 0.0330 - val_loss: 0.0215 - val_mae: 0.0459\n",
      "Epoch 906/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0095 - mae: 0.0329 - val_loss: 0.0216 - val_mae: 0.0458\n",
      "Epoch 907/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0095 - mae: 0.0328 - val_loss: 0.0215 - val_mae: 0.0457\n",
      "Epoch 908/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0095 - mae: 0.0331 - val_loss: 0.0215 - val_mae: 0.0460\n",
      "Epoch 909/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0095 - mae: 0.0327 - val_loss: 0.0216 - val_mae: 0.0458\n",
      "Epoch 910/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0095 - mae: 0.0328 - val_loss: 0.0215 - val_mae: 0.0455\n",
      "Epoch 911/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0094 - mae: 0.0328 - val_loss: 0.0215 - val_mae: 0.0455\n",
      "Epoch 912/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0095 - mae: 0.0327 - val_loss: 0.0215 - val_mae: 0.0456\n",
      "Epoch 913/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0094 - mae: 0.0327 - val_loss: 0.0215 - val_mae: 0.0454\n",
      "Epoch 914/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0094 - mae: 0.0325 - val_loss: 0.0215 - val_mae: 0.0455\n",
      "Epoch 915/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0094 - mae: 0.0326 - val_loss: 0.0215 - val_mae: 0.0455\n",
      "Epoch 916/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0094 - mae: 0.0325 - val_loss: 0.0215 - val_mae: 0.0454\n",
      "Epoch 917/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0094 - mae: 0.0323 - val_loss: 0.0214 - val_mae: 0.0448\n",
      "Epoch 918/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0094 - mae: 0.0323 - val_loss: 0.0214 - val_mae: 0.0453\n",
      "Epoch 919/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0094 - mae: 0.0325 - val_loss: 0.0214 - val_mae: 0.0454\n",
      "Epoch 920/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0094 - mae: 0.0322 - val_loss: 0.0214 - val_mae: 0.0448\n",
      "Epoch 921/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0093 - mae: 0.0322 - val_loss: 0.0214 - val_mae: 0.0448\n",
      "Epoch 922/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0093 - mae: 0.0323 - val_loss: 0.0214 - val_mae: 0.0450\n",
      "Epoch 923/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0093 - mae: 0.0319 - val_loss: 0.0214 - val_mae: 0.0449\n",
      "Epoch 924/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0093 - mae: 0.0320 - val_loss: 0.0214 - val_mae: 0.0449\n",
      "Epoch 925/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0093 - mae: 0.0320 - val_loss: 0.0214 - val_mae: 0.0449\n",
      "Epoch 926/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0093 - mae: 0.0321 - val_loss: 0.0214 - val_mae: 0.0449\n",
      "Epoch 927/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0093 - mae: 0.0318 - val_loss: 0.0214 - val_mae: 0.0447\n",
      "Epoch 928/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0093 - mae: 0.0320 - val_loss: 0.0214 - val_mae: 0.0451\n",
      "Epoch 929/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0093 - mae: 0.0318 - val_loss: 0.0213 - val_mae: 0.0448\n",
      "Epoch 930/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0092 - mae: 0.0318 - val_loss: 0.0213 - val_mae: 0.0446\n",
      "Epoch 931/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0093 - mae: 0.0318 - val_loss: 0.0213 - val_mae: 0.0448\n",
      "Epoch 932/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0092 - mae: 0.0316 - val_loss: 0.0213 - val_mae: 0.0447\n",
      "Epoch 933/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0092 - mae: 0.0317 - val_loss: 0.0213 - val_mae: 0.0445\n",
      "Epoch 934/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0092 - mae: 0.0314 - val_loss: 0.0213 - val_mae: 0.0445\n",
      "Epoch 935/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0092 - mae: 0.0317 - val_loss: 0.0213 - val_mae: 0.0444\n",
      "Epoch 936/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0092 - mae: 0.0314 - val_loss: 0.0213 - val_mae: 0.0444\n",
      "Epoch 937/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0092 - mae: 0.0314 - val_loss: 0.0213 - val_mae: 0.0444\n",
      "Epoch 938/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0092 - mae: 0.0316 - val_loss: 0.0213 - val_mae: 0.0443\n",
      "Epoch 939/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0092 - mae: 0.0315 - val_loss: 0.0213 - val_mae: 0.0445\n",
      "Epoch 940/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0092 - mae: 0.0313 - val_loss: 0.0213 - val_mae: 0.0441\n",
      "Epoch 941/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0092 - mae: 0.0312 - val_loss: 0.0212 - val_mae: 0.0441\n",
      "Epoch 942/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0091 - mae: 0.0312 - val_loss: 0.0212 - val_mae: 0.0439\n",
      "Epoch 943/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0091 - mae: 0.0312 - val_loss: 0.0212 - val_mae: 0.0440\n",
      "Epoch 944/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0091 - mae: 0.0309 - val_loss: 0.0213 - val_mae: 0.0439\n",
      "Epoch 945/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0091 - mae: 0.0310 - val_loss: 0.0212 - val_mae: 0.0440\n",
      "Epoch 946/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0091 - mae: 0.0311 - val_loss: 0.0212 - val_mae: 0.0440\n",
      "Epoch 947/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0091 - mae: 0.0308 - val_loss: 0.0212 - val_mae: 0.0441\n",
      "Epoch 948/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0091 - mae: 0.0308 - val_loss: 0.0212 - val_mae: 0.0440\n",
      "Epoch 949/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0091 - mae: 0.0309 - val_loss: 0.0212 - val_mae: 0.0440\n",
      "Epoch 950/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0091 - mae: 0.0312 - val_loss: 0.0212 - val_mae: 0.0438\n",
      "Epoch 951/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0091 - mae: 0.0308 - val_loss: 0.0212 - val_mae: 0.0436\n",
      "Epoch 952/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0091 - mae: 0.0306 - val_loss: 0.0212 - val_mae: 0.0433\n",
      "Epoch 953/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0091 - mae: 0.0305 - val_loss: 0.0212 - val_mae: 0.0435\n",
      "Epoch 954/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0090 - mae: 0.0305 - val_loss: 0.0211 - val_mae: 0.0433\n",
      "Epoch 955/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0090 - mae: 0.0307 - val_loss: 0.0211 - val_mae: 0.0435\n",
      "Epoch 956/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0090 - mae: 0.0305 - val_loss: 0.0211 - val_mae: 0.0432\n",
      "Epoch 957/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0090 - mae: 0.0306 - val_loss: 0.0211 - val_mae: 0.0435\n",
      "Epoch 958/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0090 - mae: 0.0306 - val_loss: 0.0211 - val_mae: 0.0431\n",
      "Epoch 959/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0090 - mae: 0.0302 - val_loss: 0.0211 - val_mae: 0.0433\n",
      "Epoch 960/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0090 - mae: 0.0302 - val_loss: 0.0211 - val_mae: 0.0432\n",
      "Epoch 961/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0090 - mae: 0.0303 - val_loss: 0.0211 - val_mae: 0.0433\n",
      "Epoch 962/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0090 - mae: 0.0305 - val_loss: 0.0211 - val_mae: 0.0429\n",
      "Epoch 963/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0090 - mae: 0.0302 - val_loss: 0.0211 - val_mae: 0.0431\n",
      "Epoch 964/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0090 - mae: 0.0302 - val_loss: 0.0211 - val_mae: 0.0429\n",
      "Epoch 965/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0089 - mae: 0.0302 - val_loss: 0.0211 - val_mae: 0.0428\n",
      "Epoch 966/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0090 - mae: 0.0301 - val_loss: 0.0211 - val_mae: 0.0428\n",
      "Epoch 967/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0089 - mae: 0.0300 - val_loss: 0.0210 - val_mae: 0.0426\n",
      "Epoch 968/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0089 - mae: 0.0301 - val_loss: 0.0211 - val_mae: 0.0431\n",
      "Epoch 969/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0090 - mae: 0.0301 - val_loss: 0.0211 - val_mae: 0.0427\n",
      "Epoch 970/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0089 - mae: 0.0299 - val_loss: 0.0210 - val_mae: 0.0427\n",
      "Epoch 971/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0089 - mae: 0.0300 - val_loss: 0.0210 - val_mae: 0.0428\n",
      "Epoch 972/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0089 - mae: 0.0297 - val_loss: 0.0210 - val_mae: 0.0424\n",
      "Epoch 973/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0089 - mae: 0.0297 - val_loss: 0.0210 - val_mae: 0.0428\n",
      "Epoch 974/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0089 - mae: 0.0298 - val_loss: 0.0210 - val_mae: 0.0426\n",
      "Epoch 975/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0090 - mae: 0.0304 - val_loss: 0.0210 - val_mae: 0.0425\n",
      "Epoch 976/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0089 - mae: 0.0299 - val_loss: 0.0210 - val_mae: 0.0426\n",
      "Epoch 977/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0089 - mae: 0.0295 - val_loss: 0.0210 - val_mae: 0.0421\n",
      "Epoch 978/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0089 - mae: 0.0296 - val_loss: 0.0210 - val_mae: 0.0423\n",
      "Epoch 979/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0089 - mae: 0.0297 - val_loss: 0.0210 - val_mae: 0.0424\n",
      "Epoch 980/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0088 - mae: 0.0296 - val_loss: 0.0209 - val_mae: 0.0421\n",
      "Epoch 981/1500\n",
      "52/52 [==============================] - 0s 803us/step - loss: 0.0088 - mae: 0.0296 - val_loss: 0.0210 - val_mae: 0.0424\n",
      "Epoch 982/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0089 - mae: 0.0294 - val_loss: 0.0210 - val_mae: 0.0422\n",
      "Epoch 983/1500\n",
      "52/52 [==============================] - 0s 994us/step - loss: 0.0088 - mae: 0.0294 - val_loss: 0.0209 - val_mae: 0.0420\n",
      "Epoch 984/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0088 - mae: 0.0296 - val_loss: 0.0210 - val_mae: 0.0425\n",
      "Epoch 985/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0088 - mae: 0.0295 - val_loss: 0.0209 - val_mae: 0.0425\n",
      "Epoch 986/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0088 - mae: 0.0292 - val_loss: 0.0209 - val_mae: 0.0420\n",
      "Epoch 987/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0088 - mae: 0.0295 - val_loss: 0.0209 - val_mae: 0.0421\n",
      "Epoch 988/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0088 - mae: 0.0294 - val_loss: 0.0209 - val_mae: 0.0420\n",
      "Epoch 989/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0088 - mae: 0.0290 - val_loss: 0.0209 - val_mae: 0.0420\n",
      "Epoch 990/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0088 - mae: 0.0293 - val_loss: 0.0209 - val_mae: 0.0421\n",
      "Epoch 991/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0088 - mae: 0.0293 - val_loss: 0.0209 - val_mae: 0.0421\n",
      "Epoch 992/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0088 - mae: 0.0291 - val_loss: 0.0209 - val_mae: 0.0419\n",
      "Epoch 993/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0088 - mae: 0.0292 - val_loss: 0.0209 - val_mae: 0.0418\n",
      "Epoch 994/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0088 - mae: 0.0291 - val_loss: 0.0209 - val_mae: 0.0421\n",
      "Epoch 995/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0088 - mae: 0.0291 - val_loss: 0.0209 - val_mae: 0.0417\n",
      "Epoch 996/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0088 - mae: 0.0290 - val_loss: 0.0209 - val_mae: 0.0415\n",
      "Epoch 997/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0088 - mae: 0.0290 - val_loss: 0.0209 - val_mae: 0.0416\n",
      "Epoch 998/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0087 - mae: 0.0289 - val_loss: 0.0209 - val_mae: 0.0418\n",
      "Epoch 999/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0087 - mae: 0.0288 - val_loss: 0.0209 - val_mae: 0.0419\n",
      "Epoch 1000/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0087 - mae: 0.0289 - val_loss: 0.0208 - val_mae: 0.0415\n",
      "Epoch 1001/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0087 - mae: 0.0289 - val_loss: 0.0208 - val_mae: 0.0416\n",
      "Epoch 1002/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0087 - mae: 0.0289 - val_loss: 0.0209 - val_mae: 0.0416\n",
      "Epoch 1003/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0087 - mae: 0.0286 - val_loss: 0.0208 - val_mae: 0.0412\n",
      "Epoch 1004/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0087 - mae: 0.0287 - val_loss: 0.0208 - val_mae: 0.0416\n",
      "Epoch 1005/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0087 - mae: 0.0288 - val_loss: 0.0208 - val_mae: 0.0416\n",
      "Epoch 1006/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0087 - mae: 0.0286 - val_loss: 0.0208 - val_mae: 0.0416\n",
      "Epoch 1007/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0087 - mae: 0.0287 - val_loss: 0.0208 - val_mae: 0.0416\n",
      "Epoch 1008/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0087 - mae: 0.0285 - val_loss: 0.0208 - val_mae: 0.0413\n",
      "Epoch 1009/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0087 - mae: 0.0284 - val_loss: 0.0208 - val_mae: 0.0413\n",
      "Epoch 1010/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0087 - mae: 0.0286 - val_loss: 0.0208 - val_mae: 0.0414\n",
      "Epoch 1011/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0087 - mae: 0.0284 - val_loss: 0.0208 - val_mae: 0.0411\n",
      "Epoch 1012/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0086 - mae: 0.0285 - val_loss: 0.0208 - val_mae: 0.0412\n",
      "Epoch 1013/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0086 - mae: 0.0285 - val_loss: 0.0208 - val_mae: 0.0409\n",
      "Epoch 1014/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0086 - mae: 0.0285 - val_loss: 0.0208 - val_mae: 0.0410\n",
      "Epoch 1015/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0086 - mae: 0.0283 - val_loss: 0.0208 - val_mae: 0.0412\n",
      "Epoch 1016/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0086 - mae: 0.0283 - val_loss: 0.0208 - val_mae: 0.0409\n",
      "Epoch 1017/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0086 - mae: 0.0280 - val_loss: 0.0208 - val_mae: 0.0410\n",
      "Epoch 1018/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.0086 - mae: 0.0283 - val_loss: 0.0208 - val_mae: 0.0409\n",
      "Epoch 1019/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0086 - mae: 0.0283 - val_loss: 0.0208 - val_mae: 0.0409\n",
      "Epoch 1020/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0086 - mae: 0.0284 - val_loss: 0.0207 - val_mae: 0.0412\n",
      "Epoch 1021/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0086 - mae: 0.0281 - val_loss: 0.0207 - val_mae: 0.0407\n",
      "Epoch 1022/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0086 - mae: 0.0281 - val_loss: 0.0207 - val_mae: 0.0409\n",
      "Epoch 1023/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0086 - mae: 0.0283 - val_loss: 0.0208 - val_mae: 0.0410\n",
      "Epoch 1024/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0086 - mae: 0.0279 - val_loss: 0.0208 - val_mae: 0.0409\n",
      "Epoch 1025/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0086 - mae: 0.0280 - val_loss: 0.0207 - val_mae: 0.0406\n",
      "Epoch 1026/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0086 - mae: 0.0281 - val_loss: 0.0207 - val_mae: 0.0410\n",
      "Epoch 1027/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0086 - mae: 0.0281 - val_loss: 0.0208 - val_mae: 0.0405\n",
      "Epoch 1028/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0086 - mae: 0.0279 - val_loss: 0.0207 - val_mae: 0.0406\n",
      "Epoch 1029/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0085 - mae: 0.0278 - val_loss: 0.0207 - val_mae: 0.0406\n",
      "Epoch 1030/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.0085 - mae: 0.0278 - val_loss: 0.0207 - val_mae: 0.0404\n",
      "Epoch 1031/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0085 - mae: 0.0277 - val_loss: 0.0207 - val_mae: 0.0406\n",
      "Epoch 1032/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0085 - mae: 0.0277 - val_loss: 0.0207 - val_mae: 0.0408\n",
      "Epoch 1033/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0086 - mae: 0.0279 - val_loss: 0.0207 - val_mae: 0.0403\n",
      "Epoch 1034/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0085 - mae: 0.0276 - val_loss: 0.0207 - val_mae: 0.0404\n",
      "Epoch 1035/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0085 - mae: 0.0279 - val_loss: 0.0207 - val_mae: 0.0406\n",
      "Epoch 1036/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0085 - mae: 0.0277 - val_loss: 0.0207 - val_mae: 0.0404\n",
      "Epoch 1037/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0085 - mae: 0.0276 - val_loss: 0.0207 - val_mae: 0.0403\n",
      "Epoch 1038/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0085 - mae: 0.0274 - val_loss: 0.0207 - val_mae: 0.0402\n",
      "Epoch 1039/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0085 - mae: 0.0276 - val_loss: 0.0207 - val_mae: 0.0403\n",
      "Epoch 1040/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0085 - mae: 0.0276 - val_loss: 0.0207 - val_mae: 0.0401\n",
      "Epoch 1041/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0085 - mae: 0.0275 - val_loss: 0.0207 - val_mae: 0.0403\n",
      "Epoch 1042/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0085 - mae: 0.0275 - val_loss: 0.0207 - val_mae: 0.0403\n",
      "Epoch 1043/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0085 - mae: 0.0274 - val_loss: 0.0207 - val_mae: 0.0399\n",
      "Epoch 1044/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0085 - mae: 0.0275 - val_loss: 0.0207 - val_mae: 0.0400\n",
      "Epoch 1045/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0085 - mae: 0.0274 - val_loss: 0.0207 - val_mae: 0.0404\n",
      "Epoch 1046/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0085 - mae: 0.0272 - val_loss: 0.0206 - val_mae: 0.0399\n",
      "Epoch 1047/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0085 - mae: 0.0274 - val_loss: 0.0207 - val_mae: 0.0401\n",
      "Epoch 1048/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0085 - mae: 0.0274 - val_loss: 0.0206 - val_mae: 0.0403\n",
      "Epoch 1049/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0085 - mae: 0.0272 - val_loss: 0.0207 - val_mae: 0.0400\n",
      "Epoch 1050/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0084 - mae: 0.0271 - val_loss: 0.0206 - val_mae: 0.0398\n",
      "Epoch 1051/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0084 - mae: 0.0272 - val_loss: 0.0207 - val_mae: 0.0401\n",
      "Epoch 1052/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0084 - mae: 0.0271 - val_loss: 0.0206 - val_mae: 0.0399\n",
      "Epoch 1053/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0084 - mae: 0.0271 - val_loss: 0.0206 - val_mae: 0.0397\n",
      "Epoch 1054/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0084 - mae: 0.0274 - val_loss: 0.0206 - val_mae: 0.0398\n",
      "Epoch 1055/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0085 - mae: 0.0269 - val_loss: 0.0206 - val_mae: 0.0395\n",
      "Epoch 1056/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0084 - mae: 0.0272 - val_loss: 0.0206 - val_mae: 0.0398\n",
      "Epoch 1057/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0206 - val_mae: 0.0397\n",
      "Epoch 1058/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0206 - val_mae: 0.0395\n",
      "Epoch 1059/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0084 - mae: 0.0269 - val_loss: 0.0206 - val_mae: 0.0398\n",
      "Epoch 1060/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0084 - mae: 0.0269 - val_loss: 0.0206 - val_mae: 0.0395\n",
      "Epoch 1061/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0206 - val_mae: 0.0396\n",
      "Epoch 1062/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0084 - mae: 0.0268 - val_loss: 0.0206 - val_mae: 0.0396\n",
      "Epoch 1063/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0084 - mae: 0.0269 - val_loss: 0.0206 - val_mae: 0.0394\n",
      "Epoch 1064/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0084 - mae: 0.0267 - val_loss: 0.0206 - val_mae: 0.0392\n",
      "Epoch 1065/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0084 - mae: 0.0269 - val_loss: 0.0206 - val_mae: 0.0396\n",
      "Epoch 1066/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0084 - mae: 0.0266 - val_loss: 0.0206 - val_mae: 0.0391\n",
      "Epoch 1067/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0084 - mae: 0.0268 - val_loss: 0.0206 - val_mae: 0.0393\n",
      "Epoch 1068/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0084 - mae: 0.0270 - val_loss: 0.0206 - val_mae: 0.0395\n",
      "Epoch 1069/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0084 - mae: 0.0265 - val_loss: 0.0206 - val_mae: 0.0392\n",
      "Epoch 1070/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0084 - mae: 0.0265 - val_loss: 0.0206 - val_mae: 0.0389\n",
      "Epoch 1071/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0084 - mae: 0.0267 - val_loss: 0.0206 - val_mae: 0.0391\n",
      "Epoch 1072/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0083 - mae: 0.0266 - val_loss: 0.0206 - val_mae: 0.0390\n",
      "Epoch 1073/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0083 - mae: 0.0265 - val_loss: 0.0206 - val_mae: 0.0392\n",
      "Epoch 1074/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0083 - mae: 0.0266 - val_loss: 0.0206 - val_mae: 0.0393\n",
      "Epoch 1075/1500\n",
      "52/52 [==============================] - 0s 805us/step - loss: 0.0083 - mae: 0.0266 - val_loss: 0.0206 - val_mae: 0.0390\n",
      "Epoch 1076/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0083 - mae: 0.0263 - val_loss: 0.0206 - val_mae: 0.0390\n",
      "Epoch 1077/1500\n",
      "52/52 [==============================] - 0s 798us/step - loss: 0.0083 - mae: 0.0264 - val_loss: 0.0206 - val_mae: 0.0394\n",
      "Epoch 1078/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0083 - mae: 0.0262 - val_loss: 0.0206 - val_mae: 0.0390\n",
      "Epoch 1079/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0083 - mae: 0.0263 - val_loss: 0.0205 - val_mae: 0.0393\n",
      "Epoch 1080/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0083 - mae: 0.0265 - val_loss: 0.0205 - val_mae: 0.0390\n",
      "Epoch 1081/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0083 - mae: 0.0263 - val_loss: 0.0205 - val_mae: 0.0390\n",
      "Epoch 1082/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0083 - mae: 0.0265 - val_loss: 0.0206 - val_mae: 0.0389\n",
      "Epoch 1083/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0083 - mae: 0.0261 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1084/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0083 - mae: 0.0261 - val_loss: 0.0205 - val_mae: 0.0388\n",
      "Epoch 1085/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0083 - mae: 0.0263 - val_loss: 0.0205 - val_mae: 0.0388\n",
      "Epoch 1086/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0083 - mae: 0.0261 - val_loss: 0.0205 - val_mae: 0.0386\n",
      "Epoch 1087/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0083 - mae: 0.0259 - val_loss: 0.0205 - val_mae: 0.0387\n",
      "Epoch 1088/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0083 - mae: 0.0261 - val_loss: 0.0205 - val_mae: 0.0386\n",
      "Epoch 1089/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0083 - mae: 0.0263 - val_loss: 0.0205 - val_mae: 0.0387\n",
      "Epoch 1090/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0082 - mae: 0.0260 - val_loss: 0.0205 - val_mae: 0.0386\n",
      "Epoch 1091/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0083 - mae: 0.0259 - val_loss: 0.0205 - val_mae: 0.0389\n",
      "Epoch 1092/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0082 - mae: 0.0259 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1093/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0083 - mae: 0.0258 - val_loss: 0.0205 - val_mae: 0.0382\n",
      "Epoch 1094/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0083 - mae: 0.0261 - val_loss: 0.0205 - val_mae: 0.0387\n",
      "Epoch 1095/1500\n",
      "52/52 [==============================] - 0s 883us/step - loss: 0.0083 - mae: 0.0260 - val_loss: 0.0205 - val_mae: 0.0386\n",
      "Epoch 1096/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0082 - mae: 0.0258 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1097/1500\n",
      "52/52 [==============================] - 0s 793us/step - loss: 0.0082 - mae: 0.0260 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1098/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0082 - mae: 0.0258 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1099/1500\n",
      "52/52 [==============================] - 0s 796us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0383\n",
      "Epoch 1100/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0082 - mae: 0.0258 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1101/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0082 - mae: 0.0257 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1102/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0082 - mae: 0.0259 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1103/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0383\n",
      "Epoch 1104/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0082 - mae: 0.0258 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1105/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0082 - mae: 0.0257 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1106/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0384\n",
      "Epoch 1107/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0383\n",
      "Epoch 1108/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0082 - mae: 0.0255 - val_loss: 0.0205 - val_mae: 0.0379\n",
      "Epoch 1109/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0385\n",
      "Epoch 1110/1500\n",
      "52/52 [==============================] - 0s 800us/step - loss: 0.0082 - mae: 0.0255 - val_loss: 0.0205 - val_mae: 0.0380\n",
      "Epoch 1111/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0082 - mae: 0.0254 - val_loss: 0.0205 - val_mae: 0.0378\n",
      "Epoch 1112/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0082 - mae: 0.0256 - val_loss: 0.0205 - val_mae: 0.0381\n",
      "Epoch 1113/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0082 - mae: 0.0255 - val_loss: 0.0205 - val_mae: 0.0380\n",
      "Epoch 1114/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0082 - mae: 0.0254 - val_loss: 0.0205 - val_mae: 0.0383\n",
      "Epoch 1115/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0082 - mae: 0.0253 - val_loss: 0.0205 - val_mae: 0.0382\n",
      "Epoch 1116/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0081 - mae: 0.0253 - val_loss: 0.0205 - val_mae: 0.0381\n",
      "Epoch 1117/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0082 - mae: 0.0252 - val_loss: 0.0204 - val_mae: 0.0379\n",
      "Epoch 1118/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0082 - mae: 0.0251 - val_loss: 0.0205 - val_mae: 0.0378\n",
      "Epoch 1119/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0081 - mae: 0.0254 - val_loss: 0.0204 - val_mae: 0.0382\n",
      "Epoch 1120/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0081 - mae: 0.0252 - val_loss: 0.0204 - val_mae: 0.0377\n",
      "Epoch 1121/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0081 - mae: 0.0253 - val_loss: 0.0204 - val_mae: 0.0378\n",
      "Epoch 1122/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0082 - mae: 0.0251 - val_loss: 0.0205 - val_mae: 0.0381\n",
      "Epoch 1123/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0081 - mae: 0.0254 - val_loss: 0.0204 - val_mae: 0.0377\n",
      "Epoch 1124/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0082 - mae: 0.0253 - val_loss: 0.0204 - val_mae: 0.0376\n",
      "Epoch 1125/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0081 - mae: 0.0251 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1126/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0081 - mae: 0.0250 - val_loss: 0.0205 - val_mae: 0.0378\n",
      "Epoch 1127/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0081 - mae: 0.0251 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1128/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0081 - mae: 0.0249 - val_loss: 0.0205 - val_mae: 0.0378\n",
      "Epoch 1129/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0081 - mae: 0.0249 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1130/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0081 - mae: 0.0251 - val_loss: 0.0204 - val_mae: 0.0377\n",
      "Epoch 1131/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0081 - mae: 0.0252 - val_loss: 0.0204 - val_mae: 0.0376\n",
      "Epoch 1132/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0081 - mae: 0.0247 - val_loss: 0.0205 - val_mae: 0.0377\n",
      "Epoch 1133/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0081 - mae: 0.0250 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1134/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0081 - mae: 0.0249 - val_loss: 0.0204 - val_mae: 0.0374\n",
      "Epoch 1135/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.0081 - mae: 0.0250 - val_loss: 0.0204 - val_mae: 0.0376\n",
      "Epoch 1136/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0081 - mae: 0.0247 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1137/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0081 - mae: 0.0248 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1138/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0081 - mae: 0.0248 - val_loss: 0.0204 - val_mae: 0.0373\n",
      "Epoch 1139/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0081 - mae: 0.0250 - val_loss: 0.0204 - val_mae: 0.0374\n",
      "Epoch 1140/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0081 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0371\n",
      "Epoch 1141/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0081 - mae: 0.0248 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1142/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0081 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0371\n",
      "Epoch 1143/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0080 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1144/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0081 - mae: 0.0245 - val_loss: 0.0204 - val_mae: 0.0375\n",
      "Epoch 1145/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0081 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1146/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0081 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1147/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0080 - mae: 0.0245 - val_loss: 0.0204 - val_mae: 0.0371\n",
      "Epoch 1148/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0080 - mae: 0.0248 - val_loss: 0.0204 - val_mae: 0.0371\n",
      "Epoch 1149/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0080 - mae: 0.0243 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1150/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0080 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1151/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0080 - mae: 0.0244 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1152/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0080 - mae: 0.0245 - val_loss: 0.0204 - val_mae: 0.0371\n",
      "Epoch 1153/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0081 - mae: 0.0246 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1154/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0369\n",
      "Epoch 1155/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0081 - mae: 0.0244 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1156/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0080 - mae: 0.0244 - val_loss: 0.0204 - val_mae: 0.0368\n",
      "Epoch 1157/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0372\n",
      "Epoch 1158/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.0080 - mae: 0.0243 - val_loss: 0.0204 - val_mae: 0.0369\n",
      "Epoch 1159/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0370\n",
      "Epoch 1160/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0369\n",
      "Epoch 1161/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0080 - mae: 0.0244 - val_loss: 0.0204 - val_mae: 0.0369\n",
      "Epoch 1162/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1163/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0080 - mae: 0.0243 - val_loss: 0.0204 - val_mae: 0.0369\n",
      "Epoch 1164/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0367\n",
      "Epoch 1165/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0080 - mae: 0.0241 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1166/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0080 - mae: 0.0240 - val_loss: 0.0204 - val_mae: 0.0368\n",
      "Epoch 1167/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0080 - mae: 0.0241 - val_loss: 0.0203 - val_mae: 0.0366\n",
      "Epoch 1168/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.0080 - mae: 0.0241 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1169/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0080 - mae: 0.0241 - val_loss: 0.0204 - val_mae: 0.0368\n",
      "Epoch 1170/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0080 - mae: 0.0242 - val_loss: 0.0204 - val_mae: 0.0367\n",
      "Epoch 1171/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0080 - mae: 0.0240 - val_loss: 0.0204 - val_mae: 0.0364\n",
      "Epoch 1172/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0080 - mae: 0.0240 - val_loss: 0.0204 - val_mae: 0.0365\n",
      "Epoch 1173/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0080 - mae: 0.0239 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1174/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0079 - mae: 0.0238 - val_loss: 0.0204 - val_mae: 0.0365\n",
      "Epoch 1175/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0079 - mae: 0.0238 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1176/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0080 - mae: 0.0237 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1177/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0080 - mae: 0.0237 - val_loss: 0.0203 - val_mae: 0.0362\n",
      "Epoch 1178/1500\n",
      "52/52 [==============================] - 0s 795us/step - loss: 0.0079 - mae: 0.0238 - val_loss: 0.0203 - val_mae: 0.0365\n",
      "Epoch 1179/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0080 - mae: 0.0239 - val_loss: 0.0203 - val_mae: 0.0364\n",
      "Epoch 1180/1500\n",
      "52/52 [==============================] - 0s 814us/step - loss: 0.0080 - mae: 0.0239 - val_loss: 0.0204 - val_mae: 0.0365\n",
      "Epoch 1181/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0079 - mae: 0.0238 - val_loss: 0.0204 - val_mae: 0.0366\n",
      "Epoch 1182/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0204 - val_mae: 0.0364\n",
      "Epoch 1183/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0203 - val_mae: 0.0364\n",
      "Epoch 1184/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0079 - mae: 0.0238 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1185/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0203 - val_mae: 0.0362\n",
      "Epoch 1186/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0079 - mae: 0.0237 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1187/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1188/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1189/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0204 - val_mae: 0.0361\n",
      "Epoch 1190/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0360\n",
      "Epoch 1191/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0079 - mae: 0.0234 - val_loss: 0.0203 - val_mae: 0.0359\n",
      "Epoch 1192/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0203 - val_mae: 0.0362\n",
      "Epoch 1193/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0079 - mae: 0.0236 - val_loss: 0.0203 - val_mae: 0.0360\n",
      "Epoch 1194/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0079 - mae: 0.0234 - val_loss: 0.0203 - val_mae: 0.0362\n",
      "Epoch 1195/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1196/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0079 - mae: 0.0232 - val_loss: 0.0203 - val_mae: 0.0363\n",
      "Epoch 1197/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0079 - mae: 0.0234 - val_loss: 0.0203 - val_mae: 0.0361\n",
      "Epoch 1198/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0361\n",
      "Epoch 1199/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0360\n",
      "Epoch 1200/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0079 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0357\n",
      "Epoch 1201/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0079 - mae: 0.0233 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1202/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0079 - mae: 0.0235 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1203/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0079 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1204/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0079 - mae: 0.0233 - val_loss: 0.0203 - val_mae: 0.0359\n",
      "Epoch 1205/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0079 - mae: 0.0233 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1206/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0079 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1207/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0079 - mae: 0.0232 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1208/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0078 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0357\n",
      "Epoch 1209/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0079 - mae: 0.0232 - val_loss: 0.0203 - val_mae: 0.0360\n",
      "Epoch 1210/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0079 - mae: 0.0230 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1211/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0079 - mae: 0.0232 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1212/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0079 - mae: 0.0229 - val_loss: 0.0203 - val_mae: 0.0355\n",
      "Epoch 1213/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0078 - mae: 0.0230 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1214/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0078 - mae: 0.0230 - val_loss: 0.0203 - val_mae: 0.0355\n",
      "Epoch 1215/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0078 - mae: 0.0229 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1216/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0078 - mae: 0.0230 - val_loss: 0.0203 - val_mae: 0.0358\n",
      "Epoch 1217/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0079 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1218/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0079 - mae: 0.0231 - val_loss: 0.0203 - val_mae: 0.0356\n",
      "Epoch 1219/1500\n",
      "52/52 [==============================] - 0s 890us/step - loss: 0.0078 - mae: 0.0228 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1220/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0078 - mae: 0.0228 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1221/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0078 - mae: 0.0229 - val_loss: 0.0203 - val_mae: 0.0355\n",
      "Epoch 1222/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0079 - mae: 0.0228 - val_loss: 0.0203 - val_mae: 0.0355\n",
      "Epoch 1223/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0078 - mae: 0.0229 - val_loss: 0.0203 - val_mae: 0.0355\n",
      "Epoch 1224/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0078 - mae: 0.0227 - val_loss: 0.0203 - val_mae: 0.0354\n",
      "Epoch 1225/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0079 - mae: 0.0232 - val_loss: 0.0203 - val_mae: 0.0360\n",
      "Epoch 1226/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0078 - mae: 0.0228 - val_loss: 0.0203 - val_mae: 0.0354\n",
      "Epoch 1227/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0078 - mae: 0.0229 - val_loss: 0.0203 - val_mae: 0.0354\n",
      "Epoch 1228/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0078 - mae: 0.0227 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1229/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0078 - mae: 0.0225 - val_loss: 0.0203 - val_mae: 0.0350\n",
      "Epoch 1230/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0078 - mae: 0.0230 - val_loss: 0.0203 - val_mae: 0.0354\n",
      "Epoch 1231/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0354\n",
      "Epoch 1232/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0078 - mae: 0.0227 - val_loss: 0.0203 - val_mae: 0.0352\n",
      "Epoch 1233/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0351\n",
      "Epoch 1234/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1235/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0079 - mae: 0.0225 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1236/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0352\n",
      "Epoch 1237/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0078 - mae: 0.0225 - val_loss: 0.0203 - val_mae: 0.0352\n",
      "Epoch 1238/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0352\n",
      "Epoch 1239/1500\n",
      "52/52 [==============================] - 0s 830us/step - loss: 0.0078 - mae: 0.0224 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1240/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0078 - mae: 0.0224 - val_loss: 0.0203 - val_mae: 0.0351\n",
      "Epoch 1241/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0078 - mae: 0.0228 - val_loss: 0.0203 - val_mae: 0.0351\n",
      "Epoch 1242/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0353\n",
      "Epoch 1243/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0078 - mae: 0.0223 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1244/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0078 - mae: 0.0226 - val_loss: 0.0203 - val_mae: 0.0351\n",
      "Epoch 1245/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0077 - mae: 0.0225 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1246/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0078 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1247/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0078 - mae: 0.0223 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1248/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0077 - mae: 0.0224 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1249/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0078 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1250/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0077 - mae: 0.0221 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1251/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0078 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1252/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0078 - mae: 0.0225 - val_loss: 0.0203 - val_mae: 0.0351\n",
      "Epoch 1253/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0078 - mae: 0.0223 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1254/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0078 - mae: 0.0221 - val_loss: 0.0203 - val_mae: 0.0348\n",
      "Epoch 1255/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1256/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1257/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0350\n",
      "Epoch 1258/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0077 - mae: 0.0223 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1259/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0078 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0349\n",
      "Epoch 1260/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0077 - mae: 0.0221 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1261/1500\n",
      "52/52 [==============================] - 0s 877us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1262/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.0077 - mae: 0.0221 - val_loss: 0.0203 - val_mae: 0.0350\n",
      "Epoch 1263/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0345\n",
      "Epoch 1264/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.0077 - mae: 0.0223 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1265/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1266/1500\n",
      "52/52 [==============================] - 0s 885us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1267/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1268/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0077 - mae: 0.0219 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1269/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1270/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0346\n",
      "Epoch 1271/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0202 - val_mae: 0.0345\n",
      "Epoch 1272/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0077 - mae: 0.0222 - val_loss: 0.0203 - val_mae: 0.0347\n",
      "Epoch 1273/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0203 - val_mae: 0.0343\n",
      "Epoch 1274/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0203 - val_mae: 0.0344\n",
      "Epoch 1275/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0077 - mae: 0.0217 - val_loss: 0.0202 - val_mae: 0.0344\n",
      "Epoch 1276/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0077 - mae: 0.0219 - val_loss: 0.0202 - val_mae: 0.0345\n",
      "Epoch 1277/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0202 - val_mae: 0.0343\n",
      "Epoch 1278/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0202 - val_mae: 0.0343\n",
      "Epoch 1279/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0203 - val_mae: 0.0344\n",
      "Epoch 1280/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0203 - val_mae: 0.0345\n",
      "Epoch 1281/1500\n",
      "52/52 [==============================] - 0s 873us/step - loss: 0.0077 - mae: 0.0220 - val_loss: 0.0203 - val_mae: 0.0344\n",
      "Epoch 1282/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0203 - val_mae: 0.0344\n",
      "Epoch 1283/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0202 - val_mae: 0.0343\n",
      "Epoch 1284/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0077 - mae: 0.0218 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1285/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0077 - mae: 0.0217 - val_loss: 0.0202 - val_mae: 0.0342\n",
      "Epoch 1286/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0077 - mae: 0.0217 - val_loss: 0.0203 - val_mae: 0.0344\n",
      "Epoch 1287/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0340\n",
      "Epoch 1288/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0340\n",
      "Epoch 1289/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1290/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1291/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0077 - mae: 0.0217 - val_loss: 0.0202 - val_mae: 0.0342\n",
      "Epoch 1292/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0077 - mae: 0.0214 - val_loss: 0.0203 - val_mae: 0.0343\n",
      "Epoch 1293/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0076 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1294/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0077 - mae: 0.0217 - val_loss: 0.0203 - val_mae: 0.0342\n",
      "Epoch 1295/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0077 - mae: 0.0213 - val_loss: 0.0203 - val_mae: 0.0340\n",
      "Epoch 1296/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0076 - mae: 0.0214 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1297/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1298/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0076 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1299/1500\n",
      "52/52 [==============================] - 0s 860us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0203 - val_mae: 0.0340\n",
      "Epoch 1300/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0077 - mae: 0.0213 - val_loss: 0.0202 - val_mae: 0.0340\n",
      "Epoch 1301/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0202 - val_mae: 0.0342\n",
      "Epoch 1302/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0077 - mae: 0.0213 - val_loss: 0.0203 - val_mae: 0.0339\n",
      "Epoch 1303/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0076 - mae: 0.0213 - val_loss: 0.0202 - val_mae: 0.0337\n",
      "Epoch 1304/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0077 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0340\n",
      "Epoch 1305/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0077 - mae: 0.0214 - val_loss: 0.0202 - val_mae: 0.0337\n",
      "Epoch 1306/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0077 - mae: 0.0211 - val_loss: 0.0203 - val_mae: 0.0339\n",
      "Epoch 1307/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0076 - mae: 0.0212 - val_loss: 0.0202 - val_mae: 0.0341\n",
      "Epoch 1308/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0076 - mae: 0.0215 - val_loss: 0.0202 - val_mae: 0.0340\n",
      "Epoch 1309/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0337\n",
      "Epoch 1310/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0077 - mae: 0.0216 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1311/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0076 - mae: 0.0212 - val_loss: 0.0203 - val_mae: 0.0338\n",
      "Epoch 1312/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0203 - val_mae: 0.0339\n",
      "Epoch 1313/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1314/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0076 - mae: 0.0212 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1315/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0338\n",
      "Epoch 1316/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0203 - val_mae: 0.0337\n",
      "Epoch 1317/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1318/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0338\n",
      "Epoch 1319/1500\n",
      "52/52 [==============================] - 0s 875us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1320/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0337\n",
      "Epoch 1321/1500\n",
      "52/52 [==============================] - 0s 824us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0335\n",
      "Epoch 1322/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0337\n",
      "Epoch 1323/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0335\n",
      "Epoch 1324/1500\n",
      "52/52 [==============================] - 0s 812us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0335\n",
      "Epoch 1325/1500\n",
      "52/52 [==============================] - 0s 801us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1326/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0076 - mae: 0.0209 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1327/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0076 - mae: 0.0209 - val_loss: 0.0202 - val_mae: 0.0339\n",
      "Epoch 1328/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1329/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0076 - mae: 0.0212 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1330/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0076 - mae: 0.0209 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1331/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0203 - val_mae: 0.0335\n",
      "Epoch 1332/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0076 - mae: 0.0211 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1333/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0076 - mae: 0.0209 - val_loss: 0.0202 - val_mae: 0.0335\n",
      "Epoch 1334/1500\n",
      "52/52 [==============================] - 0s 806us/step - loss: 0.0076 - mae: 0.0208 - val_loss: 0.0203 - val_mae: 0.0334\n",
      "Epoch 1335/1500\n",
      "52/52 [==============================] - 0s 797us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1336/1500\n",
      "52/52 [==============================] - 0s 813us/step - loss: 0.0076 - mae: 0.0208 - val_loss: 0.0202 - val_mae: 0.0334\n",
      "Epoch 1337/1500\n",
      "52/52 [==============================] - 0s 807us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0203 - val_mae: 0.0336\n",
      "Epoch 1338/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0336\n",
      "Epoch 1339/1500\n",
      "52/52 [==============================] - 0s 810us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1340/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1341/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0203 - val_mae: 0.0336\n",
      "Epoch 1342/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0076 - mae: 0.0210 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1343/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0076 - mae: 0.0206 - val_loss: 0.0202 - val_mae: 0.0334\n",
      "Epoch 1344/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0076 - mae: 0.0206 - val_loss: 0.0203 - val_mae: 0.0333\n",
      "Epoch 1345/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0076 - mae: 0.0209 - val_loss: 0.0202 - val_mae: 0.0334\n",
      "Epoch 1346/1500\n",
      "52/52 [==============================] - 0s 809us/step - loss: 0.0075 - mae: 0.0206 - val_loss: 0.0202 - val_mae: 0.0332\n",
      "Epoch 1347/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0076 - mae: 0.0204 - val_loss: 0.0203 - val_mae: 0.0333\n",
      "Epoch 1348/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0076 - mae: 0.0208 - val_loss: 0.0202 - val_mae: 0.0334\n",
      "Epoch 1349/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0076 - mae: 0.0206 - val_loss: 0.0202 - val_mae: 0.0330\n",
      "Epoch 1350/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0202 - val_mae: 0.0332\n",
      "Epoch 1351/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0076 - mae: 0.0206 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1352/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0075 - mae: 0.0205 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1353/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0076 - mae: 0.0205 - val_loss: 0.0202 - val_mae: 0.0332\n",
      "Epoch 1354/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0075 - mae: 0.0205 - val_loss: 0.0202 - val_mae: 0.0331\n",
      "Epoch 1355/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0075 - mae: 0.0204 - val_loss: 0.0202 - val_mae: 0.0332\n",
      "Epoch 1356/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0076 - mae: 0.0207 - val_loss: 0.0202 - val_mae: 0.0334\n",
      "Epoch 1357/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.0075 - mae: 0.0205 - val_loss: 0.0203 - val_mae: 0.0331\n",
      "Epoch 1358/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0075 - mae: 0.0205 - val_loss: 0.0202 - val_mae: 0.0331\n",
      "Epoch 1359/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0075 - mae: 0.0204 - val_loss: 0.0203 - val_mae: 0.0331\n",
      "Epoch 1360/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0075 - mae: 0.0204 - val_loss: 0.0202 - val_mae: 0.0331\n",
      "Epoch 1361/1500\n",
      "52/52 [==============================] - 0s 881us/step - loss: 0.0075 - mae: 0.0206 - val_loss: 0.0202 - val_mae: 0.0332\n",
      "Epoch 1362/1500\n",
      "52/52 [==============================] - 0s 864us/step - loss: 0.0076 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0330\n",
      "Epoch 1363/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0075 - mae: 0.0203 - val_loss: 0.0202 - val_mae: 0.0333\n",
      "Epoch 1364/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0076 - mae: 0.0208 - val_loss: 0.0203 - val_mae: 0.0331\n",
      "Epoch 1365/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0330\n",
      "Epoch 1366/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0075 - mae: 0.0203 - val_loss: 0.0203 - val_mae: 0.0331\n",
      "Epoch 1367/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0075 - mae: 0.0204 - val_loss: 0.0202 - val_mae: 0.0330\n",
      "Epoch 1368/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0202 - val_mae: 0.0330\n",
      "Epoch 1369/1500\n",
      "52/52 [==============================] - 0s 887us/step - loss: 0.0075 - mae: 0.0204 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1370/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0202 - val_mae: 0.0330\n",
      "Epoch 1371/1500\n",
      "52/52 [==============================] - 0s 851us/step - loss: 0.0075 - mae: 0.0203 - val_loss: 0.0202 - val_mae: 0.0331\n",
      "Epoch 1372/1500\n",
      "52/52 [==============================] - 0s 869us/step - loss: 0.0075 - mae: 0.0201 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1373/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0202 - val_mae: 0.0330\n",
      "Epoch 1374/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0329\n",
      "Epoch 1375/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1376/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0075 - mae: 0.0201 - val_loss: 0.0203 - val_mae: 0.0328\n",
      "Epoch 1377/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0075 - mae: 0.0203 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1378/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0329\n",
      "Epoch 1379/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0075 - mae: 0.0201 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1380/1500\n",
      "52/52 [==============================] - 0s 848us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1381/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0202 - val_mae: 0.0327\n",
      "Epoch 1382/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0329\n",
      "Epoch 1383/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0075 - mae: 0.0201 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1384/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0202 - val_mae: 0.0326\n",
      "Epoch 1385/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1386/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1387/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0202 - val_mae: 0.0329\n",
      "Epoch 1388/1500\n",
      "52/52 [==============================] - 0s 859us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0202 - val_mae: 0.0327\n",
      "Epoch 1389/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0075 - mae: 0.0201 - val_loss: 0.0202 - val_mae: 0.0326\n",
      "Epoch 1390/1500\n",
      "52/52 [==============================] - 0s 861us/step - loss: 0.0075 - mae: 0.0202 - val_loss: 0.0202 - val_mae: 0.0326\n",
      "Epoch 1391/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1392/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0203 - val_mae: 0.0326\n",
      "Epoch 1393/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1394/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0202 - val_mae: 0.0327\n",
      "Epoch 1395/1500\n",
      "52/52 [==============================] - 0s 867us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0202 - val_mae: 0.0325\n",
      "Epoch 1396/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0203 - val_mae: 0.0326\n",
      "Epoch 1397/1500\n",
      "52/52 [==============================] - 0s 850us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1398/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0328\n",
      "Epoch 1399/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1400/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0326\n",
      "Epoch 1401/1500\n",
      "52/52 [==============================] - 0s 804us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1402/1500\n",
      "52/52 [==============================] - 0s 818us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1403/1500\n",
      "52/52 [==============================] - 0s 827us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0327\n",
      "Epoch 1404/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0075 - mae: 0.0200 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1405/1500\n",
      "52/52 [==============================] - 0s 816us/step - loss: 0.0074 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1406/1500\n",
      "52/52 [==============================] - 0s 846us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1407/1500\n",
      "52/52 [==============================] - 0s 872us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1408/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0075 - mae: 0.0198 - val_loss: 0.0202 - val_mae: 0.0323\n",
      "Epoch 1409/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0075 - mae: 0.0198 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1410/1500\n",
      "52/52 [==============================] - 0s 823us/step - loss: 0.0075 - mae: 0.0199 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1411/1500\n",
      "52/52 [==============================] - 0s 820us/step - loss: 0.0075 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1412/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1413/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0202 - val_mae: 0.0323\n",
      "Epoch 1414/1500\n",
      "52/52 [==============================] - 0s 826us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1415/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0074 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1416/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1417/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1418/1500\n",
      "52/52 [==============================] - 0s 878us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1419/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1420/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0075 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1421/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1422/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0074 - mae: 0.0196 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1423/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1424/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0325\n",
      "Epoch 1425/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1426/1500\n",
      "52/52 [==============================] - 0s 838us/step - loss: 0.0075 - mae: 0.0197 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1427/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1428/1500\n",
      "52/52 [==============================] - 0s 834us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0202 - val_mae: 0.0322\n",
      "Epoch 1429/1500\n",
      "52/52 [==============================] - 0s 866us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1430/1500\n",
      "52/52 [==============================] - 0s 870us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0324\n",
      "Epoch 1431/1500\n",
      "52/52 [==============================] - 0s 855us/step - loss: 0.0074 - mae: 0.0195 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1432/1500\n",
      "52/52 [==============================] - 0s 825us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1433/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1434/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1435/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1436/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1437/1500\n",
      "52/52 [==============================] - 0s 819us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1438/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0322\n",
      "Epoch 1439/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1440/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1441/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1442/1500\n",
      "52/52 [==============================] - 0s 900us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0323\n",
      "Epoch 1443/1500\n",
      "52/52 [==============================] - 0s 815us/step - loss: 0.0074 - mae: 0.0194 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1444/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1445/1500\n",
      "52/52 [==============================] - 0s 884us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1446/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1447/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0074 - mae: 0.0193 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1448/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1449/1500\n",
      "52/52 [==============================] - 0s 882us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0204 - val_mae: 0.0319\n",
      "Epoch 1450/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1451/1500\n",
      "52/52 [==============================] - 0s 891us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0321\n",
      "Epoch 1452/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1453/1500\n",
      "52/52 [==============================] - 0s 863us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1454/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1455/1500\n",
      "52/52 [==============================] - 0s 865us/step - loss: 0.0074 - mae: 0.0192 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1456/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1457/1500\n",
      "52/52 [==============================] - 0s 856us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1458/1500\n",
      "52/52 [==============================] - 0s 858us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1459/1500\n",
      "52/52 [==============================] - 0s 854us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1460/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1461/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1462/1500\n",
      "52/52 [==============================] - 0s 852us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1463/1500\n",
      "52/52 [==============================] - 0s 849us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0320\n",
      "Epoch 1464/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0317\n",
      "Epoch 1465/1500\n",
      "52/52 [==============================] - 0s 847us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1466/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1467/1500\n",
      "52/52 [==============================] - 0s 832us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1468/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0319\n",
      "Epoch 1469/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0073 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0317\n",
      "Epoch 1470/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0073 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1471/1500\n",
      "52/52 [==============================] - 0s 835us/step - loss: 0.0074 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0318\n",
      "Epoch 1472/1500\n",
      "52/52 [==============================] - 0s 822us/step - loss: 0.0073 - mae: 0.0188 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1473/1500\n",
      "52/52 [==============================] - 0s 821us/step - loss: 0.0074 - mae: 0.0191 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1474/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0074 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1475/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0074 - mae: 0.0190 - val_loss: 0.0204 - val_mae: 0.0321\n",
      "Epoch 1476/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0073 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0317\n",
      "Epoch 1477/1500\n",
      "52/52 [==============================] - 0s 836us/step - loss: 0.0074 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0318\n",
      "Epoch 1478/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0203 - val_mae: 0.0317\n",
      "Epoch 1479/1500\n",
      "52/52 [==============================] - 0s 853us/step - loss: 0.0073 - mae: 0.0189 - val_loss: 0.0203 - val_mae: 0.0318\n",
      "Epoch 1480/1500\n",
      "52/52 [==============================] - 0s 811us/step - loss: 0.0073 - mae: 0.0188 - val_loss: 0.0204 - val_mae: 0.0315\n",
      "Epoch 1481/1500\n",
      "52/52 [==============================] - 0s 817us/step - loss: 0.0073 - mae: 0.0185 - val_loss: 0.0203 - val_mae: 0.0316\n",
      "Epoch 1482/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0074 - mae: 0.0189 - val_loss: 0.0204 - val_mae: 0.0320\n",
      "Epoch 1483/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0074 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0318\n",
      "Epoch 1484/1500\n",
      "52/52 [==============================] - 0s 839us/step - loss: 0.0073 - mae: 0.0186 - val_loss: 0.0203 - val_mae: 0.0314\n",
      "Epoch 1485/1500\n",
      "52/52 [==============================] - 0s 871us/step - loss: 0.0073 - mae: 0.0188 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1486/1500\n",
      "52/52 [==============================] - 0s 828us/step - loss: 0.0073 - mae: 0.0188 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1487/1500\n",
      "52/52 [==============================] - 0s 845us/step - loss: 0.0073 - mae: 0.0185 - val_loss: 0.0204 - val_mae: 0.0316\n",
      "Epoch 1488/1500\n",
      "52/52 [==============================] - 0s 831us/step - loss: 0.0073 - mae: 0.0186 - val_loss: 0.0203 - val_mae: 0.0316\n",
      "Epoch 1489/1500\n",
      "52/52 [==============================] - 0s 892us/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1490/1500\n",
      "52/52 [==============================] - 0s 844us/step - loss: 0.0073 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1491/1500\n",
      "52/52 [==============================] - 0s 857us/step - loss: 0.0073 - mae: 0.0185 - val_loss: 0.0204 - val_mae: 0.0315\n",
      "Epoch 1492/1500\n",
      "52/52 [==============================] - 0s 862us/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1493/1500\n",
      "52/52 [==============================] - 0s 843us/step - loss: 0.0074 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0315\n",
      "Epoch 1494/1500\n",
      "52/52 [==============================] - 0s 833us/step - loss: 0.0073 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1495/1500\n",
      "52/52 [==============================] - 0s 829us/step - loss: 0.0073 - mae: 0.0184 - val_loss: 0.0204 - val_mae: 0.0316\n",
      "Epoch 1496/1500\n",
      "52/52 [==============================] - 0s 837us/step - loss: 0.0074 - mae: 0.0187 - val_loss: 0.0204 - val_mae: 0.0315\n",
      "Epoch 1497/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0073 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Epoch 1498/1500\n",
      "52/52 [==============================] - 0s 840us/step - loss: 0.0073 - mae: 0.0184 - val_loss: 0.0204 - val_mae: 0.0316\n",
      "Epoch 1499/1500\n",
      "52/52 [==============================] - 0s 842us/step - loss: 0.0074 - mae: 0.0186 - val_loss: 0.0204 - val_mae: 0.0314\n",
      "Epoch 1500/1500\n",
      "52/52 [==============================] - 0s 841us/step - loss: 0.0073 - mae: 0.0185 - val_loss: 0.0204 - val_mae: 0.0317\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8)                 48        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 45        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model and train it\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu')) # relu is used for performance\n",
    "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae']) # ook adam ke proberen kan zijn dat het netwerk hiermee beter leert\n",
    "# model.compile(optimizer='ftrl', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(inputs_train, outputs_train, epochs=1500, batch_size=8, validation_data=(inputs_validate, outputs_validate))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "gmiRMLprEbZf",
    "outputId": "73f32a27-735e-4ad0-91ff-959ad8e0de97"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'g.', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b.', label=\"val loss\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNp0t-ELAf5t"
   },
   "source": [
    "#Run with Test Data\n",
    "Put our test data into the model and plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "kRxeKbcPAl1W",
    "outputId": "3d93a119-87d0-495e-9914-5caff7f8cc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 786us/step\n",
      "predictions =\n",
      " [[0.014 0.    0.022 0.965 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.994 0.    0.    0.006 0.   ]\n",
      " [0.024 0.    0.029 0.947 0.   ]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.    0.957 0.003 0.04 ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.698 0.    0.219 0.015 0.067]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.967 0.003 0.03 ]\n",
      " [0.    0.    0.967 0.003 0.031]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.005 0.    0.947 0.048 0.   ]\n",
      " [0.093 0.    0.    0.907 0.   ]\n",
      " [0.    0.    0.034 0.    0.966]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.003 0.    0.957 0.041 0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.    0.957 0.003 0.04 ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.974 0.    0.    0.026]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.014 0.    0.022 0.965 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.996 0.    0.    0.004 0.   ]\n",
      " [0.094 0.    0.    0.906 0.   ]\n",
      " [0.497 0.    0.001 0.502 0.   ]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.094 0.    0.    0.906 0.   ]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.93  0.    0.036 0.034 0.   ]\n",
      " [0.    0.013 0.006 0.    0.981]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.96  0.    0.    0.04 ]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.    0.013 0.006 0.    0.981]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.    0.013 0.006 0.    0.981]\n",
      " [0.998 0.    0.    0.002 0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.929 0.    0.037 0.034 0.   ]\n",
      " [0.966 0.    0.    0.034 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.    0.034 0.    0.966]\n",
      " [0.094 0.    0.    0.906 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.96  0.    0.    0.04 ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.    0.035 0.    0.965]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.953 0.    0.028 0.02  0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.999 0.    0.    0.001 0.   ]\n",
      " [0.    0.    0.967 0.003 0.03 ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.095 0.    0.    0.905 0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.005 0.    0.947 0.048 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.024 0.    0.028 0.947 0.   ]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.    0.    0.641 0.    0.359]\n",
      " [0.    0.992 0.    0.    0.008]\n",
      " [0.    0.    0.967 0.003 0.031]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.046 0.002 0.    0.952]\n",
      " [0.094 0.    0.    0.906 0.   ]\n",
      " [0.    0.    0.074 0.    0.926]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.938 0.    0.    0.062 0.   ]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.    0.    0.035 0.    0.965]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.014 0.    0.022 0.965 0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.002 0.    0.889 0.109 0.   ]\n",
      " [0.929 0.    0.037 0.034 0.   ]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.    0.983 0.    0.    0.017]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.    0.    0.967 0.003 0.03 ]\n",
      " [0.929 0.    0.037 0.034 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.915 0.    0.    0.084]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [0.003 0.    0.957 0.04  0.   ]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.012 0.006 0.    0.982]\n",
      " [0.014 0.    0.022 0.964 0.   ]\n",
      " [0.003 0.    0.959 0.038 0.   ]\n",
      " [0.024 0.    0.    0.976 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.998 0.    0.    0.002]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.003 0.997]\n",
      " [0.    0.047 0.002 0.    0.95 ]\n",
      " [0.007 0.008 0.001 0.983 0.   ]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.003 0.    0.957 0.04  0.   ]\n",
      " [0.    0.012 0.006 0.    0.982]\n",
      " [0.    0.    0.987 0.005 0.008]\n",
      " [0.003 0.    0.958 0.039 0.   ]\n",
      " [0.005 0.    0.948 0.047 0.   ]\n",
      " [0.    0.    0.034 0.    0.966]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.013 0.006 0.    0.981]\n",
      " [0.    0.995 0.    0.    0.005]\n",
      " [0.    0.997 0.    0.    0.003]\n",
      " [0.996 0.    0.    0.004 0.   ]]\n",
      "actual =\n",
      " [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[[0.78571429 0.         0.         0.04761905 0.        ]\n",
      " [0.         0.97916667 0.         0.         0.        ]\n",
      " [0.         0.         0.96153846 0.         0.06666667]\n",
      " [0.14285714 0.         0.         0.95238095 0.        ]\n",
      " [0.07142857 0.02083333 0.03846154 0.         0.93333333]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f10980b2ac0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the model to predict the test inputs\n",
    "predictions = model.predict(inputs_test)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# print the predictions and the expected ouputs\n",
    "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
    "print(\"actual =\\n\", outputs_test)\n",
    "\n",
    "# sns.regplot(outputs_test, predictions)\n",
    "#Predict\n",
    "y_prediction = np.argmax(predictions, axis = 1)\n",
    "\n",
    "y_test=np.argmax(outputs_test, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "disp =  ConfusionMatrixDisplay(confusion_matrix=result)\n",
    "print(result)\n",
    "disp.plot()\n",
    "# Plot the predictions along with to the test data\n",
    "# plt.clf()\n",
    "# plt.title('Training data predicted vs actual values')\n",
    "# plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
    "# plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "[[9.9989247e-01 1.4909005e-20 1.2927471e-13 1.0756188e-04 5.0500826e-27]]\n",
      "0\n",
      "laptop\n"
     ]
    }
   ],
   "source": [
    "array_test = [[33., 0.152, 0.83, 27., 18.]]\n",
    "pred_test = model.predict(array_test)\n",
    "\n",
    "print(pred_test)\n",
    "class_index = np.argmax(pred_test)\n",
    "print(class_index)\n",
    "print(CLASSES[class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_saved/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
