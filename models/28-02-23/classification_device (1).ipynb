{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmIXglOU_ub6"
   },
   "source": [
    "# Setup Python Environment\n",
    "The next cell sets up the dependencies in required for the notebook, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sz8AJT14z0Tk",
    "outputId": "e4989b99-b845-4e34-828d-d0347d6ae6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.24.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (4.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Requirement already satisfied: tensorflow==2.10.0-rc1 in /opt/conda/lib/python3.9/site-packages (2.10.0rc1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.43.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.12.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (21.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (23.1.21)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (15.0.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (4.0.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.6.3)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.19.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (59.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (0.30.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.24.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0rc0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.10.0-rc1) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.10.0-rc1) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.3.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow==2.10.0-rc1) (3.0.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (4.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (2021.10.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.10.0-rc1) (3.2.0)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Setup environment\n",
    "!apt-get -qq install xxd\n",
    "!pip install pandas numpy matplotlib\n",
    "!pip install tensorflow==2.10.0-rc1\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lTI_Sx0_90e"
   },
   "source": [
    "# Train Neural Network\n",
    "## Parse and prepare the data\n",
    "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVOiR1uQKzpy",
    "outputId": "e2797aae-fc03-4def-a699-308a495dce0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 09:25:44.595240: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 09:25:44.657771: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-28 09:25:44.675957: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-28 09:25:45.038205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 09:25:45.038241: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-28 09:25:45.038245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version = 2.10.0-rc1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import fileinput\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Ftrl\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
    "\n",
    "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
    "# the same random numbers each time the notebook is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['box_better', 'laptop-better', 'pc_better', 'phone_better', 'printer', 'printer_better']\n"
     ]
    }
   ],
   "source": [
    "# SEED = 1337\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "# SEED=int(time.time())\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)\n",
    "\n",
    "CLASSES = [];\n",
    "\n",
    "for file in os.listdir(\"../data/own-csv\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        CLASSES.append(os.path.splitext(file)[0])\n",
    "\n",
    "CLASSES.sort()\n",
    "\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "YxYCUqzTAJeX",
    "outputId": "f5942731-9f3c-4ebf-ae67-d3c44d958173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;4mbox_better\u001b[0m class will be output \u001b[32m0\u001b[0m of the classifier\n",
      "49 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mlaptop-better\u001b[0m class will be output \u001b[32m1\u001b[0m of the classifier\n",
      "55 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mpc_better\u001b[0m class will be output \u001b[32m2\u001b[0m of the classifier\n",
      "54 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mphone_better\u001b[0m class will be output \u001b[32m3\u001b[0m of the classifier\n",
      "84 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mprinter\u001b[0m class will be output \u001b[32m4\u001b[0m of the classifier\n",
      "5989 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "\u001b[32;4mprinter_better\u001b[0m class will be output \u001b[32m5\u001b[0m of the classifier\n",
      "51 samples captured for training with inputs ['ApparentPower', 'Current', 'Factor', 'Power', 'ReactivePower'] \n",
      "\n",
      "['box_better', 'laptop-better', 'pc_better', 'phone_better', 'printer', 'printer_better']\n",
      "[12.     0.054  0.25   3.    11.    12.     0.054  0.24   3.    12.\n",
      " 12.     0.054  0.25   3.    12.    12.     0.054  0.25   3.    11.\n",
      " 12.     0.053  0.25   3.    11.    12.     0.054  0.24   3.    12.\n",
      " 12.     0.053  0.24   3.    11.    12.     0.053  0.24   3.    11.\n",
      " 12.     0.054  0.22   3.    12.    12.     0.054  0.25   3.    12.   ]\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "6282\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# create a one-hot encoded matrix that is used in the output\n",
    "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "SAMPLES_PER_CLASS = 10 \n",
    "\n",
    "# read each csv file and push an input and output\n",
    "for class_index in range(NUM_CLASSES):\n",
    "  objectClass = CLASSES[class_index]\n",
    "  df = pd.read_csv(\"../data/own-csv/\" + objectClass + \".csv\") #, parse_dates=['timestamp']\n",
    "  df = df.drop(columns=['time'])\n",
    "  df = df.drop(columns=['Voltage'])\n",
    "  columns = list(df)\n",
    "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
    "  df = df.dropna()\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  # calculate the number of objectClass recordings in the file\n",
    "  num_recordings = int(df.shape[0] / SAMPLES_PER_CLASS)\n",
    "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
    "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
    "  \n",
    "  #tensors\n",
    "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
    "  # for i in range(num_recordings):\n",
    "  #   tensor = []\n",
    "  #   row = []\n",
    "  #   for c in columns:           #columns\n",
    "  #     row.append(df[c][i])\n",
    "  #   tensor += row\n",
    "  #   inputs.append(tensor)\n",
    "  #   outputs.append(output)\n",
    "  for i in range(num_recordings):\n",
    "    tensor = []\n",
    "    for j in range(SAMPLES_PER_CLASS):\n",
    "        index = i * SAMPLES_PER_CLASS + j\n",
    "        tensor += [\n",
    "            df['ApparentPower'][index],\n",
    "            df['Current'][index],\n",
    "            df['Factor'][index],\n",
    "            df['Power'][index],\n",
    "            df['ReactivePower'][index],        \n",
    "        ]\n",
    "    inputs.append(tensor)\n",
    "    outputs.append(output)\n",
    "\n",
    "# convert the list to numpy array\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array(outputs)\n",
    "num_inputs_2 = len(inputs)\n",
    "print(CLASSES)\n",
    "print(inputs[0])\n",
    "print(outputs)\n",
    "print(num_inputs_2)\n",
    "\n",
    "# print(\"Data set parsing and preparation complete.\")\n",
    "# startTimestamp = df['timestamp'].iloc[0]\n",
    "# endTimestamp = df['timestamp'].iloc[-1]\n",
    "# print(endTimestamp - startTimestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OGt81LcfVetr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set randomization and splitting complete.\n",
      "[[10.     0.045  0.4   ...  0.41   4.     9.   ]\n",
      " [10.     0.044  0.38  ...  0.38   4.     9.   ]\n",
      " [10.     0.044  0.4   ...  0.41   4.     9.   ]\n",
      " ...\n",
      " [10.     0.047  0.37  ...  0.38   4.     9.   ]\n",
      " [10.     0.044  0.4   ...  0.4    4.     9.   ]\n",
      " [ 9.     0.039  0.55  ...  0.53   7.    11.   ]]\n",
      "6282\n"
     ]
    }
   ],
   "source": [
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "# https://stackoverflow.com/a/37710486/2020087\n",
    "num_inputs = len(inputs)\n",
    "randomize = np.arange(num_inputs)\n",
    "np.random.shuffle(randomize)\n",
    "\n",
    "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
    "inputs = inputs[randomize]\n",
    "outputs = outputs[randomize]\n",
    "\n",
    "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
    "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
    "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
    "\n",
    "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "print(\"Data set randomization and splitting complete.\")\n",
    "print(inputs_test)\n",
    "print(num_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Enr4twhJJgex"
   },
   "source": [
    "# Test code voor de tijd in de csv om te zetten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_snKH6OAPx4"
   },
   "source": [
    "#Build & Train the Model\n",
    "Build and train a TensorFlow model using the high-level Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "Ic2Z4XtgAaNh",
    "outputId": "ed119d8f-6237-4813-9575-9ae579eea919"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 09:25:46.639599: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-02-28 09:25:46.639617: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: b1154ec67e81\n",
      "2023-02-28 09:25:46.639621: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: b1154ec67e81\n",
      "2023-02-28 09:25:46.639659: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.78.1\n",
      "2023-02-28 09:25:46.639669: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.78.1\n",
      "2023-02-28 09:25:46.639672: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.78.1\n",
      "2023-02-28 09:25:46.640006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 528us/step\n",
      "epoch:  10  | loss:  0.015226559713482857 | val_loss:  0.014850172214210033 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 471us/step\n",
      "epoch:  20  | loss:  0.014920353889465332 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 501us/step\n",
      "epoch:  30  | loss:  0.014787527732551098 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  40  | loss:  0.014769621193408966 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 375us/step\n",
      "epoch:  50  | loss:  0.014776233583688736 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 384us/step\n",
      "epoch:  60  | loss:  0.014769651927053928 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 457us/step\n",
      "epoch:  70  | loss:  0.01476952712982893 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 445us/step\n",
      "epoch:  80  | loss:  0.01480373926460743 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 445us/step\n",
      "epoch:  90  | loss:  0.01476985402405262 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 371us/step\n",
      "epoch:  100  | loss:  0.014772715047001839 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 379us/step\n",
      "epoch:  110  | loss:  0.014769895933568478 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  120  | loss:  0.01476951502263546 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 432us/step\n",
      "epoch:  130  | loss:  0.01477243285626173 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 410us/step\n",
      "epoch:  140  | loss:  0.014768823981285095 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 378us/step\n",
      "epoch:  150  | loss:  0.014769598841667175 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 382us/step\n",
      "epoch:  160  | loss:  0.014770138077437878 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 387us/step\n",
      "epoch:  170  | loss:  0.014769759960472584 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 410us/step\n",
      "epoch:  180  | loss:  0.014771472662687302 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 398us/step\n",
      "epoch:  190  | loss:  0.0147976353764534 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  200  | loss:  0.014797776006162167 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 409us/step\n",
      "epoch:  210  | loss:  0.014770022593438625 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 414us/step\n",
      "epoch:  220  | loss:  0.0147696016356349 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 457us/step\n",
      "epoch:  230  | loss:  0.014769483357667923 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 381us/step\n",
      "epoch:  240  | loss:  0.014769373461604118 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 365us/step\n",
      "epoch:  250  | loss:  0.014769588597118855 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 392us/step\n",
      "epoch:  260  | loss:  0.014770349487662315 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  270  | loss:  0.014769433997571468 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 492us/step\n",
      "epoch:  280  | loss:  0.014769608154892921 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 383us/step\n",
      "epoch:  290  | loss:  0.01476973481476307 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 377us/step\n",
      "epoch:  300  | loss:  0.014769629575312138 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 504us/step\n",
      "epoch:  310  | loss:  0.014769624918699265 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 447us/step\n",
      "epoch:  320  | loss:  0.01476877648383379 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 428us/step\n",
      "epoch:  330  | loss:  0.0147276371717453 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 446us/step\n",
      "epoch:  340  | loss:  0.014769762754440308 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 397us/step\n",
      "epoch:  350  | loss:  0.01476940605789423 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 374us/step\n",
      "epoch:  360  | loss:  0.014747986570000648 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 401us/step\n",
      "epoch:  370  | loss:  0.014793569222092628 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 408us/step\n",
      "epoch:  380  | loss:  0.014769628643989563 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 423us/step\n",
      "epoch:  390  | loss:  0.014752970077097416 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 398us/step\n",
      "epoch:  400  | loss:  0.014768234454095364 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  410  | loss:  0.014777685515582561 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 383us/step\n",
      "epoch:  420  | loss:  0.01474180817604065 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 420us/step\n",
      "epoch:  430  | loss:  0.014681478962302208 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  440  | loss:  0.014706435613334179 | val_loss:  0.014850173145532608 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  450  | loss:  0.014620618894696236 | val_loss:  0.014744216576218605 | accuracy:  0.9554494828957836\n",
      "40/40 [==============================] - 0s 390us/step\n",
      "epoch:  460  | loss:  0.01424279622733593 | val_loss:  0.01322569977492094 | accuracy:  0.960222752585521\n",
      "40/40 [==============================] - 0s 433us/step\n",
      "epoch:  470  | loss:  0.01319788210093975 | val_loss:  0.013226969167590141 | accuracy:  0.960222752585521\n",
      "40/40 [==============================] - 0s 387us/step\n",
      "epoch:  480  | loss:  0.012776673771440983 | val_loss:  0.012322401627898216 | accuracy:  0.9618138424821002\n",
      "40/40 [==============================] - 0s 432us/step\n",
      "epoch:  490  | loss:  0.012512428686022758 | val_loss:  0.010871685110032558 | accuracy:  0.9681782020684169\n",
      "40/40 [==============================] - 0s 431us/step\n",
      "epoch:  500  | loss:  0.01185342576354742 | val_loss:  0.010139141231775284 | accuracy:  0.9673826571201273\n",
      "40/40 [==============================] - 0s 379us/step\n",
      "epoch:  510  | loss:  0.011313388124108315 | val_loss:  0.010142049752175808 | accuracy:  0.9681782020684169\n",
      "40/40 [==============================] - 0s 377us/step\n",
      "epoch:  520  | loss:  0.011829585768282413 | val_loss:  0.00897947233170271 | accuracy:  0.9689737470167065\n",
      "40/40 [==============================] - 0s 397us/step\n",
      "epoch:  530  | loss:  0.01030181348323822 | val_loss:  0.008478572592139244 | accuracy:  0.9681782020684169\n",
      "40/40 [==============================] - 0s 410us/step\n",
      "epoch:  540  | loss:  0.01005924865603447 | val_loss:  0.00797889195382595 | accuracy:  0.9689737470167065\n",
      "40/40 [==============================] - 0s 410us/step\n",
      "epoch:  550  | loss:  0.009871783666312695 | val_loss:  0.007612572051584721 | accuracy:  0.9737470167064439\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  560  | loss:  0.009195240214467049 | val_loss:  0.0071536581963300705 | accuracy:  0.9817024661893397\n",
      "40/40 [==============================] - 0s 386us/step\n",
      "epoch:  570  | loss:  0.009220545180141926 | val_loss:  0.006825896445661783 | accuracy:  0.9824980111376292\n",
      "40/40 [==============================] - 0s 421us/step\n",
      "epoch:  580  | loss:  0.008947542868554592 | val_loss:  0.0065155550837516785 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 434us/step\n",
      "epoch:  590  | loss:  0.008542989380657673 | val_loss:  0.006307214964181185 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 401us/step\n",
      "epoch:  600  | loss:  0.007818047888576984 | val_loss:  0.006364940200001001 | accuracy:  0.9824980111376292\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  610  | loss:  0.007972409948706627 | val_loss:  0.005834594834595919 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  620  | loss:  0.00782821699976921 | val_loss:  0.005887215957045555 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 380us/step\n",
      "epoch:  630  | loss:  0.008114814758300781 | val_loss:  0.005836533382534981 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 424us/step\n",
      "epoch:  640  | loss:  0.007197106257081032 | val_loss:  0.005438182502985001 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  650  | loss:  0.0072950636968016624 | val_loss:  0.005797811783850193 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 425us/step\n",
      "epoch:  660  | loss:  0.0071985251270234585 | val_loss:  0.005477194674313068 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 366us/step\n",
      "epoch:  670  | loss:  0.007425200659781694 | val_loss:  0.005493328906595707 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  680  | loss:  0.007274973671883345 | val_loss:  0.005254083313047886 | accuracy:  0.984884645982498\n",
      "40/40 [==============================] - 0s 422us/step\n",
      "epoch:  690  | loss:  0.007080188021063805 | val_loss:  0.00558409933000803 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 385us/step\n",
      "epoch:  700  | loss:  0.007088033482432365 | val_loss:  0.005528821609914303 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 386us/step\n",
      "epoch:  710  | loss:  0.006595773156732321 | val_loss:  0.005333594977855682 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 436us/step\n",
      "epoch:  720  | loss:  0.006812992971390486 | val_loss:  0.005415555089712143 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 459us/step\n",
      "epoch:  730  | loss:  0.006959185469895601 | val_loss:  0.005391098093241453 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 415us/step\n",
      "epoch:  740  | loss:  0.006595121696591377 | val_loss:  0.005337750073522329 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 426us/step\n",
      "epoch:  750  | loss:  0.006377836223691702 | val_loss:  0.005315818358212709 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 416us/step\n",
      "epoch:  760  | loss:  0.0064649381674826145 | val_loss:  0.005282548256218433 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 465us/step\n",
      "epoch:  770  | loss:  0.006617225706577301 | val_loss:  0.005329168867319822 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  780  | loss:  0.006433469709008932 | val_loss:  0.005353095475584269 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  790  | loss:  0.006332245655357838 | val_loss:  0.0053184456191957 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 440us/step\n",
      "epoch:  800  | loss:  0.006609869655221701 | val_loss:  0.005268725100904703 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 381us/step\n",
      "epoch:  810  | loss:  0.006505601108074188 | val_loss:  0.004974587820470333 | accuracy:  0.984884645982498\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  820  | loss:  0.006378220394253731 | val_loss:  0.005277174059301615 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 390us/step\n",
      "epoch:  830  | loss:  0.0062125977128744125 | val_loss:  0.005277211312204599 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  840  | loss:  0.00653258990496397 | val_loss:  0.005283023696392775 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 412us/step\n",
      "epoch:  850  | loss:  0.006282932590693235 | val_loss:  0.005265099927783012 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 456us/step\n",
      "epoch:  860  | loss:  0.006128255277872086 | val_loss:  0.005290714092552662 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 446us/step\n",
      "epoch:  870  | loss:  0.006128143984824419 | val_loss:  0.005306825041770935 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 440us/step\n",
      "epoch:  880  | loss:  0.006156036164611578 | val_loss:  0.0053032077848911285 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 375us/step\n",
      "epoch:  890  | loss:  0.0060978420078754425 | val_loss:  0.005294050555676222 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  900  | loss:  0.00613813940435648 | val_loss:  0.0052677541971206665 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 484us/step\n",
      "epoch:  910  | loss:  0.0061180260963737965 | val_loss:  0.0052535817958414555 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 405us/step\n",
      "epoch:  920  | loss:  0.00622570188716054 | val_loss:  0.005242031998932362 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  930  | loss:  0.006121331360191107 | val_loss:  0.005292042158544064 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 435us/step\n",
      "epoch:  940  | loss:  0.0060042086988687515 | val_loss:  0.005256245844066143 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 416us/step\n",
      "epoch:  950  | loss:  0.005939279682934284 | val_loss:  0.005264185834676027 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 421us/step\n",
      "epoch:  960  | loss:  0.0059377653524279594 | val_loss:  0.005254900082945824 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 429us/step\n",
      "epoch:  970  | loss:  0.0058560362085700035 | val_loss:  0.005270292051136494 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 420us/step\n",
      "epoch:  980  | loss:  0.005944789387285709 | val_loss:  0.005249707959592342 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 463us/step\n",
      "epoch:  990  | loss:  0.0061411019414663315 | val_loss:  0.005283737555146217 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 436us/step\n",
      "epoch:  1000  | loss:  0.005938507150858641 | val_loss:  0.005285492166876793 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 405us/step\n",
      "epoch:  1010  | loss:  0.005922839045524597 | val_loss:  0.005249262321740389 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  1020  | loss:  0.005998102482408285 | val_loss:  0.005303768441081047 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 430us/step\n",
      "epoch:  1030  | loss:  0.005842432379722595 | val_loss:  0.005249723792076111 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 382us/step\n",
      "epoch:  1040  | loss:  0.005880181211978197 | val_loss:  0.005278321914374828 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 401us/step\n",
      "epoch:  1050  | loss:  0.005981793627142906 | val_loss:  0.005266129970550537 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 388us/step\n",
      "epoch:  1060  | loss:  0.005994314793497324 | val_loss:  0.005251605529338121 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  1070  | loss:  0.006067504640668631 | val_loss:  0.005353578832000494 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  1080  | loss:  0.005914222914725542 | val_loss:  0.005250489339232445 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 383us/step\n",
      "epoch:  1090  | loss:  0.005873898044228554 | val_loss:  0.005302851554006338 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 385us/step\n",
      "epoch:  1100  | loss:  0.005918340291827917 | val_loss:  0.005255107302218676 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 493us/step\n",
      "epoch:  1110  | loss:  0.005968686193227768 | val_loss:  0.005286942236125469 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 398us/step\n",
      "epoch:  1120  | loss:  0.005841047503054142 | val_loss:  0.005312718451023102 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 443us/step\n",
      "epoch:  1130  | loss:  0.0058587100356817245 | val_loss:  0.00530970050022006 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 389us/step\n",
      "epoch:  1140  | loss:  0.005646021571010351 | val_loss:  0.005348877049982548 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  1150  | loss:  0.005984558258205652 | val_loss:  0.005413937382400036 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  1160  | loss:  0.005849877372384071 | val_loss:  0.005304905120283365 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 394us/step\n",
      "epoch:  1170  | loss:  0.005773160606622696 | val_loss:  0.005254197865724564 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 464us/step\n",
      "epoch:  1180  | loss:  0.0059243496507406235 | val_loss:  0.005582534242421389 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 369us/step\n",
      "epoch:  1190  | loss:  0.006124594714492559 | val_loss:  0.005292454268783331 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  1200  | loss:  0.005798832047730684 | val_loss:  0.00529963243752718 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  1210  | loss:  0.0061041731387376785 | val_loss:  0.005333066452294588 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 404us/step\n",
      "epoch:  1220  | loss:  0.005715752020478249 | val_loss:  0.005370211321860552 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 471us/step\n",
      "epoch:  1230  | loss:  0.005872518755495548 | val_loss:  0.0053482456132769585 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 404us/step\n",
      "epoch:  1240  | loss:  0.0057718935422599316 | val_loss:  0.0053121005184948444 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 425us/step\n",
      "epoch:  1250  | loss:  0.00580483116209507 | val_loss:  0.005305059719830751 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  1260  | loss:  0.005845798645168543 | val_loss:  0.005335216876119375 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 431us/step\n",
      "epoch:  1270  | loss:  0.0058394866064190865 | val_loss:  0.005322205368429422 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 463us/step\n",
      "epoch:  1280  | loss:  0.005884743761271238 | val_loss:  0.005320197436958551 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 418us/step\n",
      "epoch:  1290  | loss:  0.005915137939155102 | val_loss:  0.005305452737957239 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 431us/step\n",
      "epoch:  1300  | loss:  0.005729954224079847 | val_loss:  0.005312546622008085 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  1310  | loss:  0.006104137748479843 | val_loss:  0.005350867286324501 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  1320  | loss:  0.006051114294677973 | val_loss:  0.005323107820004225 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 474us/step\n",
      "epoch:  1330  | loss:  0.005826166830956936 | val_loss:  0.005316164344549179 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 393us/step\n",
      "epoch:  1340  | loss:  0.005902961362153292 | val_loss:  0.005316568072885275 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 431us/step\n",
      "epoch:  1350  | loss:  0.005745744798332453 | val_loss:  0.005306267645210028 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 396us/step\n",
      "epoch:  1360  | loss:  0.005822589620947838 | val_loss:  0.005279324017465115 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 484us/step\n",
      "epoch:  1370  | loss:  0.005864094942808151 | val_loss:  0.005216666962951422 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 371us/step\n",
      "epoch:  1380  | loss:  0.005812869872897863 | val_loss:  0.0053136395290493965 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 447us/step\n",
      "epoch:  1390  | loss:  0.00576048344373703 | val_loss:  0.005262236576527357 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 403us/step\n",
      "epoch:  1400  | loss:  0.005797494202852249 | val_loss:  0.005316627211868763 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 424us/step\n",
      "epoch:  1410  | loss:  0.005753395613282919 | val_loss:  0.005300609860569239 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 501us/step\n",
      "epoch:  1420  | loss:  0.005983550101518631 | val_loss:  0.005357281304895878 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  1430  | loss:  0.005878079682588577 | val_loss:  0.00527865020558238 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 433us/step\n",
      "epoch:  1440  | loss:  0.005773032084107399 | val_loss:  0.005306730046868324 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 402us/step\n",
      "epoch:  1450  | loss:  0.005784284323453903 | val_loss:  0.005621132906526327 | accuracy:  0.9824980111376292\n",
      "40/40 [==============================] - 0s 450us/step\n",
      "epoch:  1460  | loss:  0.0057326415553689 | val_loss:  0.005296444054692984 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 392us/step\n",
      "epoch:  1470  | loss:  0.005818730220198631 | val_loss:  0.0053045074455440044 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 389us/step\n",
      "epoch:  1480  | loss:  0.005823895335197449 | val_loss:  0.005304815713316202 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  1490  | loss:  0.005821878090500832 | val_loss:  0.005296460352838039 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  1500  | loss:  0.005988993216305971 | val_loss:  0.005257006268948317 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  1510  | loss:  0.00577680254355073 | val_loss:  0.005306472070515156 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 464us/step\n",
      "epoch:  1520  | loss:  0.005862150341272354 | val_loss:  0.005317470990121365 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 391us/step\n",
      "epoch:  1530  | loss:  0.005755938123911619 | val_loss:  0.0055637964978814125 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 432us/step\n",
      "epoch:  1540  | loss:  0.005652157124131918 | val_loss:  0.005304746329784393 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 423us/step\n",
      "epoch:  1550  | loss:  0.005762095097452402 | val_loss:  0.005310080014169216 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 488us/step\n",
      "epoch:  1560  | loss:  0.005719874985516071 | val_loss:  0.005343474913388491 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  1570  | loss:  0.005778685677796602 | val_loss:  0.005338286980986595 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 438us/step\n",
      "epoch:  1580  | loss:  0.005762323271483183 | val_loss:  0.005320060066878796 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  1590  | loss:  0.0057302010245621204 | val_loss:  0.00531381368637085 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 433us/step\n",
      "epoch:  1600  | loss:  0.005755733232945204 | val_loss:  0.00532258627936244 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 464us/step\n",
      "epoch:  1610  | loss:  0.005871978588402271 | val_loss:  0.005457895342260599 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  1620  | loss:  0.005881998687982559 | val_loss:  0.005377584137022495 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 438us/step\n",
      "epoch:  1630  | loss:  0.006145971827208996 | val_loss:  0.005288944113999605 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 465us/step\n",
      "epoch:  1640  | loss:  0.005708419252187014 | val_loss:  0.005518782883882523 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 389us/step\n",
      "epoch:  1650  | loss:  0.005709842313081026 | val_loss:  0.0054399375803768635 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 401us/step\n",
      "epoch:  1660  | loss:  0.005674334708601236 | val_loss:  0.005419149994850159 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  1670  | loss:  0.005754848476499319 | val_loss:  0.00531754968687892 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 402us/step\n",
      "epoch:  1680  | loss:  0.005768191069364548 | val_loss:  0.005318886134773493 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  1690  | loss:  0.005842174403369427 | val_loss:  0.005444473586976528 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 417us/step\n",
      "epoch:  1700  | loss:  0.005788744427263737 | val_loss:  0.005477828439325094 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 449us/step\n",
      "epoch:  1710  | loss:  0.005745915230363607 | val_loss:  0.005442354362457991 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 489us/step\n",
      "epoch:  1720  | loss:  0.00573272118344903 | val_loss:  0.005324384197592735 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 508us/step\n",
      "epoch:  1730  | loss:  0.005777157377451658 | val_loss:  0.005318918731063604 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 408us/step\n",
      "epoch:  1740  | loss:  0.005850127898156643 | val_loss:  0.005409035831689835 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 439us/step\n",
      "epoch:  1750  | loss:  0.005651308689266443 | val_loss:  0.005384633783251047 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 479us/step\n",
      "epoch:  1760  | loss:  0.005790557246655226 | val_loss:  0.005300603341311216 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  1770  | loss:  0.0057608443312346935 | val_loss:  0.005384449847042561 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 421us/step\n",
      "epoch:  1780  | loss:  0.005386088974773884 | val_loss:  0.005217990837991238 | accuracy:  0.9832935560859188\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  1790  | loss:  0.005206583067774773 | val_loss:  0.00505948206409812 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 485us/step\n",
      "epoch:  1800  | loss:  0.0050543793477118015 | val_loss:  0.004720717202872038 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  1810  | loss:  0.005292421206831932 | val_loss:  0.004445234779268503 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 384us/step\n",
      "epoch:  1820  | loss:  0.004913692828267813 | val_loss:  0.004403730388730764 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 380us/step\n",
      "epoch:  1830  | loss:  0.004755611065775156 | val_loss:  0.00427771732211113 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  1840  | loss:  0.005023951642215252 | val_loss:  0.00416546082124114 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 429us/step\n",
      "epoch:  1850  | loss:  0.0046221548691391945 | val_loss:  0.003989265765994787 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 421us/step\n",
      "epoch:  1860  | loss:  0.004564542789012194 | val_loss:  0.004090575966984034 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  1870  | loss:  0.004381192848086357 | val_loss:  0.004207374528050423 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 375us/step\n",
      "epoch:  1880  | loss:  0.004816281143575907 | val_loss:  0.004121525678783655 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 447us/step\n",
      "epoch:  1890  | loss:  0.00433967774733901 | val_loss:  0.0038763750344514847 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  1900  | loss:  0.004593635909259319 | val_loss:  0.0038171205669641495 | accuracy:  0.9840891010342084\n",
      "40/40 [==============================] - 0s 442us/step\n",
      "epoch:  1910  | loss:  0.004265914671123028 | val_loss:  0.0037102429196238518 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 418us/step\n",
      "epoch:  1920  | loss:  0.00397111102938652 | val_loss:  0.003670580917969346 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 465us/step\n",
      "epoch:  1930  | loss:  0.004220191854983568 | val_loss:  0.0037565443199127913 | accuracy:  0.9872712808273667\n",
      "40/40 [==============================] - 0s 446us/step\n",
      "epoch:  1940  | loss:  0.0037794022355228662 | val_loss:  0.003508788999170065 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 391us/step\n",
      "epoch:  1950  | loss:  0.003890064312145114 | val_loss:  0.0035313076805323362 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 414us/step\n",
      "epoch:  1960  | loss:  0.004264940042048693 | val_loss:  0.0034581217914819717 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  1970  | loss:  0.003935185726732016 | val_loss:  0.0034552577417343855 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  1980  | loss:  0.0037197116762399673 | val_loss:  0.003545508487150073 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 449us/step\n",
      "epoch:  1990  | loss:  0.0041375490836799145 | val_loss:  0.003499723272398114 | accuracy:  0.9880668257756563\n",
      "40/40 [==============================] - 0s 438us/step\n",
      "epoch:  2000  | loss:  0.003933644853532314 | val_loss:  0.003313118126243353 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  2010  | loss:  0.003560159122571349 | val_loss:  0.0034298677928745747 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 404us/step\n",
      "epoch:  2020  | loss:  0.004024863708764315 | val_loss:  0.003524388885125518 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 411us/step\n",
      "epoch:  2030  | loss:  0.003269834676757455 | val_loss:  0.0034813948441296816 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  2040  | loss:  0.004127527587115765 | val_loss:  0.0032705459743738174 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 456us/step\n",
      "epoch:  2050  | loss:  0.0035808964166790247 | val_loss:  0.0033671485725790262 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 382us/step\n",
      "epoch:  2060  | loss:  0.0035489113070070744 | val_loss:  0.003366966964676976 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 404us/step\n",
      "epoch:  2070  | loss:  0.003650688799098134 | val_loss:  0.0038560116663575172 | accuracy:  0.9880668257756563\n",
      "40/40 [==============================] - 0s 383us/step\n",
      "epoch:  2080  | loss:  0.0031091084238141775 | val_loss:  0.003515164600685239 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 397us/step\n",
      "epoch:  2090  | loss:  0.0030595045536756516 | val_loss:  0.0033599771559238434 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 399us/step\n",
      "epoch:  2100  | loss:  0.003202972700819373 | val_loss:  0.0033760920632630587 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 390us/step\n",
      "epoch:  2110  | loss:  0.003126651979982853 | val_loss:  0.0034035576973110437 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2120  | loss:  0.003433362813666463 | val_loss:  0.0034111575223505497 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 426us/step\n",
      "epoch:  2130  | loss:  0.002809600206092 | val_loss:  0.003364644479006529 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2140  | loss:  0.002968533430248499 | val_loss:  0.003482308704406023 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 466us/step\n",
      "epoch:  2150  | loss:  0.002849213546141982 | val_loss:  0.0034547194372862577 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  2160  | loss:  0.0033385795541107655 | val_loss:  0.003592299297451973 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 448us/step\n",
      "epoch:  2170  | loss:  0.0029227358754724264 | val_loss:  0.0033422699198126793 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 397us/step\n",
      "epoch:  2180  | loss:  0.0029206862673163414 | val_loss:  0.003373549785465002 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 386us/step\n",
      "epoch:  2190  | loss:  0.003073048312216997 | val_loss:  0.003560419660061598 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 468us/step\n",
      "epoch:  2200  | loss:  0.0032222901936620474 | val_loss:  0.0033295187167823315 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2210  | loss:  0.002953322371467948 | val_loss:  0.003437200328335166 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 392us/step\n",
      "epoch:  2220  | loss:  0.003045008983463049 | val_loss:  0.003405849914997816 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 430us/step\n",
      "epoch:  2230  | loss:  0.002950892085209489 | val_loss:  0.0033941285219043493 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 426us/step\n",
      "epoch:  2240  | loss:  0.003054838627576828 | val_loss:  0.0033939811401069164 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 409us/step\n",
      "epoch:  2250  | loss:  0.003070475999265909 | val_loss:  0.0033145553898066282 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 436us/step\n",
      "epoch:  2260  | loss:  0.003109805751591921 | val_loss:  0.0033581960014998913 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 407us/step\n",
      "epoch:  2270  | loss:  0.002854941412806511 | val_loss:  0.003401753958314657 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 472us/step\n",
      "epoch:  2280  | loss:  0.0027453736402094364 | val_loss:  0.0033666524104774 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  2290  | loss:  0.0031457836739718914 | val_loss:  0.003338573267683387 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 421us/step\n",
      "epoch:  2300  | loss:  0.002977820113301277 | val_loss:  0.003403319977223873 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 403us/step\n",
      "epoch:  2310  | loss:  0.0030344929546117783 | val_loss:  0.003386738710105419 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 389us/step\n",
      "epoch:  2320  | loss:  0.00272070849314332 | val_loss:  0.003429442411288619 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 444us/step\n",
      "epoch:  2330  | loss:  0.0029633291997015476 | val_loss:  0.003398192347958684 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 376us/step\n",
      "epoch:  2340  | loss:  0.0029065199196338654 | val_loss:  0.0032095639035105705 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 416us/step\n",
      "epoch:  2350  | loss:  0.0033236483577638865 | val_loss:  0.003382540075108409 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 442us/step\n",
      "epoch:  2360  | loss:  0.0030247352551668882 | val_loss:  0.003599494230002165 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 452us/step\n",
      "epoch:  2370  | loss:  0.002625086810439825 | val_loss:  0.003435032907873392 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 425us/step\n",
      "epoch:  2380  | loss:  0.002834317972883582 | val_loss:  0.003684327704831958 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  2390  | loss:  0.003163109067827463 | val_loss:  0.0034626692067831755 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2400  | loss:  0.002742634853348136 | val_loss:  0.0033719330094754696 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 459us/step\n",
      "epoch:  2410  | loss:  0.002963671460747719 | val_loss:  0.003436309052631259 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 384us/step\n",
      "epoch:  2420  | loss:  0.0028339396230876446 | val_loss:  0.003301088698208332 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 441us/step\n",
      "epoch:  2430  | loss:  0.0026048636063933372 | val_loss:  0.00343951559625566 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 394us/step\n",
      "epoch:  2440  | loss:  0.0031357312109321356 | val_loss:  0.00346048129722476 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 386us/step\n",
      "epoch:  2450  | loss:  0.002943698549643159 | val_loss:  0.0034701749682426453 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 446us/step\n",
      "epoch:  2460  | loss:  0.002573033794760704 | val_loss:  0.0034328096080571413 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2470  | loss:  0.0028946264646947384 | val_loss:  0.003429512958973646 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  2480  | loss:  0.0025536827743053436 | val_loss:  0.003403998911380768 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 426us/step\n",
      "epoch:  2490  | loss:  0.002561869565397501 | val_loss:  0.003413997357711196 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 396us/step\n",
      "epoch:  2500  | loss:  0.002794126980006695 | val_loss:  0.003389471909031272 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 451us/step\n",
      "epoch:  2510  | loss:  0.0029550869949162006 | val_loss:  0.003629037644714117 | accuracy:  0.9888623707239459\n",
      "40/40 [==============================] - 0s 419us/step\n",
      "epoch:  2520  | loss:  0.002497397130355239 | val_loss:  0.003428217489272356 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 384us/step\n",
      "epoch:  2530  | loss:  0.0026898467913269997 | val_loss:  0.003297657473012805 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 428us/step\n",
      "epoch:  2540  | loss:  0.002695403527468443 | val_loss:  0.0034376541152596474 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 376us/step\n",
      "epoch:  2550  | loss:  0.002469973172992468 | val_loss:  0.0034074601717293262 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2560  | loss:  0.0025618888903409243 | val_loss:  0.0033567564096301794 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 386us/step\n",
      "epoch:  2570  | loss:  0.0025570737197995186 | val_loss:  0.00337396003305912 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 442us/step\n",
      "epoch:  2580  | loss:  0.00282794120721519 | val_loss:  0.003279375610873103 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 402us/step\n",
      "epoch:  2590  | loss:  0.002926595276221633 | val_loss:  0.003321930533275008 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  2600  | loss:  0.0027156048454344273 | val_loss:  0.0031830098014324903 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 403us/step\n",
      "epoch:  2610  | loss:  0.0026054817717522383 | val_loss:  0.0033297440968453884 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 455us/step\n",
      "epoch:  2620  | loss:  0.00259586563333869 | val_loss:  0.003077309112995863 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 447us/step\n",
      "epoch:  2630  | loss:  0.0025498722679913044 | val_loss:  0.0031468828674405813 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  2640  | loss:  0.0026126320008188486 | val_loss:  0.0031629838049411774 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 388us/step\n",
      "epoch:  2650  | loss:  0.0025892136618494987 | val_loss:  0.0032862189691513777 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 430us/step\n",
      "epoch:  2660  | loss:  0.0024541248567402363 | val_loss:  0.003147031646221876 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 414us/step\n",
      "epoch:  2670  | loss:  0.0027709812857210636 | val_loss:  0.003099416848272085 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 434us/step\n",
      "epoch:  2680  | loss:  0.0026904973201453686 | val_loss:  0.0031256424263119698 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 437us/step\n",
      "epoch:  2690  | loss:  0.002526224823668599 | val_loss:  0.0031769380439072847 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 383us/step\n",
      "epoch:  2700  | loss:  0.0024988348595798016 | val_loss:  0.003160336520522833 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 403us/step\n",
      "epoch:  2710  | loss:  0.0028141657821834087 | val_loss:  0.0031919225584715605 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 441us/step\n",
      "epoch:  2720  | loss:  0.00277400529012084 | val_loss:  0.0031444919295608997 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 426us/step\n",
      "epoch:  2730  | loss:  0.0025964428205043077 | val_loss:  0.0031572640873491764 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 471us/step\n",
      "epoch:  2740  | loss:  0.0023594514932483435 | val_loss:  0.0030973413959145546 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 393us/step\n",
      "epoch:  2750  | loss:  0.0026893550530076027 | val_loss:  0.00316802435554564 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 395us/step\n",
      "epoch:  2760  | loss:  0.002625545021146536 | val_loss:  0.0031491839326918125 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 422us/step\n",
      "epoch:  2770  | loss:  0.002883000997826457 | val_loss:  0.0030895250383764505 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 400us/step\n",
      "epoch:  2780  | loss:  0.0023743638303130865 | val_loss:  0.003139419946819544 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 452us/step\n",
      "epoch:  2790  | loss:  0.002637552097439766 | val_loss:  0.0031043810304254293 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 393us/step\n",
      "epoch:  2800  | loss:  0.0026742841582745314 | val_loss:  0.0030708860140293837 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 448us/step\n",
      "epoch:  2810  | loss:  0.0028619994409382343 | val_loss:  0.003218669444322586 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 441us/step\n",
      "epoch:  2820  | loss:  0.0026145735755562782 | val_loss:  0.0031682136468589306 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 379us/step\n",
      "epoch:  2830  | loss:  0.002441342920064926 | val_loss:  0.003062790958210826 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 508us/step\n",
      "epoch:  2840  | loss:  0.002408809494227171 | val_loss:  0.003131261095404625 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 495us/step\n",
      "epoch:  2850  | loss:  0.002642429433763027 | val_loss:  0.00317035848274827 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 406us/step\n",
      "epoch:  2860  | loss:  0.0025620905216783285 | val_loss:  0.0031301036942750216 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 434us/step\n",
      "epoch:  2870  | loss:  0.002254004590213299 | val_loss:  0.003182672895491123 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 459us/step\n",
      "epoch:  2880  | loss:  0.0023568286560475826 | val_loss:  0.003165189176797867 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 424us/step\n",
      "epoch:  2890  | loss:  0.0026381760835647583 | val_loss:  0.0030618098098784685 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 439us/step\n",
      "epoch:  2900  | loss:  0.0022151030134409666 | val_loss:  0.0031224051490426064 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 404us/step\n",
      "epoch:  2910  | loss:  0.0026466662529855967 | val_loss:  0.0030606321524828672 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 408us/step\n",
      "epoch:  2920  | loss:  0.0024284939281642437 | val_loss:  0.003105748677626252 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 372us/step\n",
      "epoch:  2930  | loss:  0.002870288211852312 | val_loss:  0.0031691440381109715 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 408us/step\n",
      "epoch:  2940  | loss:  0.0026498334482312202 | val_loss:  0.0030697942711412907 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 427us/step\n",
      "epoch:  2950  | loss:  0.0023556554224342108 | val_loss:  0.003287414088845253 | accuracy:  0.9896579156722355\n",
      "40/40 [==============================] - 0s 461us/step\n",
      "epoch:  2960  | loss:  0.0023661626037210226 | val_loss:  0.003151907119899988 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 375us/step\n",
      "epoch:  2970  | loss:  0.002418956719338894 | val_loss:  0.0031145974062383175 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 413us/step\n",
      "epoch:  2980  | loss:  0.0026343984063714743 | val_loss:  0.0031515087466686964 | accuracy:  0.9904534606205251\n",
      "40/40 [==============================] - 0s 469us/step\n",
      "epoch:  2990  | loss:  0.0025020274333655834 | val_loss:  0.003070463426411152 | accuracy:  0.9904534606205251\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                1530      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                620       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 126       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,826\n",
      "Trainable params: 4,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/getting-started/174307\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Callback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X = X_val\n",
    "        self.y = y_val.argmax(axis=1)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == 0:\n",
    "            return\n",
    "        if epoch%10==0: #Hier aanpassan na hoeveel epochs je wilt zien\n",
    "            pred = (model.predict(self.X))\n",
    "            print('epoch: ',epoch, ' | loss: ', str(logs['loss']), '| val_loss: ', str(logs['val_loss']), '| accuracy: ', accuracy_score(self.y,pred.argmax(axis=1)))\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10) # early stopping -> will sotp training if no improvement for 10 epochs\n",
    "# build the model and train it\n",
    "model = tf.keras.Sequential()\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(50, activation='relu')) # relu is used for performance (50\n",
    "model.add(Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(30, activation='relu')) #30\n",
    "model.add(Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae']) # ook adam ke proberen kan zijn dat het netwerk hiermee beter leert\n",
    "# model.compile(optimizer='ftrl', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(inputs_train, outputs_train, epochs=3000, batch_size=8, validation_data=(inputs_validate, outputs_validate), callbacks=[Callback(inputs_validate, outputs_validate)], verbose=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "gmiRMLprEbZf",
    "outputId": "73f32a27-735e-4ad0-91ff-959ad8e0de97"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'g.', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, 'b.', label=\"val loss\")\n",
    "\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNp0t-ELAf5t"
   },
   "source": [
    "#Run with Test Data\n",
    "Put our test data into the model and plot the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "kRxeKbcPAl1W",
    "outputId": "3d93a119-87d0-495e-9914-5caff7f8cc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 432us/step\n",
      "predictions =\n",
      " [[0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    0.    0.998 0.002]\n",
      " [0.    0.    0.    0.    1.    0.   ]\n",
      " [0.    0.001 0.012 0.987 0.    0.   ]]\n",
      "actual =\n",
      " [[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "[[0.90909091 0.         0.         0.         0.         0.        ]\n",
      " [0.09090909 0.84210526 0.         0.         0.         0.        ]\n",
      " [0.         0.10526316 1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.99580185 0.        ]\n",
      " [0.         0.05263158 0.         0.         0.00419815 1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fcbad06e5e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the model to predict the test inputs\n",
    "predictions = model.predict(inputs_test)\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# print the predictions and the expected ouputs\n",
    "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
    "print(\"actual =\\n\", outputs_test)\n",
    "\n",
    "# sns.regplot(outputs_test, predictions)\n",
    "#Predict\n",
    "y_prediction = np.argmax(predictions, axis = 1)\n",
    "\n",
    "y_test=np.argmax(outputs_test, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "disp =  ConfusionMatrixDisplay(confusion_matrix=result)\n",
    "print(result)\n",
    "disp.plot()\n",
    "# Plot the predictions along with to the test data\n",
    "# plt.clf()\n",
    "# plt.title('Training data predicted vs actual values')\n",
    "# plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
    "# plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "[[0. 1. 0. 0. 0. 0.]]\n",
      "1\n",
      "laptop-better\n"
     ]
    }
   ],
   "source": [
    "# array_test = [[33., 0.152, 0.83, 27., 18.]]\n",
    "array_test = [[55.,     0.25,   0.93,  51.,    20.,    52.,     0.239,  1.,    52.,     0.,\n",
    " 54.,     0.247,  0.96,  52.,    15.,    40.,     0.184,  1.,    40.,    0.,\n",
    " 35.,     0.158,  0.87, 30.,    17.,    35.,     0.157,  0.77,  26.,    22.,\n",
    " 32.,     0.146,  0.9,   29.,    14.,    33.,     0.149,  0.84,  28.,    18.,\n",
    " 31.,     0.143,  0.88,  28.,    15.,    32.,     0.147,  0.79,  26.,    20.   ]]\n",
    "pred_test = model.predict(array_test)\n",
    "\n",
    "print(pred_test)\n",
    "class_index = np.argmax(pred_test)\n",
    "print(class_index)\n",
    "print(CLASSES[class_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_saved/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_saved/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_device.ipynb\n",
      "model_saved/\n",
      "model_saved/keras_metadata.pb\n",
      "model_saved/saved_model.pb\n",
      "model_saved/assets/\n",
      "model_saved/variables/\n",
      "model_saved/variables/variables.index\n",
      "model_saved/variables/variables.data-00000-of-00001\n",
      "notebook.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar chvfz notebook.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
